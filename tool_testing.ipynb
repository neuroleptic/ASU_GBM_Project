{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the VAFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing patient: p191\n",
      "  Processing sample: 41126, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 434 variants in sample 41126\n",
      "  Processing sample: 41133, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 440 variants in sample 41133\n",
      "  Processing sample: 41114, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 398 variants in sample 41114\n",
      "  Processing sample: 42407, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 313 variants in sample 42407\n",
      "  Total variants for patient p191: 1585\n",
      "Data saved to results/p191/p191_variants.tsv\n",
      "Summary statistics saved to: results/p191\n",
      "Excel workbook with multiple sheets saved to results/p191/p191_variants.xlsx\n",
      "  Generating visualizations for patient p191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient p191...\n",
      "Shared variants VAF comparison saved to: results/p191/p191_shared_variants_vaf.csv\n",
      "Processing patient: P163\n",
      "  Processing sample: 375363, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 576 variants in sample 375363\n",
      "  Processing sample: 376223, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 671 variants in sample 376223\n",
      "  Processing sample: 376213, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 651 variants in sample 376213\n",
      "  Processing sample: 376137, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 467 variants in sample 376137\n",
      "  Processing sample: 376121, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 888 variants in sample 376121\n",
      "  Processing sample: 375380, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 597 variants in sample 375380\n",
      "  Total variants for patient P163: 3850\n",
      "Data saved to results/P163/P163_variants.tsv\n",
      "Summary statistics saved to: results/P163\n",
      "Excel workbook with multiple sheets saved to results/P163/P163_variants.xlsx\n",
      "  Generating visualizations for patient P163...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient P163...\n",
      "Shared variants VAF comparison saved to: results/P163/P163_shared_variants_vaf.csv\n",
      "Processing patient: P190\n",
      "  Processing sample: 359388, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 802 variants in sample 359388\n",
      "  Processing sample: 359227, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 891 variants in sample 359227\n",
      "  Processing sample: 359287, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 611 variants in sample 359287\n",
      "  Processing sample: 359261, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 581 variants in sample 359261\n",
      "  Processing sample: 359313, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 598 variants in sample 359313\n",
      "  Total variants for patient P190: 3483\n",
      "Data saved to results/P190/P190_variants.tsv\n",
      "Summary statistics saved to: results/P190\n",
      "Excel workbook with multiple sheets saved to results/P190/P190_variants.xlsx\n",
      "  Generating visualizations for patient P190...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient P190...\n",
      "Shared variants VAF comparison saved to: results/P190/P190_shared_variants_vaf.csv\n",
      "Processing patient: p152\n",
      "  Processing sample: 387930, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 493 variants in sample 387930\n",
      "  Processing sample: 388519, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 726 variants in sample 388519\n",
      "  Processing sample: 387947, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 610 variants in sample 387947\n",
      "  Processing sample: 387979, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 455 variants in sample 387979\n",
      "  Processing sample: 387720, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 584 variants in sample 387720\n",
      "  Processing sample: 387942, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 610 variants in sample 387942\n",
      "  Processing sample: 388615, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 737 variants in sample 388615\n",
      "  Total variants for patient p152: 4215\n",
      "Data saved to results/p152/p152_variants.tsv\n",
      "Summary statistics saved to: results/p152\n",
      "Excel workbook with multiple sheets saved to results/p152/p152_variants.xlsx\n",
      "  Generating visualizations for patient p152...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient p152...\n",
      "Shared variants VAF comparison saved to: results/p152/p152_shared_variants_vaf.csv\n",
      "Processing patient: M36\n",
      "  Processing sample: R, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 605 variants in sample R\n",
      "  Processing sample: N, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 520 variants in sample N\n",
      "  Processing sample: T, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 951 variants in sample T\n",
      "  Processing sample: F, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1265 variants in sample F\n",
      "  Processing sample: E, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 965 variants in sample E\n",
      "  Processing sample: K, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 591 variants in sample K\n",
      "  Processing sample: P, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 434 variants in sample P\n",
      "  Total variants for patient M36: 5331\n",
      "Data saved to results/M36/M36_variants.tsv\n",
      "Summary statistics saved to: results/M36\n",
      "Excel workbook with multiple sheets saved to results/M36/M36_variants.xlsx\n",
      "  Generating visualizations for patient M36...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient M36...\n",
      "Shared variants VAF comparison saved to: results/M36/M36_shared_variants_vaf.csv\n",
      "Processing patient: G53\n",
      "  Processing sample: I, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 957 variants in sample I\n",
      "  Processing sample: A, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 529 variants in sample A\n",
      "  Processing sample: F, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 764 variants in sample F\n",
      "  Processing sample: J, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 799 variants in sample J\n",
      "  Processing sample: C, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 967 variants in sample C\n",
      "  Processing sample: D, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 977 variants in sample D\n",
      "  Processing sample: E, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 947 variants in sample E\n",
      "  Processing sample: B, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 719 variants in sample B\n",
      "  Processing sample: K, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 961 variants in sample K\n",
      "  Total variants for patient G53: 7620\n",
      "Data saved to results/G53/G53_variants.tsv\n",
      "Summary statistics saved to: results/G53\n",
      "Excel workbook with multiple sheets saved to results/G53/G53_variants.xlsx\n",
      "  Generating visualizations for patient G53...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient G53...\n",
      "Shared variants VAF comparison saved to: results/G53/G53_shared_variants_vaf.csv\n",
      "Processing patient: G39\n",
      "  Processing sample: G, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 608 variants in sample G\n",
      "  Processing sample: A, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 525 variants in sample A\n",
      "  Processing sample: F, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 546 variants in sample F\n",
      "  Processing sample: C, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 910 variants in sample C\n",
      "  Processing sample: E, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 519 variants in sample E\n",
      "  Processing sample: B, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 743 variants in sample B\n",
      "  Total variants for patient G39: 3851\n",
      "Data saved to results/G39/G39_variants.tsv\n",
      "Summary statistics saved to: results/G39\n",
      "Excel workbook with multiple sheets saved to results/G39/G39_variants.xlsx\n",
      "  Generating visualizations for patient G39...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient G39...\n",
      "Shared variants VAF comparison saved to: results/G39/G39_shared_variants_vaf.csv\n",
      "Processing patient: G97\n",
      "  Processing sample: I, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 702 variants in sample I\n",
      "  Processing sample: G, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1432 variants in sample G\n",
      "  Processing sample: F, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 15273 variants in sample F\n",
      "  Processing sample: H, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1197 variants in sample H\n",
      "  Processing sample: C, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 857 variants in sample C\n",
      "  Processing sample: D, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 921 variants in sample D\n",
      "  Processing sample: E, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 663 variants in sample E\n",
      "  Processing sample: B, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 10976 variants in sample B\n",
      "  Total variants for patient G97: 32021\n",
      "Data saved to results/G97/G97_variants.tsv\n",
      "Summary statistics saved to: results/G97\n",
      "Excel workbook with multiple sheets saved to results/G97/G97_variants.xlsx\n",
      "  Generating visualizations for patient G97...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient G97...\n",
      "Shared variants VAF comparison saved to: results/G97/G97_shared_variants_vaf.csv\n",
      "Processing patient: G52\n",
      "  Processing sample: A, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 565 variants in sample A\n",
      "  Processing sample: F, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 445 variants in sample F\n",
      "  Processing sample: H, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 592 variants in sample H\n",
      "  Processing sample: M, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 493 variants in sample M\n",
      "  Processing sample: J, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 499 variants in sample J\n",
      "  Processing sample: C, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 355 variants in sample C\n",
      "  Processing sample: D, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 590 variants in sample D\n",
      "  Processing sample: E, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 605 variants in sample E\n",
      "  Processing sample: B, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 440 variants in sample B\n",
      "  Processing sample: L, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 514 variants in sample L\n",
      "  Total variants for patient G52: 5098\n",
      "Data saved to results/G52/G52_variants.tsv\n",
      "Summary statistics saved to: results/G52\n",
      "Excel workbook with multiple sheets saved to results/G52/G52_variants.xlsx\n",
      "  Generating visualizations for patient G52...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient G52...\n",
      "Shared variants VAF comparison saved to: results/G52/G52_shared_variants_vaf.csv\n",
      "Processing patient: G99\n",
      "  Processing sample: I, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 12006 variants in sample I\n",
      "  Processing sample: F, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 12178 variants in sample F\n",
      "  Processing sample: E, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 8903 variants in sample E\n",
      "  Processing sample: K, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 12422 variants in sample K\n",
      "  Total variants for patient G99: 45509\n",
      "Data saved to results/G99/G99_variants.tsv\n",
      "Summary statistics saved to: results/G99\n",
      "Excel workbook with multiple sheets saved to results/G99/G99_variants.xlsx\n",
      "  Generating visualizations for patient G99...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient G99...\n",
      "Shared variants VAF comparison saved to: results/G99/G99_shared_variants_vaf.csv\n",
      "Processing patient: G77\n",
      "  Processing sample: I, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1007 variants in sample I\n",
      "  Processing sample: G, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1679 variants in sample G\n",
      "  Processing sample: F, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1186 variants in sample F\n",
      "  Processing sample: H, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1237 variants in sample H\n",
      "  Processing sample: J, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 796 variants in sample J\n",
      "  Processing sample: C, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 771 variants in sample C\n",
      "  Processing sample: D, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1095 variants in sample D\n",
      "  Processing sample: B, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1073 variants in sample B\n",
      "  Total variants for patient G77: 8844\n",
      "Data saved to results/G77/G77_variants.tsv\n",
      "Summary statistics saved to: results/G77\n",
      "Excel workbook with multiple sheets saved to results/G77/G77_variants.xlsx\n",
      "  Generating visualizations for patient G77...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient G77...\n",
      "Shared variants VAF comparison saved to: results/G77/G77_shared_variants_vaf.csv\n",
      "Processing patient: G12\n",
      "  Processing sample: F, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 2469 variants in sample F\n",
      "  Processing sample: C, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1916 variants in sample C\n",
      "  Processing sample: E, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1903 variants in sample E\n",
      "  Processing sample: B, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1873 variants in sample B\n",
      "  Total variants for patient G12: 8161\n",
      "Data saved to results/G12/G12_variants.tsv\n",
      "Summary statistics saved to: results/G12\n",
      "Excel workbook with multiple sheets saved to results/G12/G12_variants.xlsx\n",
      "  Generating visualizations for patient G12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient G12...\n",
      "Shared variants VAF comparison saved to: results/G12/G12_shared_variants_vaf.csv\n",
      "Processing patient: M24\n",
      "  Processing sample: F, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 875 variants in sample F\n",
      "  Processing sample: C, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 792 variants in sample C\n",
      "  Processing sample: E, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 713 variants in sample E\n",
      "  Processing sample: B, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 925 variants in sample B\n",
      "  Total variants for patient M24: 3305\n",
      "Data saved to results/M24/M24_variants.tsv\n",
      "Summary statistics saved to: results/M24\n",
      "Excel workbook with multiple sheets saved to results/M24/M24_variants.xlsx\n",
      "  Generating visualizations for patient M24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient M24...\n",
      "Shared variants VAF comparison saved to: results/M24/M24_shared_variants_vaf.csv\n",
      "Processing patient: G23\n",
      "  Processing sample: F, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 870 variants in sample F\n",
      "  Processing sample: H, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 785 variants in sample H\n",
      "  Processing sample: B1, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 883 variants in sample B1\n",
      "  Processing sample: C, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 929 variants in sample C\n",
      "  Processing sample: D, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1182 variants in sample D\n",
      "  Processing sample: B, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 508 variants in sample B\n",
      "  Total variants for patient G23: 5157\n",
      "Data saved to results/G23/G23_variants.tsv\n",
      "Summary statistics saved to: results/G23\n",
      "Excel workbook with multiple sheets saved to results/G23/G23_variants.xlsx\n",
      "  Generating visualizations for patient G23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient G23...\n",
      "Shared variants VAF comparison saved to: results/G23/G23_shared_variants_vaf.csv\n",
      "Processing patient: G40\n",
      "  Processing sample: I, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 527 variants in sample I\n",
      "  Processing sample: G, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 561 variants in sample G\n",
      "  Processing sample: A, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 504 variants in sample A\n",
      "  Processing sample: F, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 550 variants in sample F\n",
      "  Processing sample: M, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 580 variants in sample M\n",
      "  Processing sample: C, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 574 variants in sample C\n",
      "  Processing sample: D, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 562 variants in sample D\n",
      "  Processing sample: E, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 508 variants in sample E\n",
      "  Processing sample: K, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 575 variants in sample K\n",
      "  Total variants for patient G40: 4941\n",
      "Data saved to results/G40/G40_variants.tsv\n",
      "Summary statistics saved to: results/G40\n",
      "Excel workbook with multiple sheets saved to results/G40/G40_variants.xlsx\n",
      "  Generating visualizations for patient G40...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient G40...\n",
      "Shared variants VAF comparison saved to: results/G40/G40_shared_variants_vaf.csv\n",
      "Processing patient: M78\n",
      "  Processing sample: F, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 649 variants in sample F\n",
      "  Processing sample: H, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 708 variants in sample H\n",
      "  Processing sample: J, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 1532 variants in sample J\n",
      "  Processing sample: C, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 1176 variants in sample C\n",
      "  Processing sample: B, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 806 variants in sample B\n",
      "  Total variants for patient M78: 4871\n",
      "Data saved to results/M78/M78_variants.tsv\n",
      "Summary statistics saved to: results/M78\n",
      "Excel workbook with multiple sheets saved to results/M78/M78_variants.xlsx\n",
      "  Generating visualizations for patient M78...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient M78...\n",
      "Shared variants VAF comparison saved to: results/M78/M78_shared_variants_vaf.csv\n",
      "Processing patient: G85\n",
      "  Processing sample: I, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 727 variants in sample I\n",
      "  Processing sample: G, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 1107 variants in sample G\n",
      "  Processing sample: F, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 989 variants in sample F\n",
      "  Processing sample: H, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 1258 variants in sample H\n",
      "  Processing sample: C, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 1038 variants in sample C\n",
      "  Processing sample: D, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 934 variants in sample D\n",
      "  Processing sample: B, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 2363 variants in sample B\n",
      "  Total variants for patient G85: 8416\n",
      "Data saved to results/G85/G85_variants.tsv\n",
      "Summary statistics saved to: results/G85\n",
      "Excel workbook with multiple sheets saved to results/G85/G85_variants.xlsx\n",
      "  Generating visualizations for patient G85...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient G85...\n",
      "Shared variants VAF comparison saved to: results/G85/G85_shared_variants_vaf.csv\n",
      "Processing patient: p182\n",
      "  Processing sample: 112083, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 568 variants in sample 112083\n",
      "  Processing sample: 111761, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 602 variants in sample 111761\n",
      "  Processing sample: 111948, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 322 variants in sample 111948\n",
      "  Processing sample: 11967, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 413 variants in sample 11967\n",
      "  Total variants for patient p182: 1905\n",
      "Data saved to results/p182/p182_variants.tsv\n",
      "Summary statistics saved to: results/p182\n",
      "Excel workbook with multiple sheets saved to results/p182/p182_variants.xlsx\n",
      "  Generating visualizations for patient p182...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient p182...\n",
      "Shared variants VAF comparison saved to: results/p182/p182_shared_variants_vaf.csv\n",
      "Processing patient: P158\n",
      "  Processing sample: 375150, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 534 variants in sample 375150\n",
      "  Processing sample: 375281, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 634 variants in sample 375281\n",
      "  Processing sample: 375266, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 689 variants in sample 375266\n",
      "  Processing sample: 375284, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 534 variants in sample 375284\n",
      "  Processing sample: 375546, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 463 variants in sample 375546\n",
      "  Total variants for patient P158: 2854\n",
      "Data saved to results/P158/P158_variants.tsv\n",
      "Summary statistics saved to: results/P158\n",
      "Excel workbook with multiple sheets saved to results/P158/P158_variants.xlsx\n",
      "  Generating visualizations for patient P158...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient P158...\n",
      "Shared variants VAF comparison saved to: results/P158/P158_shared_variants_vaf.csv\n",
      "Processing patient: G5\n",
      "  Processing sample: I, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 461 variants in sample I\n",
      "  Processing sample: G, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 516 variants in sample G\n",
      "  Processing sample: H, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 541 variants in sample H\n",
      "  Processing sample: C, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 467 variants in sample C\n",
      "  Processing sample: D, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 485 variants in sample D\n",
      "  Processing sample: B, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 497 variants in sample B\n",
      "  Total variants for patient G5: 2967\n",
      "Data saved to results/G5/G5_variants.tsv\n",
      "Summary statistics saved to: results/G5\n",
      "Excel workbook with multiple sheets saved to results/G5/G5_variants.xlsx\n",
      "  Generating visualizations for patient G5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient G5...\n",
      "Shared variants VAF comparison saved to: results/G5/G5_shared_variants_vaf.csv\n",
      "Processing patient: M95\n",
      "  Processing sample: F, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 896 variants in sample F\n",
      "  Processing sample: M, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 781 variants in sample M\n",
      "  Processing sample: C, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 917 variants in sample C\n",
      "  Processing sample: D, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1281 variants in sample D\n",
      "  Processing sample: B, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1029 variants in sample B\n",
      "  Processing sample: L, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 591 variants in sample L\n",
      "  Total variants for patient M95: 5495\n",
      "Data saved to results/M95/M95_variants.tsv\n",
      "Summary statistics saved to: results/M95\n",
      "Excel workbook with multiple sheets saved to results/M95/M95_variants.xlsx\n",
      "  Generating visualizations for patient M95...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient M95...\n",
      "Shared variants VAF comparison saved to: results/M95/M95_shared_variants_vaf.csv\n",
      "Processing patient: G92\n",
      "  Processing sample: G, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 843 variants in sample G\n",
      "  Processing sample: F, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 52570 variants in sample F\n",
      "  Processing sample: M, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1256 variants in sample M\n",
      "  Processing sample: C, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1425 variants in sample C\n",
      "  Processing sample: D, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 737 variants in sample D\n",
      "  Processing sample: K, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 765 variants in sample K\n",
      "  Total variants for patient G92: 57596\n",
      "Data saved to results/G92/G92_variants.tsv\n",
      "Summary statistics saved to: results/G92\n",
      "Excel workbook with multiple sheets saved to results/G92/G92_variants.xlsx\n",
      "  Generating visualizations for patient G92...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient G92...\n",
      "Shared variants VAF comparison saved to: results/G92/G92_shared_variants_vaf.csv\n",
      "Processing patient: G59\n",
      "  Processing sample: I, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 475 variants in sample I\n",
      "  Processing sample: F, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 532 variants in sample F\n",
      "  Processing sample: H, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 491 variants in sample H\n",
      "  Processing sample: J, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 1155 variants in sample J\n",
      "  Processing sample: D, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 1003 variants in sample D\n",
      "  Processing sample: E, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 452 variants in sample E\n",
      "  Processing sample: B, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 480 variants in sample B\n",
      "  Processing sample: K, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 1121 variants in sample K\n",
      "  Processing sample: L, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 502 variants in sample L\n",
      "  Total variants for patient G59: 6211\n",
      "Data saved to results/G59/G59_variants.tsv\n",
      "Summary statistics saved to: results/G59\n",
      "Excel workbook with multiple sheets saved to results/G59/G59_variants.xlsx\n",
      "  Generating visualizations for patient G59...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient G59...\n",
      "Shared variants VAF comparison saved to: results/G59/G59_shared_variants_vaf.csv\n",
      "Processing patient: G87\n",
      "  Processing sample: A, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 665 variants in sample A\n",
      "  Processing sample: H, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1029 variants in sample H\n",
      "  Processing sample: J, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1647 variants in sample J\n",
      "  Processing sample: K, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1088 variants in sample K\n",
      "  Total variants for patient G87: 4429\n",
      "Data saved to results/G87/G87_variants.tsv\n",
      "Summary statistics saved to: results/G87\n",
      "Excel workbook with multiple sheets saved to results/G87/G87_variants.xlsx\n",
      "  Generating visualizations for patient G87...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient G87...\n",
      "Shared variants VAF comparison saved to: results/G87/G87_shared_variants_vaf.csv\n",
      "Processing patient: p065\n",
      "  Processing sample: 359303, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 484 variants in sample 359303\n",
      "  Processing sample: 359319, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 481 variants in sample 359319\n",
      "  Processing sample: 359306, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 651 variants in sample 359306\n",
      "  Processing sample: 358799, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 462 variants in sample 358799\n",
      "  Processing sample: 359322, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 35428 variants in sample 359322\n",
      "  Processing sample: 358802, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 332 variants in sample 358802\n",
      "  Total variants for patient p065: 37838\n",
      "Data saved to results/p065/p065_variants.tsv\n",
      "Summary statistics saved to: results/p065\n",
      "Excel workbook with multiple sheets saved to results/p065/p065_variants.xlsx\n",
      "  Generating visualizations for patient p065...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient p065...\n",
      "Shared variants VAF comparison saved to: results/p065/p065_shared_variants_vaf.csv\n",
      "Processing patient: G45\n",
      "  Processing sample: H, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 626 variants in sample H\n",
      "  Processing sample: M, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 604 variants in sample M\n",
      "  Processing sample: B, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1247 variants in sample B\n",
      "  Processing sample: K, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 724 variants in sample K\n",
      "  Processing sample: L, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 605 variants in sample L\n",
      "  Total variants for patient G45: 3806\n",
      "Data saved to results/G45/G45_variants.tsv\n",
      "Summary statistics saved to: results/G45\n",
      "Excel workbook with multiple sheets saved to results/G45/G45_variants.xlsx\n",
      "  Generating visualizations for patient G45...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient G45...\n",
      "Shared variants VAF comparison saved to: results/G45/G45_shared_variants_vaf.csv\n",
      "Processing patient: G43\n",
      "  Processing sample: R, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 1807 variants in sample R\n",
      "  Processing sample: I, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 881 variants in sample I\n",
      "  Processing sample: T, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 781 variants in sample T\n",
      "  Processing sample: C, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 831 variants in sample C\n",
      "  Processing sample: P, file: somatic.XY.snvs.pass.vcf\n",
      "    Found 548 variants in sample P\n",
      "  Total variants for patient G43: 4848\n",
      "Data saved to results/G43/G43_variants.tsv\n",
      "Summary statistics saved to: results/G43\n",
      "Excel workbook with multiple sheets saved to results/G43/G43_variants.xlsx\n",
      "  Generating visualizations for patient G43...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient G43...\n",
      "Shared variants VAF comparison saved to: results/G43/G43_shared_variants_vaf.csv\n",
      "Processing patient: p129\n",
      "  Processing sample: 376176, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 589 variants in sample 376176\n",
      "  Processing sample: 376050, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 458 variants in sample 376050\n",
      "  Processing sample: 375688, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 630 variants in sample 375688\n",
      "  Processing sample: 376304, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 602 variants in sample 376304\n",
      "  Processing sample: 376165, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 543 variants in sample 376165\n",
      "  Processing sample: 375970, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 448 variants in sample 375970\n",
      "  Processing sample: 376085, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 506 variants in sample 376085\n",
      "  Processing sample: 375973, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 649 variants in sample 375973\n",
      "  Processing sample: 375918, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 533 variants in sample 375918\n",
      "  Processing sample: 376167, file: somatic.XX.snvs.pass.vcf\n",
      "    Found 534 variants in sample 376167\n",
      "  Total variants for patient p129: 5492\n",
      "Data saved to results/p129/p129_variants.tsv\n",
      "Summary statistics saved to: results/p129\n",
      "Excel workbook with multiple sheets saved to results/p129/p129_variants.xlsx\n",
      "  Generating visualizations for patient p129...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/839645181.py:308: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comparing samples for patient p129...\n",
      "Shared variants VAF comparison saved to: results/p129/p129_shared_variants_vaf.csv\n",
      "\n",
      "Processed 28 patients:\n",
      "  - p191: 1585 variants across 4 samples\n",
      "  - P163: 3850 variants across 6 samples\n",
      "  - P190: 3483 variants across 5 samples\n",
      "  - p152: 4215 variants across 7 samples\n",
      "  - M36: 5331 variants across 7 samples\n",
      "  - G53: 7620 variants across 9 samples\n",
      "  - G39: 3851 variants across 6 samples\n",
      "  - G97: 32021 variants across 8 samples\n",
      "  - G52: 5098 variants across 10 samples\n",
      "  - G99: 45509 variants across 4 samples\n",
      "  - G77: 8844 variants across 8 samples\n",
      "  - G12: 8161 variants across 4 samples\n",
      "  - M24: 3305 variants across 4 samples\n",
      "  - G23: 5157 variants across 6 samples\n",
      "  - G40: 4941 variants across 9 samples\n",
      "  - M78: 4871 variants across 5 samples\n",
      "  - G85: 8416 variants across 7 samples\n",
      "  - p182: 1905 variants across 4 samples\n",
      "  - P158: 2854 variants across 5 samples\n",
      "  - G5: 2967 variants across 6 samples\n",
      "  - M95: 5495 variants across 6 samples\n",
      "  - G92: 57596 variants across 6 samples\n",
      "  - G59: 6211 variants across 9 samples\n",
      "  - G87: 4429 variants across 4 samples\n",
      "  - p065: 37838 variants across 6 samples\n",
      "  - G45: 3806 variants across 5 samples\n",
      "  - G43: 4848 variants across 5 samples\n",
      "  - p129: 5492 variants across 10 samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set directories\n",
    "DATA_DIR = 'data'\n",
    "RESULTS_DIR = 'results'\n",
    "\n",
    "def parse_strelka2_vcf(vcf_file, sample_name=None, patient_id=None):\n",
    "    variants = []\n",
    "    \n",
    "    with open(vcf_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "                \n",
    "            fields = line.strip().split('\\t')\n",
    "            \n",
    "            if len(fields) < 10:\n",
    "                continue\n",
    "                \n",
    "            chrom = fields[0]\n",
    "            pos = int(fields[1])\n",
    "            ref = fields[3]\n",
    "            alt = fields[4]\n",
    "            info = fields[7]\n",
    "            format_fields = fields[8].split(':')\n",
    "            normal_values = fields[9].split(':')\n",
    "            tumor_values = fields[10].split(':')\n",
    "            \n",
    "            normal_data = dict(zip(format_fields, normal_values))\n",
    "            tumor_data = dict(zip(format_fields, tumor_values))\n",
    "            \n",
    "            nucleotide_to_field = {'A': 'AU', 'C': 'CU', 'G': 'GU', 'T': 'TU'}\n",
    "            \n",
    "            try:\n",
    "                ref_counts_index = format_fields.index(nucleotide_to_field[ref])\n",
    "                alt_counts_index = format_fields.index(nucleotide_to_field[alt])\n",
    "            except (ValueError, KeyError):\n",
    "                continue\n",
    "                \n",
    "            tumor_ref_counts = tumor_values[ref_counts_index].split(',')\n",
    "            tumor_alt_counts = tumor_values[alt_counts_index].split(',')\n",
    "            \n",
    "            tier1_ref_counts = int(tumor_ref_counts[0])\n",
    "            tier1_alt_counts = int(tumor_alt_counts[0])\n",
    "            \n",
    "            total_counts = tier1_ref_counts + tier1_alt_counts\n",
    "            vaf = tier1_alt_counts / total_counts if total_counts > 0 else 0\n",
    "            \n",
    "            tumor_depth = int(tumor_data.get('DP', 0))\n",
    "            is_somatic = \"SOMATIC\" in info\n",
    "            \n",
    "            variant_data = {\n",
    "                'PATIENT_ID': patient_id,\n",
    "                'SAMPLE': sample_name,\n",
    "                'CHROM': chrom,\n",
    "                'POS': pos,\n",
    "                'REF': ref,\n",
    "                'ALT': alt,\n",
    "                'REF_COUNT': tier1_ref_counts,\n",
    "                'ALT_COUNT': tier1_alt_counts,\n",
    "                'TOTAL_COUNT': total_counts,\n",
    "                'VAF': round(vaf, 4),\n",
    "                'TUMOR_DEPTH': tumor_depth,\n",
    "                'SOMATIC': is_somatic\n",
    "            }\n",
    "            \n",
    "            variants.append(variant_data)\n",
    "    \n",
    "    variants_df = pd.DataFrame(variants)\n",
    "    return variants_df\n",
    "\n",
    "def find_vcf_files(data_dir):\n",
    "    vcf_files = []\n",
    "    \n",
    "    for patient_dir in os.listdir(data_dir):\n",
    "        patient_path = os.path.join(data_dir, patient_dir)\n",
    "        \n",
    "        if os.path.isdir(patient_path):\n",
    "            patient_id = patient_dir\n",
    "            \n",
    "            for sample_dir in os.listdir(patient_path):\n",
    "                sample_path = os.path.join(patient_path, sample_dir)\n",
    "                \n",
    "                if os.path.isdir(sample_path):\n",
    "                    sample_id = sample_dir\n",
    "                    \n",
    "                    for filename in os.listdir(sample_path):\n",
    "                        if filename.endswith(\"snvs.pass.vcf\"):\n",
    "                            vcf_path = os.path.join(sample_path, filename)\n",
    "                            \n",
    "                            vcf_files.append({\n",
    "                                'patient_id': patient_id,\n",
    "                                'sample_id': sample_id,\n",
    "                                'vcf_path': vcf_path\n",
    "                            })\n",
    "    \n",
    "    return vcf_files\n",
    "\n",
    "def create_manhattan_plot(data, by_sample=True, output_dir=None, patient_id=None):\n",
    "    multiple_samples = 'SAMPLE' in data.columns and len(data['SAMPLE'].unique()) > 1\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    data_copy = data.copy()\n",
    "    data_copy['CHROM_CLEAN'] = data_copy['CHROM'].str.replace('chr', '')\n",
    "    \n",
    "    def chrom_to_num(chrom):\n",
    "        if chrom in ['X', 'Y', 'M']:\n",
    "            return {'X': 23, 'Y': 24, 'M': 25}[chrom]\n",
    "        try:\n",
    "            return int(chrom)\n",
    "        except ValueError:\n",
    "            return 999\n",
    "    \n",
    "    data_copy['CHROM_NUM'] = data_copy['CHROM_CLEAN'].apply(chrom_to_num)\n",
    "    data_copy = data_copy.sort_values(['CHROM_NUM', 'POS'])\n",
    "    \n",
    "    chromosomes = data_copy['CHROM'].unique()\n",
    "    chrom_sizes = {}\n",
    "    \n",
    "    for chrom in chromosomes:\n",
    "        chrom_data = data_copy[data_copy['CHROM'] == chrom]\n",
    "        if not chrom_data.empty:\n",
    "            chrom_sizes[chrom] = chrom_data['POS'].max()\n",
    "    \n",
    "    chrom_pos = {}\n",
    "    cum_pos = 0\n",
    "    \n",
    "    for chrom in sorted(chromosomes, key=lambda x: data_copy[data_copy['CHROM'] == x]['CHROM_NUM'].iloc[0]):\n",
    "        chrom_pos[chrom] = cum_pos\n",
    "        cum_pos += chrom_sizes.get(chrom, 0) + 5000000\n",
    "    \n",
    "    data_copy['GENOME_POS'] = data_copy.apply(lambda row: chrom_pos[row['CHROM']] + row['POS'], axis=1)\n",
    "    \n",
    "    if multiple_samples and by_sample:\n",
    "        samples = data_copy['SAMPLE'].unique()\n",
    "        palette = sns.color_palette(\"tab10\", n_colors=len(samples))\n",
    "        sample_colors = dict(zip(samples, palette))\n",
    "        \n",
    "        for sample, color in sample_colors.items():\n",
    "            sample_data = data_copy[data_copy['SAMPLE'] == sample]\n",
    "            plt.scatter(sample_data['GENOME_POS'], sample_data['VAF'], \n",
    "                      color=color, alpha=0.7, s=10, label=sample)\n",
    "        \n",
    "        plt.legend(title='Sample', loc='upper right')\n",
    "    else:\n",
    "        for i, chrom in enumerate(sorted(chromosomes, key=lambda x: data_copy[data_copy['CHROM'] == x]['CHROM_NUM'].iloc[0])):\n",
    "            chrom_data = data_copy[data_copy['CHROM'] == chrom]\n",
    "            if not chrom_data.empty:\n",
    "                plt.scatter(chrom_data['GENOME_POS'], chrom_data['VAF'], \n",
    "                          color=f'C{i % 10}', alpha=0.7, s=10, label=chrom)\n",
    "    \n",
    "    chrom_midpoints = {}\n",
    "    for chrom in chromosomes:\n",
    "        chrom_data = data_copy[data_copy['CHROM'] == chrom]\n",
    "        if not chrom_data.empty:\n",
    "            midpoint = chrom_pos[chrom] + chrom_sizes.get(chrom, 0) / 2\n",
    "            chrom_midpoints[chrom] = midpoint\n",
    "    \n",
    "    plt.xticks([midpoint for midpoint in chrom_midpoints.values()], \n",
    "              [chrom for chrom in chrom_midpoints.keys()],\n",
    "              rotation=45)\n",
    "    \n",
    "    plt.ylabel('VAF')\n",
    "    plt.title('VAF Distribution Across the Genome')\n",
    "    plt.axhline(y=0.5, color='r', linestyle='--', label='VAF = 0.5')\n",
    "    \n",
    "    if not (multiple_samples and by_sample):\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        if len(handles) > 10:\n",
    "            handles = handles[:10]\n",
    "            labels = labels[:10]\n",
    "            labels[-1] += \"...\"\n",
    "        \n",
    "        plt.legend(handles, labels, loc='upper right', title='Chromosomes')\n",
    "    \n",
    "    plt.xlim(0, cum_pos)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        fig_file = os.path.join(output_dir, f\"{patient_id}_manhattan_plot.png\")\n",
    "        plt.savefig(fig_file, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def visualize_vafs(data, by_sample=True, output_dir=None, patient_id=None):\n",
    "    multiple_samples = 'SAMPLE' in data.columns and len(data['SAMPLE'].unique()) > 1\n",
    "    \n",
    "    if multiple_samples and by_sample:\n",
    "        samples = data['SAMPLE'].unique()\n",
    "        palette = sns.color_palette(\"tab10\", n_colors=len(samples))\n",
    "        sample_colors = dict(zip(samples, palette))\n",
    "    \n",
    "    # Save each plot separately\n",
    "    \n",
    "    # 1. VAF Distribution Histogram\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if multiple_samples and by_sample:\n",
    "        for sample, color in sample_colors.items():\n",
    "            sample_data = data[data['SAMPLE'] == sample]\n",
    "            sns.histplot(sample_data['VAF'], bins=20, kde=True, \n",
    "                         label=sample, color=color, alpha=0.5)\n",
    "        plt.legend(title='Sample')\n",
    "    else:\n",
    "        sns.histplot(data['VAF'], bins=20, kde=True)\n",
    "    \n",
    "    plt.xlabel('Variant Allele Fraction (VAF)')\n",
    "    plt.ylabel('Number of Variants')\n",
    "    plt.title('Distribution of Variant Allele Fractions')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        fig_file = os.path.join(output_dir, f\"{patient_id}_vaf_histogram.png\")\n",
    "        plt.savefig(fig_file, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # 2. Scatter plot of VAF vs Read Depth\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if multiple_samples and by_sample:\n",
    "        for sample, color in sample_colors.items():\n",
    "            sample_data = data[data['SAMPLE'] == sample]\n",
    "            plt.scatter(sample_data['TUMOR_DEPTH'], sample_data['VAF'], \n",
    "                       alpha=0.5, color=color, label=sample)\n",
    "        plt.legend(title='Sample')\n",
    "    else:\n",
    "        plt.scatter(data['TUMOR_DEPTH'], data['VAF'], alpha=0.5)\n",
    "    \n",
    "    plt.xlabel('Tumor Read Depth')\n",
    "    plt.ylabel('VAF')\n",
    "    plt.title('VAF vs. Tumor Read Depth')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        fig_file = os.path.join(output_dir, f\"{patient_id}_depth_vs_vaf.png\")\n",
    "        plt.savefig(fig_file, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # 3. VAF by chromosome (boxplot)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    chrom_counts = data['CHROM'].value_counts()\n",
    "    top_chroms = chrom_counts.index[:10].tolist()\n",
    "    chrom_data = data[data['CHROM'].isin(top_chroms)]\n",
    "    \n",
    "    if multiple_samples and by_sample:\n",
    "        sns.boxplot(x='CHROM', y='VAF', hue='SAMPLE', data=chrom_data, palette=sample_colors)\n",
    "        plt.legend(title='Sample', loc='upper right')\n",
    "    else:\n",
    "        sns.boxplot(x='CHROM', y='VAF', data=chrom_data)\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('VAF Distribution by Chromosome')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        fig_file = os.path.join(output_dir, f\"{patient_id}_chrom_boxplot.png\")\n",
    "        plt.savefig(fig_file, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # 4. Alt Count vs. Total Count scatter plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    if multiple_samples and by_sample:\n",
    "        for sample, color in sample_colors.items():\n",
    "            sample_data = data[data['SAMPLE'] == sample]\n",
    "            plt.scatter(sample_data['TOTAL_COUNT'], sample_data['ALT_COUNT'], \n",
    "                       alpha=0.7, color=color, label=sample)\n",
    "        plt.legend(title='Sample')\n",
    "    else:\n",
    "        scatter = plt.scatter(data['TOTAL_COUNT'], data['ALT_COUNT'], \n",
    "                             c=data['VAF'], cmap='viridis', \n",
    "                             alpha=0.7)\n",
    "        plt.colorbar(scatter, label='VAF')\n",
    "    \n",
    "    max_count = max(data['TOTAL_COUNT'])\n",
    "    plt.plot([0, max_count], [0, max_count/2], 'r--', label='VAF = 0.5')\n",
    "    \n",
    "    plt.xlabel('Total Read Count')\n",
    "    plt.ylabel('Alternate Allele Count')\n",
    "    plt.title('Alternate vs. Total Read Count')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        fig_file = os.path.join(output_dir, f\"{patient_id}_alt_vs_total.png\")\n",
    "        plt.savefig(fig_file, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # 5. Create VIOLIN plot for VAF distribution by sample\n",
    "    if multiple_samples:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Use violin plot instead of boxplot\n",
    "        sns.violinplot(x='SAMPLE', y='VAF', data=data, palette=sample_colors if by_sample else None,\n",
    "                     inner='quartile')\n",
    "        \n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title('VAF Distribution by Sample (Violin Plot)')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if output_dir:\n",
    "            fig_file = os.path.join(output_dir, f\"{patient_id}_sample_violin.png\")\n",
    "            plt.savefig(fig_file, dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    # 6. Create Manhattan plot\n",
    "    create_manhattan_plot(data, by_sample=by_sample, output_dir=output_dir, patient_id=patient_id)\n",
    "\n",
    "def generate_summary_stats(data, output_dir=None, patient_id=None):\n",
    "    \"\"\"\n",
    "    Generate and save summary statistics for the VAF data\n",
    "    This is a renamed function to avoid confusion with any other functions\n",
    "    \"\"\"\n",
    "    # Overall stats\n",
    "    overall_stats = {\n",
    "        \"Total variants\": len(data),\n",
    "        \"Mean VAF\": data['VAF'].mean(),\n",
    "        \"Median VAF\": data['VAF'].median(),\n",
    "        \"Min VAF\": data['VAF'].min(),\n",
    "        \"Max VAF\": data['VAF'].max()\n",
    "    }\n",
    "    \n",
    "    overall_df = pd.DataFrame([overall_stats])\n",
    "    \n",
    "    # Stats by sample if multiple samples\n",
    "    if 'SAMPLE' in data.columns and len(data['SAMPLE'].unique()) > 1:\n",
    "        sample_summary = data.groupby('SAMPLE').agg({\n",
    "            'VAF': ['count', 'mean', 'median', 'min', 'max'],\n",
    "            'TUMOR_DEPTH': ['mean', 'median']\n",
    "        })\n",
    "        sample_summary.columns = ['Variant Count', 'Mean VAF', 'Median VAF', 'Min VAF', 'Max VAF', \n",
    "                               'Mean Depth', 'Median Depth']\n",
    "        sample_summary = sample_summary.round(4)\n",
    "        \n",
    "        # Chromosome distribution\n",
    "        chrom_counts = data['CHROM'].value_counts().reset_index()\n",
    "        chrom_counts.columns = ['Chromosome', 'Count']\n",
    "        \n",
    "        # VAF range distribution\n",
    "        vaf_ranges = [\n",
    "            (0.0, 0.1), (0.1, 0.2), (0.2, 0.3), (0.3, 0.4),\n",
    "            (0.4, 0.5), (0.5, 0.6), (0.6, 1.0)\n",
    "        ]\n",
    "        \n",
    "        vaf_range_data = []\n",
    "        for vaf_min, vaf_max in vaf_ranges:\n",
    "            count = ((data['VAF'] >= vaf_min) & (data['VAF'] < vaf_max)).sum()\n",
    "            percent = (count / len(data)) * 100\n",
    "            vaf_range_data.append({\n",
    "                'Range': f\"{vaf_min:.1f}-{vaf_max:.1f}\",\n",
    "                'Count': count,\n",
    "                'Percentage': f\"{percent:.1f}%\"\n",
    "            })\n",
    "        \n",
    "        vaf_range_df = pd.DataFrame(vaf_range_data)\n",
    "        \n",
    "        # Save to CSV files if output directory is provided\n",
    "        if output_dir:\n",
    "            overall_stats_file = os.path.join(output_dir, f\"{patient_id}_overall_stats.csv\")\n",
    "            sample_summary_file = os.path.join(output_dir, f\"{patient_id}_sample_summary.csv\")\n",
    "            chrom_file = os.path.join(output_dir, f\"{patient_id}_chromosome_counts.csv\")\n",
    "            vaf_range_file = os.path.join(output_dir, f\"{patient_id}_vaf_ranges.csv\")\n",
    "            \n",
    "            overall_df.to_csv(overall_stats_file, index=False)\n",
    "            sample_summary.to_csv(sample_summary_file)\n",
    "            chrom_counts.to_csv(chrom_file, index=False)\n",
    "            vaf_range_df.to_csv(vaf_range_file, index=False)\n",
    "            \n",
    "            print(f\"Summary statistics saved to: {output_dir}\")\n",
    "        else:\n",
    "            # Display in notebook\n",
    "            print(\"Overall Statistics:\")\n",
    "            display(overall_df)\n",
    "            \n",
    "            print(\"\\nSample Summary:\")\n",
    "            display(sample_summary)\n",
    "            \n",
    "            print(\"\\nChromosome Distribution:\")\n",
    "            display(chrom_counts)\n",
    "            \n",
    "            print(\"\\nVAF Range Distribution:\")\n",
    "            display(vaf_range_df)\n",
    "    \n",
    "    return {\n",
    "        'overall': overall_df,\n",
    "        'sample_summary': sample_summary if 'sample_summary' in locals() else None,\n",
    "        'chrom_counts': chrom_counts if 'chrom_counts' in locals() else None,\n",
    "        'vaf_ranges': vaf_range_df if 'vaf_range_df' in locals() else None\n",
    "    }\n",
    "\n",
    "def compare_samples_shared_variants(data, output_dir=None, patient_id=None):\n",
    "    \"\"\"\n",
    "    Compare variants shared between samples\n",
    "    \"\"\"\n",
    "    if 'SAMPLE' not in data.columns or len(data['SAMPLE'].unique()) <= 1:\n",
    "        print(\"This function requires data from multiple samples\")\n",
    "        return\n",
    "    \n",
    "    data['VARIANT_ID'] = data['CHROM'] + '_' + data['POS'].astype(str) + '_' + data['REF'] + '_' + data['ALT']\n",
    "    \n",
    "    variant_counts = data['VARIANT_ID'].value_counts().reset_index()\n",
    "    variant_counts.columns = ['VARIANT_ID', 'SAMPLE_COUNT']\n",
    "    \n",
    "    shared_data = pd.merge(data, variant_counts, on='VARIANT_ID')\n",
    "    \n",
    "    samples = data['SAMPLE'].unique()\n",
    "    num_samples = len(samples)\n",
    "    \n",
    "    # Create summary dataframe for shared variants\n",
    "    shared_summary = []\n",
    "    for i in range(1, num_samples + 1):\n",
    "        count = (shared_data['SAMPLE_COUNT'] == i).sum() // i  # Divide by i because each variant appears i times\n",
    "        percent = (count / len(variant_counts)) * 100\n",
    "        \n",
    "        if i == 1:\n",
    "            category = \"Unique to one sample\"\n",
    "        elif i == num_samples:\n",
    "            category = f\"Shared across all {num_samples} samples\"\n",
    "        else:\n",
    "            category = f\"Shared across {i} samples\"\n",
    "            \n",
    "        shared_summary.append({\n",
    "            'Category': category,\n",
    "            'Count': count,\n",
    "            'Percentage': f\"{percent:.1f}%\"\n",
    "        })\n",
    "    \n",
    "    shared_summary_df = pd.DataFrame(shared_summary)\n",
    "    \n",
    "    # Save or display\n",
    "    if output_dir:\n",
    "        summary_file = os.path.join(output_dir, f\"{patient_id}_shared_variants_summary.csv\")\n",
    "        shared_summary_df.to_csv(summary_file, index=False)\n",
    "    else:\n",
    "        print(\"Shared Variants Summary:\")\n",
    "        display(shared_summary_df)\n",
    "    \n",
    "    if num_samples >= 2:\n",
    "        shared_variants = variant_counts[variant_counts['SAMPLE_COUNT'] >= 2]['VARIANT_ID'].tolist()\n",
    "        \n",
    "        if shared_variants:\n",
    "            shared_data = data[data['VARIANT_ID'].isin(shared_variants)]\n",
    "            \n",
    "            pivot_vaf = shared_data.pivot_table(\n",
    "                index='VARIANT_ID', \n",
    "                columns='SAMPLE', \n",
    "                values='VAF', \n",
    "                aggfunc='mean'\n",
    "            ).reset_index()\n",
    "            \n",
    "            if output_dir:\n",
    "                pivot_file = os.path.join(output_dir, f\"{patient_id}_shared_variants_vaf.csv\")\n",
    "                pivot_vaf.to_csv(pivot_file)\n",
    "                print(f\"Shared variants VAF comparison saved to: {pivot_file}\")\n",
    "            else:\n",
    "                print(\"\\nSample of shared variants VAF comparison:\")\n",
    "                display(pivot_vaf.head(10))\n",
    "            \n",
    "            if num_samples > 1:\n",
    "                plt.figure(figsize=(15, 10))\n",
    "                plot_idx = 1\n",
    "                \n",
    "                for i in range(num_samples):\n",
    "                    for j in range(i+1, num_samples):\n",
    "                        sample1 = samples[i]\n",
    "                        sample2 = samples[j]\n",
    "                        \n",
    "                        pair_data = pivot_vaf[[sample1, sample2]].dropna()\n",
    "                        \n",
    "                        if len(pair_data) > 0:\n",
    "                            corr = pair_data[sample1].corr(pair_data[sample2])\n",
    "                            \n",
    "                            plt.subplot(num_samples-1, num_samples-1, plot_idx)\n",
    "                            plt.scatter(pair_data[sample1], pair_data[sample2], alpha=0.5)\n",
    "                            plt.plot([0, 1], [0, 1], 'r--')  # Identity line\n",
    "                            plt.xlabel(f'{sample1} VAF')\n",
    "                            plt.ylabel(f'{sample2} VAF')\n",
    "                            plt.title(f'Correlation: {corr:.3f}')\n",
    "                            plt.axis('square')\n",
    "                            plt.xlim(0, 1)\n",
    "                            plt.ylim(0, 1)\n",
    "                        \n",
    "                        plot_idx += 1\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                if output_dir:\n",
    "                    fig_file = os.path.join(output_dir, f\"{patient_id}_vaf_correlations.png\")\n",
    "                    plt.savefig(fig_file, dpi=300)\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    plt.show()\n",
    "        else:\n",
    "            print(\"No variants are shared between samples\")\n",
    "    \n",
    "    return shared_summary_df\n",
    "\n",
    "def save_results(data, output_dir, base_filename):\n",
    "    \"\"\"\n",
    "    Save the parsed data to TSV format\n",
    "    \"\"\"\n",
    "    # Save to TSV (full dataset with all columns)\n",
    "    tsv_path = os.path.join(output_dir, f\"{base_filename}.tsv\")\n",
    "    data.to_csv(tsv_path, sep='\\t', index=False)\n",
    "    print(f\"Data saved to {tsv_path}\")\n",
    "    \n",
    "    # Generate and save summary statistics\n",
    "    stats = generate_summary_stats(data, output_dir=output_dir, patient_id=base_filename.split('_')[0])\n",
    "    \n",
    "    # Save to Excel if more than one sample\n",
    "    if 'SAMPLE' in data.columns and len(data['SAMPLE'].unique()) > 1:\n",
    "        excel_path = os.path.join(output_dir, f\"{base_filename}.xlsx\")\n",
    "        \n",
    "        with pd.ExcelWriter(excel_path) as writer:\n",
    "            data.to_excel(writer, sheet_name='All Variants', index=False)\n",
    "            \n",
    "            # Add samples to separate sheets\n",
    "            for sample in data['SAMPLE'].unique():\n",
    "                sample_data = data[data['SAMPLE'] == sample]\n",
    "                sample_data.to_excel(writer, sheet_name=sample, index=False)\n",
    "                \n",
    "            # Add summary sheets\n",
    "            if stats['overall'] is not None:\n",
    "                stats['overall'].to_excel(writer, sheet_name='Overall_Stats', index=False)\n",
    "            if stats['sample_summary'] is not None:\n",
    "                stats['sample_summary'].to_excel(writer, sheet_name='Sample_Summary')\n",
    "            if stats['vaf_ranges'] is not None:\n",
    "                stats['vaf_ranges'].to_excel(writer, sheet_name='VAF_Ranges', index=False)\n",
    "        \n",
    "        print(f\"Excel workbook with multiple sheets saved to {excel_path}\")\n",
    "    \n",
    "    return {\n",
    "        'full_path': tsv_path,\n",
    "        'excel_path': excel_path if 'excel_path' in locals() else None\n",
    "    }\n",
    "\n",
    "def process_patient_vcfs(data_dir=DATA_DIR, results_dir=RESULTS_DIR, patient_id=None):\n",
    "    \"\"\"\n",
    "    Process VCF files for a specific patient or all patients\n",
    "    \"\"\"\n",
    "    # Find all VCF files\n",
    "    all_vcf_files = find_vcf_files(data_dir)\n",
    "    \n",
    "    # Filter by patient if specified\n",
    "    if patient_id:\n",
    "        vcf_files = [f for f in all_vcf_files if f['patient_id'] == patient_id]\n",
    "    else:\n",
    "        vcf_files = all_vcf_files\n",
    "    \n",
    "    # Group by patient\n",
    "    patient_vcfs = {}\n",
    "    for vcf_file in vcf_files:\n",
    "        patient = vcf_file['patient_id']\n",
    "        if patient not in patient_vcfs:\n",
    "            patient_vcfs[patient] = []\n",
    "        patient_vcfs[patient].append(vcf_file)\n",
    "    \n",
    "    # Process each patient's VCF files\n",
    "    patient_variants = {}\n",
    "    \n",
    "    for patient, files in patient_vcfs.items():\n",
    "        print(f\"Processing patient: {patient}\")\n",
    "        \n",
    "        # Create results directory for this patient\n",
    "        patient_results_dir = os.path.join(results_dir, patient)\n",
    "        \n",
    "        all_patient_variants = []\n",
    "        \n",
    "        # Process each sample for this patient\n",
    "        for file_info in files:\n",
    "            vcf_path = file_info['vcf_path']\n",
    "            sample_id = file_info['sample_id']\n",
    "            \n",
    "            print(f\"  Processing sample: {sample_id}, file: {os.path.basename(vcf_path)}\")\n",
    "            \n",
    "            try:\n",
    "                variants_df = parse_strelka2_vcf(vcf_path, sample_name=sample_id, patient_id=patient)\n",
    "                all_patient_variants.append(variants_df)\n",
    "                print(f\"    Found {len(variants_df)} variants in sample {sample_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error processing {vcf_path}: {e}\")\n",
    "        \n",
    "        # Combine all variants for this patient\n",
    "        if all_patient_variants:\n",
    "            combined_df = pd.concat(all_patient_variants, ignore_index=True)\n",
    "            print(f\"  Total variants for patient {patient}: {len(combined_df)}\")\n",
    "            \n",
    "            # Save the variants\n",
    "            save_results(combined_df, patient_results_dir, f\"{patient}_variants\")\n",
    "            \n",
    "            # Generate and save visualizations\n",
    "            print(f\"  Generating visualizations for patient {patient}...\")\n",
    "            visualize_vafs(combined_df, by_sample=True, output_dir=patient_results_dir, patient_id=patient)\n",
    "            \n",
    "            # Compare samples if there are multiple\n",
    "            if 'SAMPLE' in combined_df.columns and len(combined_df['SAMPLE'].unique()) > 1:\n",
    "                print(f\"  Comparing samples for patient {patient}...\")\n",
    "                compare_samples_shared_variants(combined_df, output_dir=patient_results_dir, patient_id=patient)\n",
    "            \n",
    "            # Save the data for return\n",
    "            patient_variants[patient] = combined_df\n",
    "        else:\n",
    "            print(f\"  No variants found for patient {patient}\")\n",
    "    \n",
    "    return patient_variants\n",
    "\n",
    "# For use in Jupyter Notebook - run this cell to process all patients\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all patients or specific patient\n",
    "    # To process a specific patient, use: process_patient_vcfs(patient_id='G12')\n",
    "    all_patient_variants = process_patient_vcfs()\n",
    "    \n",
    "    print(f\"\\nProcessed {len(all_patient_variants)} patients:\")\n",
    "    for patient, variants in all_patient_variants.items():\n",
    "        sample_count = len(variants['SAMPLE'].unique())\n",
    "        print(f\"  - {patient}: {len(variants)} variants across {sample_count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyClone-VI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the VAF data into the format expected by PyClone-VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding patient variant files in results\n",
      "Found variant file for patient p191: results/p191/p191_variants.tsv\n",
      "Found variant file for patient P163: results/P163/P163_variants.tsv\n",
      "Found variant file for patient P190: results/P190/P190_variants.tsv\n",
      "Found variant file for patient p152: results/p152/p152_variants.tsv\n",
      "Found variant file for patient M36: results/M36/M36_variants.tsv\n",
      "Found variant file for patient G53: results/G53/G53_variants.tsv\n",
      "Found variant file for patient G39: results/G39/G39_variants.tsv\n",
      "Found variant file for patient G97: results/G97/G97_variants.tsv\n",
      "Found variant file for patient G52: results/G52/G52_variants.tsv\n",
      "Found variant file for patient G99: results/G99/G99_variants.tsv\n",
      "Found variant file for patient G77: results/G77/G77_variants.tsv\n",
      "Found variant file for patient G12: results/G12/G12_variants.tsv\n",
      "Found variant file for patient M24: results/M24/M24_variants.tsv\n",
      "Found variant file for patient G23: results/G23/G23_variants.tsv\n",
      "Found variant file for patient G40: results/G40/G40_variants.tsv\n",
      "Found variant file for patient M78: results/M78/M78_variants.tsv\n",
      "Found variant file for patient G85: results/G85/G85_variants.tsv\n",
      "Found variant file for patient p182: results/p182/p182_variants.tsv\n",
      "Found variant file for patient P158: results/P158/P158_variants.tsv\n",
      "Found variant file for patient G5: results/G5/G5_variants.tsv\n",
      "Found variant file for patient M95: results/M95/M95_variants.tsv\n",
      "Found variant file for patient G92: results/G92/G92_variants.tsv\n",
      "Found variant file for patient G59: results/G59/G59_variants.tsv\n",
      "Found variant file for patient G87: results/G87/G87_variants.tsv\n",
      "Found variant file for patient p065: results/p065/p065_variants.tsv\n",
      "Found variant file for patient G45: results/G45/G45_variants.tsv\n",
      "Found variant file for patient G43: results/G43/G43_variants.tsv\n",
      "Found variant file for patient p129: results/p129/p129_variants.tsv\n",
      "Processing patient p191...\n",
      "  Loaded 1585 variants\n",
      "  Filtered from 1585 to 1585 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Error processing patient p191: sequence item 0: expected str instance, numpy.int64 found\n",
      "Processing patient P163...\n",
      "  Loaded 3850 variants\n",
      "  Filtered from 3850 to 3850 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Error processing patient P163: sequence item 0: expected str instance, numpy.int64 found\n",
      "Processing patient P190...\n",
      "  Loaded 3483 variants\n",
      "  Filtered from 3483 to 3483 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Error processing patient P190: sequence item 0: expected str instance, numpy.int64 found\n",
      "Processing patient p152...\n",
      "  Loaded 4215 variants\n",
      "  Filtered from 4215 to 4215 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Error processing patient p152: sequence item 0: expected str instance, numpy.int64 found\n",
      "Processing patient M36...\n",
      "  Loaded 5331 variants\n",
      "  Filtered from 5331 to 5331 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 7 samples: R, N, T, F, E, K, P\n",
      "  Processing sample R: 605 variants\n",
      "  Processing sample N: 520 variants\n",
      "  Processing sample T: 951 variants\n",
      "  Processing sample F: 1265 variants\n",
      "  Processing sample E: 965 variants\n",
      "  Processing sample K: 591 variants\n",
      "  Processing sample P: 434 variants\n",
      "  Saved combined PyClone input for patient M36 to results/M36/M36_pyclone_input.tsv\n",
      "Processing patient G53...\n",
      "  Loaded 7620 variants\n",
      "  Filtered from 7620 to 7620 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 9 samples: I, A, F, J, C, D, E, B, K\n",
      "  Processing sample I: 957 variants\n",
      "  Processing sample A: 529 variants\n",
      "  Processing sample F: 764 variants\n",
      "  Processing sample J: 799 variants\n",
      "  Processing sample C: 967 variants\n",
      "  Processing sample D: 977 variants\n",
      "  Processing sample E: 947 variants\n",
      "  Processing sample B: 719 variants\n",
      "  Processing sample K: 961 variants\n",
      "  Saved combined PyClone input for patient G53 to results/G53/G53_pyclone_input.tsv\n",
      "Processing patient G39...\n",
      "  Loaded 3851 variants\n",
      "  Filtered from 3851 to 3851 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 6 samples: G, A, F, C, E, B\n",
      "  Processing sample G: 608 variants\n",
      "  Processing sample A: 525 variants\n",
      "  Processing sample F: 546 variants\n",
      "  Processing sample C: 910 variants\n",
      "  Processing sample E: 519 variants\n",
      "  Processing sample B: 743 variants\n",
      "  Saved combined PyClone input for patient G39 to results/G39/G39_pyclone_input.tsv\n",
      "Processing patient G97...\n",
      "  Loaded 32021 variants\n",
      "  Filtered from 32021 to 32021 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 8 samples: I, G, F, H, C, D, E, B\n",
      "  Processing sample I: 702 variants\n",
      "  Processing sample G: 1432 variants\n",
      "  Processing sample F: 15273 variants\n",
      "  Processing sample H: 1197 variants\n",
      "  Processing sample C: 857 variants\n",
      "  Processing sample D: 921 variants\n",
      "  Processing sample E: 663 variants\n",
      "  Processing sample B: 10976 variants\n",
      "  Saved combined PyClone input for patient G97 to results/G97/G97_pyclone_input.tsv\n",
      "Processing patient G52...\n",
      "  Loaded 5098 variants\n",
      "  Filtered from 5098 to 5098 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 10 samples: A, F, H, M, J, C, D, E, B, L\n",
      "  Processing sample A: 565 variants\n",
      "  Processing sample F: 445 variants\n",
      "  Processing sample H: 592 variants\n",
      "  Processing sample M: 493 variants\n",
      "  Processing sample J: 499 variants\n",
      "  Processing sample C: 355 variants\n",
      "  Processing sample D: 590 variants\n",
      "  Processing sample E: 605 variants\n",
      "  Processing sample B: 440 variants\n",
      "  Processing sample L: 514 variants\n",
      "  Saved combined PyClone input for patient G52 to results/G52/G52_pyclone_input.tsv\n",
      "Processing patient G99...\n",
      "  Loaded 45509 variants\n",
      "  Filtered from 45509 to 45509 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 4 samples: I, F, E, K\n",
      "  Processing sample I: 12006 variants\n",
      "  Processing sample F: 12178 variants\n",
      "  Processing sample E: 8903 variants\n",
      "  Processing sample K: 12422 variants\n",
      "  Saved combined PyClone input for patient G99 to results/G99/G99_pyclone_input.tsv\n",
      "Processing patient G77...\n",
      "  Loaded 8844 variants\n",
      "  Filtered from 8844 to 8844 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 8 samples: I, G, F, H, J, C, D, B\n",
      "  Processing sample I: 1007 variants\n",
      "  Processing sample G: 1679 variants\n",
      "  Processing sample F: 1186 variants\n",
      "  Processing sample H: 1237 variants\n",
      "  Processing sample J: 796 variants\n",
      "  Processing sample C: 771 variants\n",
      "  Processing sample D: 1095 variants\n",
      "  Processing sample B: 1073 variants\n",
      "  Saved combined PyClone input for patient G77 to results/G77/G77_pyclone_input.tsv\n",
      "Processing patient G12...\n",
      "  Loaded 8161 variants\n",
      "  Filtered from 8161 to 8161 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 4 samples: F, C, E, B\n",
      "  Processing sample F: 2469 variants\n",
      "  Processing sample C: 1916 variants\n",
      "  Processing sample E: 1903 variants\n",
      "  Processing sample B: 1873 variants\n",
      "  Saved combined PyClone input for patient G12 to results/G12/G12_pyclone_input.tsv\n",
      "Processing patient M24...\n",
      "  Loaded 3305 variants\n",
      "  Filtered from 3305 to 3305 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 4 samples: F, C, E, B\n",
      "  Processing sample F: 875 variants\n",
      "  Processing sample C: 792 variants\n",
      "  Processing sample E: 713 variants\n",
      "  Processing sample B: 925 variants\n",
      "  Saved combined PyClone input for patient M24 to results/M24/M24_pyclone_input.tsv\n",
      "Processing patient G23...\n",
      "  Loaded 5157 variants\n",
      "  Filtered from 5157 to 5157 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 6 samples: F, H, B1, C, D, B\n",
      "  Processing sample F: 870 variants\n",
      "  Processing sample H: 785 variants\n",
      "  Processing sample B1: 883 variants\n",
      "  Processing sample C: 929 variants\n",
      "  Processing sample D: 1182 variants\n",
      "  Processing sample B: 508 variants\n",
      "  Saved combined PyClone input for patient G23 to results/G23/G23_pyclone_input.tsv\n",
      "Processing patient G40...\n",
      "  Loaded 4941 variants\n",
      "  Filtered from 4941 to 4941 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 9 samples: I, G, A, F, M, C, D, E, K\n",
      "  Processing sample I: 527 variants\n",
      "  Processing sample G: 561 variants\n",
      "  Processing sample A: 504 variants\n",
      "  Processing sample F: 550 variants\n",
      "  Processing sample M: 580 variants\n",
      "  Processing sample C: 574 variants\n",
      "  Processing sample D: 562 variants\n",
      "  Processing sample E: 508 variants\n",
      "  Processing sample K: 575 variants\n",
      "  Saved combined PyClone input for patient G40 to results/G40/G40_pyclone_input.tsv\n",
      "Processing patient M78...\n",
      "  Loaded 4871 variants\n",
      "  Filtered from 4871 to 4871 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 5 samples: F, H, J, C, B\n",
      "  Processing sample F: 649 variants\n",
      "  Processing sample H: 708 variants\n",
      "  Processing sample J: 1532 variants\n",
      "  Processing sample C: 1176 variants\n",
      "  Processing sample B: 806 variants\n",
      "  Saved combined PyClone input for patient M78 to results/M78/M78_pyclone_input.tsv\n",
      "Processing patient G85...\n",
      "  Loaded 8416 variants\n",
      "  Filtered from 8416 to 8416 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 7 samples: I, G, F, H, C, D, B\n",
      "  Processing sample I: 727 variants\n",
      "  Processing sample G: 1107 variants\n",
      "  Processing sample F: 989 variants\n",
      "  Processing sample H: 1258 variants\n",
      "  Processing sample C: 1038 variants\n",
      "  Processing sample D: 934 variants\n",
      "  Processing sample B: 2363 variants\n",
      "  Saved combined PyClone input for patient G85 to results/G85/G85_pyclone_input.tsv\n",
      "Processing patient p182...\n",
      "  Loaded 1905 variants\n",
      "  Filtered from 1905 to 1905 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Error processing patient p182: sequence item 0: expected str instance, numpy.int64 found\n",
      "Processing patient P158...\n",
      "  Loaded 2854 variants\n",
      "  Filtered from 2854 to 2854 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Error processing patient P158: sequence item 0: expected str instance, numpy.int64 found\n",
      "Processing patient G5...\n",
      "  Loaded 2967 variants\n",
      "  Filtered from 2967 to 2967 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 6 samples: I, G, H, C, D, B\n",
      "  Processing sample I: 461 variants\n",
      "  Processing sample G: 516 variants\n",
      "  Processing sample H: 541 variants\n",
      "  Processing sample C: 467 variants\n",
      "  Processing sample D: 485 variants\n",
      "  Processing sample B: 497 variants\n",
      "  Saved combined PyClone input for patient G5 to results/G5/G5_pyclone_input.tsv\n",
      "Processing patient M95...\n",
      "  Loaded 5495 variants\n",
      "  Filtered from 5495 to 5495 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 6 samples: F, M, C, D, B, L\n",
      "  Processing sample F: 896 variants\n",
      "  Processing sample M: 781 variants\n",
      "  Processing sample C: 917 variants\n",
      "  Processing sample D: 1281 variants\n",
      "  Processing sample B: 1029 variants\n",
      "  Processing sample L: 591 variants\n",
      "  Saved combined PyClone input for patient M95 to results/M95/M95_pyclone_input.tsv\n",
      "Processing patient G92...\n",
      "  Loaded 57596 variants\n",
      "  Filtered from 57596 to 57596 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 6 samples: G, F, M, C, D, K\n",
      "  Processing sample G: 843 variants\n",
      "  Processing sample F: 52570 variants\n",
      "  Processing sample M: 1256 variants\n",
      "  Processing sample C: 1425 variants\n",
      "  Processing sample D: 737 variants\n",
      "  Processing sample K: 765 variants\n",
      "  Saved combined PyClone input for patient G92 to results/G92/G92_pyclone_input.tsv\n",
      "Processing patient G59...\n",
      "  Loaded 6211 variants\n",
      "  Filtered from 6211 to 6211 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 9 samples: I, F, H, J, D, E, B, K, L\n",
      "  Processing sample I: 475 variants\n",
      "  Processing sample F: 532 variants\n",
      "  Processing sample H: 491 variants\n",
      "  Processing sample J: 1155 variants\n",
      "  Processing sample D: 1003 variants\n",
      "  Processing sample E: 452 variants\n",
      "  Processing sample B: 480 variants\n",
      "  Processing sample K: 1121 variants\n",
      "  Processing sample L: 502 variants\n",
      "  Saved combined PyClone input for patient G59 to results/G59/G59_pyclone_input.tsv\n",
      "Processing patient G87...\n",
      "  Loaded 4429 variants\n",
      "  Filtered from 4429 to 4429 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 4 samples: A, H, J, K\n",
      "  Processing sample A: 665 variants\n",
      "  Processing sample H: 1029 variants\n",
      "  Processing sample J: 1647 variants\n",
      "  Processing sample K: 1088 variants\n",
      "  Saved combined PyClone input for patient G87 to results/G87/G87_pyclone_input.tsv\n",
      "Processing patient p065...\n",
      "  Loaded 37838 variants\n",
      "  Filtered from 37838 to 37838 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Error processing patient p065: sequence item 0: expected str instance, numpy.int64 found\n",
      "Processing patient G45...\n",
      "  Loaded 3806 variants\n",
      "  Filtered from 3806 to 3806 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 5 samples: H, M, B, K, L\n",
      "  Processing sample H: 626 variants\n",
      "  Processing sample M: 604 variants\n",
      "  Processing sample B: 1247 variants\n",
      "  Processing sample K: 724 variants\n",
      "  Processing sample L: 605 variants\n",
      "  Saved combined PyClone input for patient G45 to results/G45/G45_pyclone_input.tsv\n",
      "Processing patient G43...\n",
      "  Loaded 4848 variants\n",
      "  Filtered from 4848 to 4848 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 5 samples: R, I, T, C, P\n",
      "  Processing sample R: 1807 variants\n",
      "  Processing sample I: 881 variants\n",
      "  Processing sample T: 781 variants\n",
      "  Processing sample C: 831 variants\n",
      "  Processing sample P: 548 variants\n",
      "  Saved combined PyClone input for patient G43 to results/G43/G43_pyclone_input.tsv\n",
      "Processing patient p129...\n",
      "  Loaded 5492 variants\n",
      "  Filtered from 5492 to 5492 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Error processing patient p129: sequence item 0: expected str instance, numpy.int64 found\n",
      "\n",
      "PyClone conversion complete:\n",
      "  Patient M36: 5331 variants processed across 7 samples\n",
      "  Patient G53: 7620 variants processed across 9 samples\n",
      "  Patient G39: 3851 variants processed across 6 samples\n",
      "  Patient G97: 32021 variants processed across 8 samples\n",
      "  Patient G52: 5098 variants processed across 10 samples\n",
      "  Patient G99: 45509 variants processed across 4 samples\n",
      "  Patient G77: 8844 variants processed across 8 samples\n",
      "  Patient G12: 8161 variants processed across 4 samples\n",
      "  Patient M24: 3305 variants processed across 4 samples\n",
      "  Patient G23: 5157 variants processed across 6 samples\n",
      "  Patient G40: 4941 variants processed across 9 samples\n",
      "  Patient M78: 4871 variants processed across 5 samples\n",
      "  Patient G85: 8416 variants processed across 7 samples\n",
      "  Patient G5: 2967 variants processed across 6 samples\n",
      "  Patient M95: 5495 variants processed across 6 samples\n",
      "  Patient G92: 57596 variants processed across 6 samples\n",
      "  Patient G59: 6211 variants processed across 9 samples\n",
      "  Patient G87: 4429 variants processed across 4 samples\n",
      "  Patient G45: 3806 variants processed across 5 samples\n",
      "  Patient G43: 4848 variants processed across 5 samples\n",
      "\n",
      "First few rows of PyClone input for patient M36:\n",
      "        mutation_id sample_id  ref_counts  alt_counts  normal_cn  major_cn  \\\n",
      "0   chr1:646149:C>T         R         135           4          2         1   \n",
      "1   chr1:685220:G>A         R           1           2          2         1   \n",
      "2   chr1:938352:G>T         R           3           2          2         1   \n",
      "3  chr1:5864855:T>C         R           3           2          2         1   \n",
      "4  chr1:7592381:A>G         R           5           2          2         1   \n",
      "\n",
      "   minor_cn  tumor_content  \n",
      "0         1            1.0  \n",
      "1         1            1.0  \n",
      "2         1            1.0  \n",
      "3         1            1.0  \n",
      "4         1            1.0  \n"
     ]
    }
   ],
   "source": [
    "# Standalone VCF to PyClone-VI Converter\n",
    "# This code reads variant data from TSV files in the results directory\n",
    "# and saves PyClone input files in the same patient directories\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def get_patient_variant_files(results_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Find all patient variant TSV files in the results directory\n",
    "    \n",
    "    Args:\n",
    "        results_dir (str): Path to the results directory\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping patient IDs to their variant file paths\n",
    "    \"\"\"\n",
    "    patient_files = {}\n",
    "    \n",
    "    # Look for patient directories in the results directory\n",
    "    patient_dirs = glob.glob(os.path.join(results_dir, \"*\"))\n",
    "    \n",
    "    for patient_dir in patient_dirs:\n",
    "        if os.path.isdir(patient_dir):\n",
    "            # Extract patient ID from directory name\n",
    "            patient_id = os.path.basename(patient_dir)\n",
    "            \n",
    "            # Look for variant TSV file with patient ID prefix\n",
    "            variant_file_pattern = os.path.join(patient_dir, f\"{patient_id}_variants.tsv\")\n",
    "            variant_files = glob.glob(variant_file_pattern)\n",
    "            \n",
    "            if variant_files:\n",
    "                patient_files[patient_id] = variant_files[0]\n",
    "                print(f\"Found variant file for patient {patient_id}: {variant_files[0]}\")\n",
    "    \n",
    "    if not patient_files:\n",
    "        print(f\"No patient variant files found in {results_dir}\")\n",
    "    \n",
    "    return patient_files\n",
    "\n",
    "def process_patient_variants(patient_id, variant_file, output_dir=None, \n",
    "                            default_tumor_content=0.5,\n",
    "                            min_coverage=20, min_vaf=0.05):\n",
    "    \"\"\"\n",
    "    Process a single patient's variants and convert to PyClone format\n",
    "    \n",
    "    Args:\n",
    "        patient_id (str): Patient identifier\n",
    "        variant_file (str): Path to patient's variant TSV file\n",
    "        output_dir (str): Directory to save output (if None, save in same directory as input)\n",
    "        default_tumor_content (float): Default tumor purity estimate (0-1)\n",
    "        min_coverage (int): Minimum read coverage to include a variant\n",
    "        min_vaf (float): Minimum VAF to include a variant\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: PyClone formatted data for the patient\n",
    "    \"\"\"\n",
    "    # Determine output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(variant_file)\n",
    "    \n",
    "    print(f\"Processing patient {patient_id}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read variant file\n",
    "        variants_df = pd.read_csv(variant_file, sep='\\t')\n",
    "        print(f\"  Loaded {len(variants_df)} variants\")\n",
    "        \n",
    "        # Make sure we have the required columns\n",
    "        required_columns = ['CHROM', 'POS', 'REF', 'ALT', 'REF_COUNT', 'ALT_COUNT', 'TOTAL_COUNT', 'VAF', 'SAMPLE']\n",
    "        missing_columns = [col for col in required_columns if col not in variants_df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns in variant file: {', '.join(missing_columns)}\")\n",
    "        \n",
    "        # Filter variants by coverage and VAF\n",
    "        filtered_variants = variants_df[\n",
    "            (variants_df['TOTAL_COUNT'] >= min_coverage) & \n",
    "            (variants_df['VAF'] >= min_vaf)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"  Filtered from {len(variants_df)} to {len(filtered_variants)} variants\")\n",
    "        print(f\"    Minimum coverage: {min_coverage}\")\n",
    "        print(f\"    Minimum VAF: {min_vaf}\")\n",
    "        \n",
    "        # Get unique samples for this patient\n",
    "        samples = filtered_variants['SAMPLE'].unique()\n",
    "        print(f\"  Found {len(samples)} samples: {', '.join(samples)}\")\n",
    "        \n",
    "        # Process each sample\n",
    "        sample_pyclone_dfs = {}\n",
    "        \n",
    "        for sample in samples:\n",
    "            sample_data = filtered_variants[filtered_variants['SAMPLE'] == sample]\n",
    "            print(f\"  Processing sample {sample}: {len(sample_data)} variants\")\n",
    "            \n",
    "            # Convert to PyClone format\n",
    "            pyclone_data = []\n",
    "            for _, row in sample_data.iterrows():\n",
    "                # Create mutation_id from chromosome and position\n",
    "                mutation_id = f\"{row['CHROM']}:{row['POS']}:{row['REF']}>{row['ALT']}\"\n",
    "                \n",
    "                # Set default copy number values (assuming diploid)\n",
    "                normal_cn = 2  # Assuming autosomal\n",
    "                major_cn = 1   # Default major copy number\n",
    "                minor_cn = 1   # Default minor copy number\n",
    "                \n",
    "                # Adjust for sex chromosomes\n",
    "                if row['CHROM'] in ['chrX', 'X', 'chrY', 'Y']:\n",
    "                    if row['CHROM'] in ['chrX', 'X']:\n",
    "                        # Set X chromosome values (assuming female)\n",
    "                        pass  # Using defaults\n",
    "                    elif row['CHROM'] in ['chrY', 'Y']:\n",
    "                        # Set Y chromosome values (assuming male)\n",
    "                        normal_cn = 1\n",
    "                        major_cn = 1\n",
    "                        minor_cn = 0\n",
    "                \n",
    "                # Add to PyClone data\n",
    "                pyclone_data.append({\n",
    "                    'mutation_id': mutation_id,\n",
    "                    'sample_id': sample,\n",
    "                    'ref_counts': row['REF_COUNT'],\n",
    "                    'alt_counts': row['ALT_COUNT'],\n",
    "                    'normal_cn': normal_cn,\n",
    "                    'major_cn': major_cn,\n",
    "                    'minor_cn': minor_cn,\n",
    "                    'tumor_content': default_tumor_content\n",
    "                })\n",
    "            \n",
    "            # Create PyClone DataFrame for this sample\n",
    "            sample_pyclone_df = pd.DataFrame(pyclone_data)\n",
    "            sample_pyclone_dfs[sample] = sample_pyclone_df\n",
    "        \n",
    "        # Combine all samples for this patient into a single PyClone file\n",
    "        combined_pyclone_df = pd.concat(sample_pyclone_dfs.values(), ignore_index=True)\n",
    "        \n",
    "        # Save combined file to patient's directory\n",
    "        output_file = os.path.join(output_dir, f\"{patient_id}_pyclone_input.tsv\")\n",
    "        combined_pyclone_df.to_csv(output_file, sep='\\t', index=False)\n",
    "        print(f\"  Saved combined PyClone input for patient {patient_id} to {output_file}\")\n",
    "        \n",
    "        return combined_pyclone_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing patient {patient_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Set parameters\n",
    "    results_directory = \"results\"  # Directory containing patient subdirectories\n",
    "    tumor_purity = 1.0            # Estimated tumor purity (0-1)\n",
    "    min_read_coverage = 1         # Minimum total reads required\n",
    "    min_vaf_threshold = 0.0       # Minimum VAF required (0%)\n",
    "    \n",
    "    # Get all patient variant files\n",
    "    print(f\"Finding patient variant files in {results_directory}\")\n",
    "    patient_files = get_patient_variant_files(results_dir=results_directory)\n",
    "    \n",
    "    if not patient_files:\n",
    "        print(\"No patient variant files found. Exiting.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Process each patient independently\n",
    "    pyclone_results = {}\n",
    "    for patient_id, variant_file in patient_files.items():\n",
    "        pyclone_df = process_patient_variants(\n",
    "            patient_id,\n",
    "            variant_file,\n",
    "            default_tumor_content=tumor_purity,\n",
    "            min_coverage=min_read_coverage,\n",
    "            min_vaf=min_vaf_threshold\n",
    "        )\n",
    "        \n",
    "        if pyclone_df is not None:\n",
    "            pyclone_results[patient_id] = pyclone_df\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\nPyClone conversion complete:\")\n",
    "    for patient, data in pyclone_results.items():\n",
    "        print(f\"  Patient {patient}: {len(data)} variants processed across {len(data['sample_id'].unique())} samples\")\n",
    "        \n",
    "    # Display the first few rows of the first patient's PyClone input\n",
    "    if pyclone_results:\n",
    "        first_patient = list(pyclone_results.keys())[0]\n",
    "        print(f\"\\nFirst few rows of PyClone input for patient {first_patient}:\")\n",
    "        print(pyclone_results[first_patient].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add default for missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding patient variant files in results\n",
      "Found variant file for patient p191: results/p191/p191_variants.tsv\n",
      "Found variant file for patient P163: results/P163/P163_variants.tsv\n",
      "Found variant file for patient P190: results/P190/P190_variants.tsv\n",
      "Found variant file for patient p152: results/p152/p152_variants.tsv\n",
      "Found variant file for patient M36: results/M36/M36_variants.tsv\n",
      "Found variant file for patient G53: results/G53/G53_variants.tsv\n",
      "Found variant file for patient G39: results/G39/G39_variants.tsv\n",
      "Found variant file for patient G97: results/G97/G97_variants.tsv\n",
      "Found variant file for patient G52: results/G52/G52_variants.tsv\n",
      "Found variant file for patient G99: results/G99/G99_variants.tsv\n",
      "Found variant file for patient G77: results/G77/G77_variants.tsv\n",
      "Found variant file for patient G12: results/G12/G12_variants.tsv\n",
      "Found variant file for patient M24: results/M24/M24_variants.tsv\n",
      "Found variant file for patient G23: results/G23/G23_variants.tsv\n",
      "Found variant file for patient G40: results/G40/G40_variants.tsv\n",
      "Found variant file for patient M78: results/M78/M78_variants.tsv\n",
      "Found variant file for patient G85: results/G85/G85_variants.tsv\n",
      "Found variant file for patient p182: results/p182/p182_variants.tsv\n",
      "Found variant file for patient P158: results/P158/P158_variants.tsv\n",
      "Found variant file for patient G5: results/G5/G5_variants.tsv\n",
      "Found variant file for patient M95: results/M95/M95_variants.tsv\n",
      "Found variant file for patient G92: results/G92/G92_variants.tsv\n",
      "Found variant file for patient G59: results/G59/G59_variants.tsv\n",
      "Found variant file for patient G87: results/G87/G87_variants.tsv\n",
      "Found variant file for patient p065: results/p065/p065_variants.tsv\n",
      "Found variant file for patient G45: results/G45/G45_variants.tsv\n",
      "Found variant file for patient G43: results/G43/G43_variants.tsv\n",
      "Found variant file for patient p129: results/p129/p129_variants.tsv\n",
      "Processing patient p191...\n",
      "  Loaded 1585 variants\n",
      "  Filtered from 1585 to 952 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Error processing patient p191: sequence item 0: expected str instance, numpy.int64 found\n",
      "Processing patient P163...\n",
      "  Loaded 3850 variants\n",
      "  Filtered from 3850 to 1858 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Error processing patient P163: sequence item 0: expected str instance, numpy.int64 found\n",
      "Processing patient P190...\n",
      "  Loaded 3483 variants\n",
      "  Filtered from 3483 to 2063 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Error processing patient P190: sequence item 0: expected str instance, numpy.int64 found\n",
      "Processing patient p152...\n",
      "  Loaded 4215 variants\n",
      "  Filtered from 4215 to 1926 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Error processing patient p152: sequence item 0: expected str instance, numpy.int64 found\n",
      "Processing patient M36...\n",
      "  Loaded 5331 variants\n",
      "  Filtered from 5331 to 2239 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 7 samples: R, N, T, F, E, K, P\n",
      "  Found 2192 unique mutations across all samples\n",
      "  Processing sample R: 203 variants\n",
      "    Added 1989 missing mutations with default values\n",
      "  Processing sample N: 216 variants\n",
      "    Added 1976 missing mutations with default values\n",
      "  Processing sample T: 427 variants\n",
      "    Added 1765 missing mutations with default values\n",
      "  Processing sample F: 543 variants\n",
      "    Added 1649 missing mutations with default values\n",
      "  Processing sample E: 447 variants\n",
      "    Added 1745 missing mutations with default values\n",
      "  Processing sample K: 237 variants\n",
      "    Added 1955 missing mutations with default values\n",
      "  Processing sample P: 166 variants\n",
      "    Added 2026 missing mutations with default values\n",
      "  Saved combined PyClone input for patient M36 to results/M36/M36_pyclone_input.tsv\n",
      "Processing patient G53...\n",
      "  Loaded 7620 variants\n",
      "  Filtered from 7620 to 4255 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 9 samples: I, A, F, J, C, D, E, B, K\n",
      "  Found 3231 unique mutations across all samples\n",
      "  Processing sample I: 608 variants\n",
      "    Added 2623 missing mutations with default values\n",
      "  Processing sample A: 235 variants\n",
      "    Added 2996 missing mutations with default values\n",
      "  Processing sample F: 438 variants\n",
      "    Added 2793 missing mutations with default values\n",
      "  Processing sample J: 414 variants\n",
      "    Added 2817 missing mutations with default values\n",
      "  Processing sample C: 633 variants\n",
      "    Added 2598 missing mutations with default values\n",
      "  Processing sample D: 451 variants\n",
      "    Added 2780 missing mutations with default values\n",
      "  Processing sample E: 461 variants\n",
      "    Added 2770 missing mutations with default values\n",
      "  Processing sample B: 422 variants\n",
      "    Added 2809 missing mutations with default values\n",
      "  Processing sample K: 593 variants\n",
      "    Added 2638 missing mutations with default values\n",
      "  Saved combined PyClone input for patient G53 to results/G53/G53_pyclone_input.tsv\n",
      "Processing patient G39...\n",
      "  Loaded 3851 variants\n",
      "  Filtered from 3851 to 2000 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 6 samples: G, A, F, C, E, B\n",
      "  Found 1515 unique mutations across all samples\n",
      "  Processing sample G: 352 variants\n",
      "    Added 1163 missing mutations with default values\n",
      "  Processing sample A: 335 variants\n",
      "    Added 1180 missing mutations with default values\n",
      "  Processing sample F: 309 variants\n",
      "    Added 1206 missing mutations with default values\n",
      "  Processing sample C: 428 variants\n",
      "    Added 1087 missing mutations with default values\n",
      "  Processing sample E: 313 variants\n",
      "    Added 1202 missing mutations with default values\n",
      "  Processing sample B: 263 variants\n",
      "    Added 1252 missing mutations with default values\n",
      "  Saved combined PyClone input for patient G39 to results/G39/G39_pyclone_input.tsv\n",
      "Processing patient G97...\n",
      "  Loaded 32021 variants\n",
      "  Filtered from 32021 to 24542 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 8 samples: I, G, F, H, C, D, E, B\n",
      "  Found 17414 unique mutations across all samples\n",
      "  Processing sample I: 293 variants\n",
      "    Added 17121 missing mutations with default values\n",
      "  Processing sample G: 663 variants\n",
      "    Added 16751 missing mutations with default values\n",
      "  Processing sample F: 12817 variants\n",
      "    Added 4597 missing mutations with default values\n",
      "  Processing sample H: 438 variants\n",
      "    Added 16976 missing mutations with default values\n",
      "  Processing sample C: 545 variants\n",
      "    Added 16869 missing mutations with default values\n",
      "  Processing sample D: 452 variants\n",
      "    Added 16962 missing mutations with default values\n",
      "  Processing sample E: 289 variants\n",
      "    Added 17125 missing mutations with default values\n",
      "  Processing sample B: 9045 variants\n",
      "    Added 8369 missing mutations with default values\n",
      "  Saved combined PyClone input for patient G97 to results/G97/G97_pyclone_input.tsv\n",
      "Processing patient G52...\n",
      "  Loaded 5098 variants\n",
      "  Filtered from 5098 to 2563 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 10 samples: A, F, H, M, J, C, D, E, B, L\n",
      "  Found 1861 unique mutations across all samples\n",
      "  Processing sample A: 327 variants\n",
      "    Added 1534 missing mutations with default values\n",
      "  Processing sample F: 230 variants\n",
      "    Added 1631 missing mutations with default values\n",
      "  Processing sample H: 289 variants\n",
      "    Added 1572 missing mutations with default values\n",
      "  Processing sample M: 259 variants\n",
      "    Added 1602 missing mutations with default values\n",
      "  Processing sample J: 230 variants\n",
      "    Added 1631 missing mutations with default values\n",
      "  Processing sample C: 161 variants\n",
      "    Added 1700 missing mutations with default values\n",
      "  Processing sample D: 303 variants\n",
      "    Added 1558 missing mutations with default values\n",
      "  Processing sample E: 306 variants\n",
      "    Added 1555 missing mutations with default values\n",
      "  Processing sample B: 217 variants\n",
      "    Added 1644 missing mutations with default values\n",
      "  Processing sample L: 241 variants\n",
      "    Added 1620 missing mutations with default values\n",
      "  Saved combined PyClone input for patient G52 to results/G52/G52_pyclone_input.tsv\n",
      "Processing patient G99...\n",
      "  Loaded 45509 variants\n",
      "  Filtered from 45509 to 30146 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 4 samples: I, F, E, K\n",
      "  Found 12242 unique mutations across all samples\n",
      "  Processing sample I: 7539 variants\n",
      "    Added 4703 missing mutations with default values\n",
      "  Processing sample F: 7704 variants\n",
      "    Added 4538 missing mutations with default values\n",
      "  Processing sample E: 7196 variants\n",
      "    Added 5046 missing mutations with default values\n",
      "  Processing sample K: 7707 variants\n",
      "    Added 4535 missing mutations with default values\n",
      "  Saved combined PyClone input for patient G99 to results/G99/G99_pyclone_input.tsv\n",
      "Processing patient G77...\n",
      "  Loaded 8844 variants\n",
      "  Filtered from 8844 to 4407 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 8 samples: I, G, F, H, J, C, D, B\n",
      "  Found 3161 unique mutations across all samples\n",
      "  Processing sample I: 423 variants\n",
      "    Added 2738 missing mutations with default values\n",
      "  Processing sample G: 729 variants\n",
      "    Added 2432 missing mutations with default values\n",
      "  Processing sample F: 690 variants\n",
      "    Added 2471 missing mutations with default values\n",
      "  Processing sample H: 668 variants\n",
      "    Added 2493 missing mutations with default values\n",
      "  Processing sample J: 381 variants\n",
      "    Added 2780 missing mutations with default values\n",
      "  Processing sample C: 355 variants\n",
      "    Added 2806 missing mutations with default values\n",
      "  Processing sample D: 672 variants\n",
      "    Added 2489 missing mutations with default values\n",
      "  Processing sample B: 489 variants\n",
      "    Added 2672 missing mutations with default values\n",
      "  Saved combined PyClone input for patient G77 to results/G77/G77_pyclone_input.tsv\n",
      "Processing patient G12...\n",
      "  Loaded 8161 variants\n",
      "  Filtered from 8161 to 5412 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 4 samples: F, C, E, B\n",
      "  Found 2476 unique mutations across all samples\n",
      "  Processing sample F: 1779 variants\n",
      "    Added 697 missing mutations with default values\n",
      "  Processing sample C: 1252 variants\n",
      "    Added 1224 missing mutations with default values\n",
      "  Processing sample E: 1222 variants\n",
      "    Added 1254 missing mutations with default values\n",
      "  Processing sample B: 1159 variants\n",
      "    Added 1317 missing mutations with default values\n",
      "  Saved combined PyClone input for patient G12 to results/G12/G12_pyclone_input.tsv\n",
      "Processing patient M24...\n",
      "  Loaded 3305 variants\n",
      "  Filtered from 3305 to 1687 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 4 samples: F, C, E, B\n",
      "  Found 1109 unique mutations across all samples\n",
      "  Processing sample F: 428 variants\n",
      "    Added 681 missing mutations with default values\n",
      "  Processing sample C: 420 variants\n",
      "    Added 689 missing mutations with default values\n",
      "  Processing sample E: 390 variants\n",
      "    Added 719 missing mutations with default values\n",
      "  Processing sample B: 449 variants\n",
      "    Added 660 missing mutations with default values\n",
      "  Saved combined PyClone input for patient M24 to results/M24/M24_pyclone_input.tsv\n",
      "Processing patient G23...\n",
      "  Loaded 5157 variants\n",
      "  Filtered from 5157 to 2345 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 6 samples: F, H, B1, C, D, B\n",
      "  Found 2010 unique mutations across all samples\n",
      "  Processing sample F: 563 variants\n",
      "    Added 1447 missing mutations with default values\n",
      "  Processing sample H: 321 variants\n",
      "    Added 1689 missing mutations with default values\n",
      "  Processing sample B1: 331 variants\n",
      "    Added 1679 missing mutations with default values\n",
      "  Processing sample C: 533 variants\n",
      "    Added 1477 missing mutations with default values\n",
      "  Processing sample D: 427 variants\n",
      "    Added 1583 missing mutations with default values\n",
      "  Processing sample B: 170 variants\n",
      "    Added 1840 missing mutations with default values\n",
      "  Saved combined PyClone input for patient G23 to results/G23/G23_pyclone_input.tsv\n",
      "Processing patient G40...\n",
      "  Loaded 4941 variants\n",
      "  Filtered from 4941 to 2482 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 9 samples: I, G, A, F, M, C, D, E, K\n",
      "  Found 1692 unique mutations across all samples\n",
      "  Processing sample I: 254 variants\n",
      "    Added 1438 missing mutations with default values\n",
      "  Processing sample G: 289 variants\n",
      "    Added 1403 missing mutations with default values\n",
      "  Processing sample A: 248 variants\n",
      "    Added 1444 missing mutations with default values\n",
      "  Processing sample F: 267 variants\n",
      "    Added 1425 missing mutations with default values\n",
      "  Processing sample M: 288 variants\n",
      "    Added 1404 missing mutations with default values\n",
      "  Processing sample C: 283 variants\n",
      "    Added 1409 missing mutations with default values\n",
      "  Processing sample D: 315 variants\n",
      "    Added 1377 missing mutations with default values\n",
      "  Processing sample E: 256 variants\n",
      "    Added 1436 missing mutations with default values\n",
      "  Processing sample K: 282 variants\n",
      "    Added 1410 missing mutations with default values\n",
      "  Saved combined PyClone input for patient G40 to results/G40/G40_pyclone_input.tsv\n",
      "Processing patient M78...\n",
      "  Loaded 4871 variants\n",
      "  Filtered from 4871 to 2583 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 5 samples: F, H, J, C, B\n",
      "  Found 2415 unique mutations across all samples\n",
      "  Processing sample F: 404 variants\n",
      "    Added 2011 missing mutations with default values\n",
      "  Processing sample H: 475 variants\n",
      "    Added 1940 missing mutations with default values\n",
      "  Processing sample J: 593 variants\n",
      "    Added 1822 missing mutations with default values\n",
      "  Processing sample C: 607 variants\n",
      "    Added 1808 missing mutations with default values\n",
      "  Processing sample B: 504 variants\n",
      "    Added 1911 missing mutations with default values\n",
      "  Saved combined PyClone input for patient M78 to results/M78/M78_pyclone_input.tsv\n",
      "Processing patient G85...\n",
      "  Loaded 8416 variants\n",
      "  Filtered from 8416 to 3703 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 7 samples: I, G, F, H, C, D, B\n",
      "  Found 2950 unique mutations across all samples\n",
      "  Processing sample I: 268 variants\n",
      "    Added 2682 missing mutations with default values\n",
      "  Processing sample G: 456 variants\n",
      "    Added 2494 missing mutations with default values\n",
      "  Processing sample F: 450 variants\n",
      "    Added 2500 missing mutations with default values\n",
      "  Processing sample H: 546 variants\n",
      "    Added 2404 missing mutations with default values\n",
      "  Processing sample C: 307 variants\n",
      "    Added 2643 missing mutations with default values\n",
      "  Processing sample D: 388 variants\n",
      "    Added 2562 missing mutations with default values\n",
      "  Processing sample B: 1288 variants\n",
      "    Added 1662 missing mutations with default values\n",
      "  Saved combined PyClone input for patient G85 to results/G85/G85_pyclone_input.tsv\n",
      "Processing patient p182...\n",
      "  Loaded 1905 variants\n",
      "  Filtered from 1905 to 999 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Error processing patient p182: sequence item 0: expected str instance, numpy.int64 found\n",
      "Processing patient P158...\n",
      "  Loaded 2854 variants\n",
      "  Filtered from 2854 to 1395 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Error processing patient P158: sequence item 0: expected str instance, numpy.int64 found\n",
      "Processing patient G5...\n",
      "  Loaded 2967 variants\n",
      "  Filtered from 2967 to 1573 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 6 samples: I, G, H, C, D, B\n",
      "  Found 1107 unique mutations across all samples\n",
      "  Processing sample I: 253 variants\n",
      "    Added 854 missing mutations with default values\n",
      "  Processing sample G: 270 variants\n",
      "    Added 837 missing mutations with default values\n",
      "  Processing sample H: 299 variants\n",
      "    Added 808 missing mutations with default values\n",
      "  Processing sample C: 250 variants\n",
      "    Added 857 missing mutations with default values\n",
      "  Processing sample D: 250 variants\n",
      "    Added 857 missing mutations with default values\n",
      "  Processing sample B: 251 variants\n",
      "    Added 856 missing mutations with default values\n",
      "  Saved combined PyClone input for patient G5 to results/G5/G5_pyclone_input.tsv\n",
      "Processing patient M95...\n",
      "  Loaded 5495 variants\n",
      "  Filtered from 5495 to 3071 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 6 samples: F, M, C, D, B, L\n",
      "  Found 2595 unique mutations across all samples\n",
      "  Processing sample F: 606 variants\n",
      "    Added 1989 missing mutations with default values\n",
      "  Processing sample M: 350 variants\n",
      "    Added 2245 missing mutations with default values\n",
      "  Processing sample C: 624 variants\n",
      "    Added 1971 missing mutations with default values\n",
      "  Processing sample D: 592 variants\n",
      "    Added 2003 missing mutations with default values\n",
      "  Processing sample B: 639 variants\n",
      "    Added 1956 missing mutations with default values\n",
      "  Processing sample L: 260 variants\n",
      "    Added 2335 missing mutations with default values\n",
      "  Saved combined PyClone input for patient M95 to results/M95/M95_pyclone_input.tsv\n",
      "Processing patient G92...\n",
      "  Loaded 57596 variants\n",
      "  Filtered from 57596 to 31156 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 6 samples: G, F, M, C, D, K\n",
      "  Found 30807 unique mutations across all samples\n",
      "  Processing sample G: 531 variants\n",
      "    Added 30276 missing mutations with default values\n",
      "  Processing sample F: 28562 variants\n",
      "    Added 2245 missing mutations with default values\n",
      "  Processing sample M: 578 variants\n",
      "    Added 30229 missing mutations with default values\n",
      "  Processing sample C: 590 variants\n",
      "    Added 30217 missing mutations with default values\n",
      "  Processing sample D: 462 variants\n",
      "    Added 30345 missing mutations with default values\n",
      "  Processing sample K: 433 variants\n",
      "    Added 30374 missing mutations with default values\n",
      "  Saved combined PyClone input for patient G92 to results/G92/G92_pyclone_input.tsv\n",
      "Processing patient G59...\n",
      "  Loaded 6211 variants\n",
      "  Filtered from 6211 to 2960 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 9 samples: I, F, H, J, D, E, B, K, L\n",
      "  Found 2575 unique mutations across all samples\n",
      "  Processing sample I: 242 variants\n",
      "    Added 2333 missing mutations with default values\n",
      "  Processing sample F: 225 variants\n",
      "    Added 2350 missing mutations with default values\n",
      "  Processing sample H: 246 variants\n",
      "    Added 2329 missing mutations with default values\n",
      "  Processing sample J: 606 variants\n",
      "    Added 1969 missing mutations with default values\n",
      "  Processing sample D: 468 variants\n",
      "    Added 2107 missing mutations with default values\n",
      "  Processing sample E: 209 variants\n",
      "    Added 2366 missing mutations with default values\n",
      "  Processing sample B: 219 variants\n",
      "    Added 2356 missing mutations with default values\n",
      "  Processing sample K: 524 variants\n",
      "    Added 2051 missing mutations with default values\n",
      "  Processing sample L: 221 variants\n",
      "    Added 2354 missing mutations with default values\n",
      "  Saved combined PyClone input for patient G59 to results/G59/G59_pyclone_input.tsv\n",
      "Processing patient G87...\n",
      "  Loaded 4429 variants\n",
      "  Filtered from 4429 to 2559 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 4 samples: A, H, J, K\n",
      "  Found 2146 unique mutations across all samples\n",
      "  Processing sample A: 440 variants\n",
      "    Added 1706 missing mutations with default values\n",
      "  Processing sample H: 572 variants\n",
      "    Added 1574 missing mutations with default values\n",
      "  Processing sample J: 827 variants\n",
      "    Added 1319 missing mutations with default values\n",
      "  Processing sample K: 720 variants\n",
      "    Added 1426 missing mutations with default values\n",
      "  Saved combined PyClone input for patient G87 to results/G87/G87_pyclone_input.tsv\n",
      "Processing patient p065...\n",
      "  Loaded 37838 variants\n",
      "  Filtered from 37838 to 17713 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Error processing patient p065: sequence item 0: expected str instance, numpy.int64 found\n",
      "Processing patient G45...\n",
      "  Loaded 3806 variants\n",
      "  Filtered from 3806 to 2403 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 5 samples: H, M, B, K, L\n",
      "  Found 1733 unique mutations across all samples\n",
      "  Processing sample H: 378 variants\n",
      "    Added 1355 missing mutations with default values\n",
      "  Processing sample M: 332 variants\n",
      "    Added 1401 missing mutations with default values\n",
      "  Processing sample B: 904 variants\n",
      "    Added 829 missing mutations with default values\n",
      "  Processing sample K: 421 variants\n",
      "    Added 1312 missing mutations with default values\n",
      "  Processing sample L: 368 variants\n",
      "    Added 1365 missing mutations with default values\n",
      "  Saved combined PyClone input for patient G45 to results/G45/G45_pyclone_input.tsv\n",
      "Processing patient G43...\n",
      "  Loaded 4848 variants\n",
      "  Filtered from 4848 to 2695 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Found 5 samples: R, I, T, C, P\n",
      "  Found 2454 unique mutations across all samples\n",
      "  Processing sample R: 1058 variants\n",
      "    Added 1396 missing mutations with default values\n",
      "  Processing sample I: 533 variants\n",
      "    Added 1921 missing mutations with default values\n",
      "  Processing sample T: 338 variants\n",
      "    Added 2116 missing mutations with default values\n",
      "  Processing sample C: 553 variants\n",
      "    Added 1901 missing mutations with default values\n",
      "  Processing sample P: 213 variants\n",
      "    Added 2241 missing mutations with default values\n",
      "  Saved combined PyClone input for patient G43 to results/G43/G43_pyclone_input.tsv\n",
      "Processing patient p129...\n",
      "  Loaded 5492 variants\n",
      "  Filtered from 5492 to 2758 variants\n",
      "    Minimum coverage: 50\n",
      "    Minimum VAF: 0.01\n",
      "  Error processing patient p129: sequence item 0: expected str instance, numpy.int64 found\n",
      "\n",
      "PyClone conversion complete:\n",
      "  Patient M36: 15344 variants processed across 7 samples\n",
      "  Patient G53: 29079 variants processed across 9 samples\n",
      "  Patient G39: 9090 variants processed across 6 samples\n",
      "  Patient G97: 139312 variants processed across 8 samples\n",
      "  Patient G52: 18610 variants processed across 10 samples\n",
      "  Patient G99: 48968 variants processed across 4 samples\n",
      "  Patient G77: 25288 variants processed across 8 samples\n",
      "  Patient G12: 9904 variants processed across 4 samples\n",
      "  Patient M24: 4436 variants processed across 4 samples\n",
      "  Patient G23: 12060 variants processed across 6 samples\n",
      "  Patient G40: 15228 variants processed across 9 samples\n",
      "  Patient M78: 12075 variants processed across 5 samples\n",
      "  Patient G85: 20650 variants processed across 7 samples\n",
      "  Patient G5: 6642 variants processed across 6 samples\n",
      "  Patient M95: 15570 variants processed across 6 samples\n",
      "  Patient G92: 184842 variants processed across 6 samples\n",
      "  Patient G59: 23175 variants processed across 9 samples\n",
      "  Patient G87: 8584 variants processed across 4 samples\n",
      "  Patient G45: 8665 variants processed across 5 samples\n",
      "  Patient G43: 12270 variants processed across 5 samples\n"
     ]
    }
   ],
   "source": [
    "# Standalone VCF to PyClone-VI Converter\n",
    "# This code reads variant data from TSV files in the results directory\n",
    "# and saves PyClone input files in the same patient directories\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def get_patient_variant_files(results_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Find all patient variant TSV files in the results directory\n",
    "    \n",
    "    Args:\n",
    "        results_dir (str): Path to the results directory\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping patient IDs to their variant file paths\n",
    "    \"\"\"\n",
    "    patient_files = {}\n",
    "    \n",
    "    # Look for patient directories in the results directory\n",
    "    patient_dirs = glob.glob(os.path.join(results_dir, \"*\"))\n",
    "    \n",
    "    for patient_dir in patient_dirs:\n",
    "        if os.path.isdir(patient_dir):\n",
    "            # Extract patient ID from directory name\n",
    "            patient_id = os.path.basename(patient_dir)\n",
    "            \n",
    "            # Look for variant TSV file with patient ID prefix\n",
    "            variant_file_pattern = os.path.join(patient_dir, f\"{patient_id}_variants.tsv\")\n",
    "            variant_files = glob.glob(variant_file_pattern)\n",
    "            \n",
    "            if variant_files:\n",
    "                patient_files[patient_id] = variant_files[0]\n",
    "                print(f\"Found variant file for patient {patient_id}: {variant_files[0]}\")\n",
    "    \n",
    "    if not patient_files:\n",
    "        print(f\"No patient variant files found in {results_dir}\")\n",
    "    \n",
    "    return patient_files\n",
    "\n",
    "def process_patient_variants(patient_id, variant_file, output_dir=None, \n",
    "                            default_tumor_content=0.5,\n",
    "                            min_coverage=20, min_vaf=0.05):\n",
    "    \"\"\"\n",
    "    Process a single patient's variants and convert to PyClone format\n",
    "    \n",
    "    Args:\n",
    "        patient_id (str): Patient identifier\n",
    "        variant_file (str): Path to patient's variant TSV file\n",
    "        output_dir (str): Directory to save output (if None, save in same directory as input)\n",
    "        default_tumor_content (float): Default tumor purity estimate (0-1)\n",
    "        min_coverage (int): Minimum read coverage to include a variant\n",
    "        min_vaf (float): Minimum VAF to include a variant\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: PyClone formatted data for the patient\n",
    "    \"\"\"\n",
    "    # Determine output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(variant_file)\n",
    "    \n",
    "    print(f\"Processing patient {patient_id}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read variant file\n",
    "        variants_df = pd.read_csv(variant_file, sep='\\t')\n",
    "        print(f\"  Loaded {len(variants_df)} variants\")\n",
    "        \n",
    "        # Make sure we have the required columns\n",
    "        required_columns = ['CHROM', 'POS', 'REF', 'ALT', 'REF_COUNT', 'ALT_COUNT', 'TOTAL_COUNT', 'VAF', 'SAMPLE']\n",
    "        missing_columns = [col for col in required_columns if col not in variants_df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns in variant file: {', '.join(missing_columns)}\")\n",
    "        \n",
    "        # Filter variants by coverage and VAF\n",
    "        filtered_variants = variants_df[\n",
    "            (variants_df['TOTAL_COUNT'] >= min_coverage) & \n",
    "            (variants_df['VAF'] >= min_vaf)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"  Filtered from {len(variants_df)} to {len(filtered_variants)} variants\")\n",
    "        print(f\"    Minimum coverage: {min_coverage}\")\n",
    "        print(f\"    Minimum VAF: {min_vaf}\")\n",
    "        \n",
    "        # Get unique samples for this patient\n",
    "        samples = filtered_variants['SAMPLE'].unique()\n",
    "        print(f\"  Found {len(samples)} samples: {', '.join(samples)}\")\n",
    "        \n",
    "        # Create a comprehensive list of all mutations across all samples\n",
    "        all_mutations = set()\n",
    "        for _, row in filtered_variants.iterrows():\n",
    "            mutation_id = f\"{row['CHROM']}:{row['POS']}:{row['REF']}>{row['ALT']}\"\n",
    "            all_mutations.add(mutation_id)\n",
    "        \n",
    "        print(f\"  Found {len(all_mutations)} unique mutations across all samples\")\n",
    "        \n",
    "        # Process each sample\n",
    "        sample_pyclone_dfs = {}\n",
    "        \n",
    "        for sample in samples:\n",
    "            sample_data = filtered_variants[filtered_variants['SAMPLE'] == sample]\n",
    "            print(f\"  Processing sample {sample}: {len(sample_data)} variants\")\n",
    "            \n",
    "            # Create a set of mutations present in this sample\n",
    "            sample_mutations = set()\n",
    "            for _, row in sample_data.iterrows():\n",
    "                mutation_id = f\"{row['CHROM']}:{row['POS']}:{row['REF']}>{row['ALT']}\"\n",
    "                sample_mutations.add(mutation_id)\n",
    "            \n",
    "            # Convert to PyClone format\n",
    "            pyclone_data = []\n",
    "            \n",
    "            # Process mutations present in this sample\n",
    "            for _, row in sample_data.iterrows():\n",
    "                mutation_id = f\"{row['CHROM']}:{row['POS']}:{row['REF']}>{row['ALT']}\"\n",
    "                \n",
    "                # Set default copy number values (assuming diploid)\n",
    "                normal_cn = 2  # Assuming autosomal\n",
    "                major_cn = 1   # Default major copy number\n",
    "                minor_cn = 1   # Default minor copy number\n",
    "                \n",
    "                # Adjust for sex chromosomes\n",
    "                if row['CHROM'] in ['chrX', 'X', 'chrY', 'Y']:\n",
    "                    if row['CHROM'] in ['chrX', 'X']:\n",
    "                        # Set X chromosome values (assuming female)\n",
    "                        pass  # Using defaults\n",
    "                    elif row['CHROM'] in ['chrY', 'Y']:\n",
    "                        # Set Y chromosome values (assuming male)\n",
    "                        normal_cn = 1\n",
    "                        major_cn = 1\n",
    "                        minor_cn = 0\n",
    "                \n",
    "                # Add to PyClone data\n",
    "                pyclone_data.append({\n",
    "                    'mutation_id': mutation_id,\n",
    "                    'sample_id': sample,\n",
    "                    'ref_counts': row['REF_COUNT'],\n",
    "                    'alt_counts': row['ALT_COUNT'],\n",
    "                    'normal_cn': normal_cn,\n",
    "                    'major_cn': major_cn,\n",
    "                    'minor_cn': minor_cn,\n",
    "                    'tumor_content': default_tumor_content\n",
    "                })\n",
    "            \n",
    "            # Add missing mutations with default values (100 ref, 0 alt)\n",
    "            missing_mutations = all_mutations - sample_mutations\n",
    "            for mutation_id in missing_mutations:\n",
    "                # Parse chromosome from mutation ID\n",
    "                chrom = mutation_id.split(':')[0]\n",
    "                \n",
    "                # Set default copy number values (assuming diploid)\n",
    "                normal_cn = 2  # Assuming autosomal\n",
    "                major_cn = 1   # Default major copy number\n",
    "                minor_cn = 1   # Default minor copy number\n",
    "                \n",
    "                # Adjust for sex chromosomes\n",
    "                if chrom in ['chrX', 'X', 'chrY', 'Y']:\n",
    "                    if chrom in ['chrX', 'X']:\n",
    "                        # Set X chromosome values (assuming female)\n",
    "                        pass  # Using defaults\n",
    "                    elif chrom in ['chrY', 'Y']:\n",
    "                        # Set Y chromosome values (assuming male)\n",
    "                        normal_cn = 1\n",
    "                        major_cn = 1\n",
    "                        minor_cn = 0\n",
    "                \n",
    "                # Add to PyClone data with default counts\n",
    "                pyclone_data.append({\n",
    "                    'mutation_id': mutation_id,\n",
    "                    'sample_id': sample,\n",
    "                    'ref_counts': 100,  # Default reference count for missing mutations\n",
    "                    'alt_counts': 0,    # Default alternate count for missing mutations\n",
    "                    'normal_cn': normal_cn,\n",
    "                    'major_cn': major_cn,\n",
    "                    'minor_cn': minor_cn,\n",
    "                    'tumor_content': default_tumor_content\n",
    "                })\n",
    "            \n",
    "            # Create PyClone DataFrame for this sample\n",
    "            sample_pyclone_df = pd.DataFrame(pyclone_data)\n",
    "            sample_pyclone_dfs[sample] = sample_pyclone_df\n",
    "            \n",
    "            print(f\"    Added {len(missing_mutations)} missing mutations with default values\")\n",
    "        \n",
    "        # Combine all samples for this patient into a single PyClone file\n",
    "        combined_pyclone_df = pd.concat(sample_pyclone_dfs.values(), ignore_index=True)\n",
    "        \n",
    "        # Save combined file to patient's directory\n",
    "        output_file = os.path.join(output_dir, f\"{patient_id}_pyclone_input.tsv\")\n",
    "        combined_pyclone_df.to_csv(output_file, sep='\\t', index=False)\n",
    "        print(f\"  Saved combined PyClone input for patient {patient_id} to {output_file}\")\n",
    "        \n",
    "        return combined_pyclone_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing patient {patient_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Set parameters\n",
    "    results_directory = \"results\"  # Directory containing patient subdirectories\n",
    "    tumor_purity = 1.0            # Estimated tumor purity (0-1)\n",
    "    min_read_coverage = 50        # Minimum total reads required\n",
    "    min_vaf_threshold = 0.01     \n",
    "    \n",
    "    # Get all patient variant files\n",
    "    print(f\"Finding patient variant files in {results_directory}\")\n",
    "    patient_files = get_patient_variant_files(results_dir=results_directory)\n",
    "    \n",
    "    if not patient_files:\n",
    "        print(\"No patient variant files found. Exiting.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Process each patient independently\n",
    "    pyclone_results = {}\n",
    "    for patient_id, variant_file in patient_files.items():\n",
    "        pyclone_df = process_patient_variants(\n",
    "            patient_id,\n",
    "            variant_file,\n",
    "            default_tumor_content=tumor_purity,\n",
    "            min_coverage=min_read_coverage,\n",
    "            min_vaf=min_vaf_threshold\n",
    "        )\n",
    "        \n",
    "        if pyclone_df is not None:\n",
    "            pyclone_results[patient_id] = pyclone_df\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\nPyClone conversion complete:\")\n",
    "    for patient, data in pyclone_results.items():\n",
    "        print(f\"  Patient {patient}: {len(data)} variants processed across {len(data['sample_id'].unique())} samples\")\n",
    "        \n",
    "    # # Display the first few rows of the first patient's PyClone input\n",
    "    # if pyclone_results:\n",
    "    #     first_patient = list(pyclone_results.keys())[0]\n",
    "    #     print(f\"\\nFirst few rows of PyClone input for patient {first_patient}:\")\n",
    "    #     print(pyclone_results[first_patient].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pyclone-vi fit -i pyclone_input/combined_pyclone_input.tsv -o pyclone_results.h5 -c 40 -d beta-binomial -r 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nick/Library/Python/3.9/lib/python/site-packages/pyclone_vi/post_process.py:64: RuntimeWarning: invalid value encountered in sqrt\n",
      "  std = np.sqrt(var)\n"
     ]
    }
   ],
   "source": [
    "!pyclone-vi write-results-file -i pyclone_results.h5 -o pyclone_results.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CloneFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the VAF data into the format expected by CloneFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding patient variant files in results\n",
      "Found variant file for patient p191: results/p191/p191_variants.tsv\n",
      "Found variant file for patient P163: results/P163/P163_variants.tsv\n",
      "Found variant file for patient P190: results/P190/P190_variants.tsv\n",
      "Found variant file for patient p152: results/p152/p152_variants.tsv\n",
      "Found variant file for patient M36: results/M36/M36_variants.tsv\n",
      "Found variant file for patient G53: results/G53/G53_variants.tsv\n",
      "Found variant file for patient G39: results/G39/G39_variants.tsv\n",
      "Found variant file for patient G97: results/G97/G97_variants.tsv\n",
      "Found variant file for patient G52: results/G52/G52_variants.tsv\n",
      "Found variant file for patient G99: results/G99/G99_variants.tsv\n",
      "Found variant file for patient G77: results/G77/G77_variants.tsv\n",
      "Found variant file for patient G12: results/G12/G12_variants.tsv\n",
      "Found variant file for patient M24: results/M24/M24_variants.tsv\n",
      "Found variant file for patient G23: results/G23/G23_variants.tsv\n",
      "Found variant file for patient G40: results/G40/G40_variants.tsv\n",
      "Found variant file for patient M78: results/M78/M78_variants.tsv\n",
      "Found variant file for patient G85: results/G85/G85_variants.tsv\n",
      "Found variant file for patient p182: results/p182/p182_variants.tsv\n",
      "Found variant file for patient P158: results/P158/P158_variants.tsv\n",
      "Found variant file for patient G5: results/G5/G5_variants.tsv\n",
      "Found variant file for patient M95: results/M95/M95_variants.tsv\n",
      "Found variant file for patient G92: results/G92/G92_variants.tsv\n",
      "Found variant file for patient G59: results/G59/G59_variants.tsv\n",
      "Found variant file for patient G87: results/G87/G87_variants.tsv\n",
      "Found variant file for patient p065: results/p065/p065_variants.tsv\n",
      "Found variant file for patient G45: results/G45/G45_variants.tsv\n",
      "Found variant file for patient G43: results/G43/G43_variants.tsv\n",
      "Found variant file for patient p129: results/p129/p129_variants.tsv\n",
      "Processing patient p191...\n",
      "  Loaded 1585 variants\n",
      "  Filtered from 1585 to 1585 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 4 samples for patient p191: 41114, 41126, 41133, 42407\n",
      "  Including all 1071 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/p191/p191_clonefinder_input.txt\n",
      "Processing patient P163...\n",
      "  Loaded 3850 variants\n",
      "  Filtered from 3850 to 3850 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 6 samples for patient P163: 375363, 375380, 376121, 376137, 376213, 376223\n",
      "  Including all 2505 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/P163/P163_clonefinder_input.txt\n",
      "Processing patient P190...\n",
      "  Loaded 3483 variants\n",
      "  Filtered from 3483 to 3483 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 5 samples for patient P190: 359227, 359261, 359287, 359313, 359388\n",
      "  Including all 2118 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/P190/P190_clonefinder_input.txt\n",
      "Processing patient p152...\n",
      "  Loaded 4215 variants\n",
      "  Filtered from 4215 to 4215 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 7 samples for patient p152: 387720, 387930, 387942, 387947, 387979, 388519, 388615\n",
      "  Including all 2886 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/p152/p152_clonefinder_input.txt\n",
      "Processing patient M36...\n",
      "  Loaded 5331 variants\n",
      "  Filtered from 5331 to 5331 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 7 samples for patient M36: E, F, K, N, P, R, T\n",
      "  Including all 5065 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/M36/M36_clonefinder_input.txt\n",
      "Processing patient G53...\n",
      "  Loaded 7620 variants\n",
      "  Filtered from 7620 to 7620 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 9 samples for patient G53: A, B, C, D, E, F, I, J, K\n",
      "  Including all 5666 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/G53/G53_clonefinder_input.txt\n",
      "Processing patient G39...\n",
      "  Loaded 3851 variants\n",
      "  Filtered from 3851 to 3851 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 6 samples for patient G39: A, B, C, E, F, G\n",
      "  Including all 3003 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/G39/G39_clonefinder_input.txt\n",
      "Processing patient G97...\n",
      "  Loaded 32021 variants\n",
      "  Filtered from 32021 to 32021 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 8 samples for patient G97: B, C, D, E, F, G, H, I\n",
      "  Including all 23577 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/G97/G97_clonefinder_input.txt\n",
      "Processing patient G52...\n",
      "  Loaded 5098 variants\n",
      "  Filtered from 5098 to 5098 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 10 samples for patient G52: A, B, C, D, E, F, H, J, L, M\n",
      "  Including all 3451 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/G52/G52_clonefinder_input.txt\n",
      "Processing patient G99...\n",
      "  Loaded 45509 variants\n",
      "  Filtered from 45509 to 45509 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 4 samples for patient G99: E, F, I, K\n",
      "  Including all 17360 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/G99/G99_clonefinder_input.txt\n",
      "Processing patient G77...\n",
      "  Loaded 8844 variants\n",
      "  Filtered from 8844 to 8844 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 8 samples for patient G77: B, C, D, F, G, H, I, J\n",
      "  Including all 6344 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/G77/G77_clonefinder_input.txt\n",
      "Processing patient G12...\n",
      "  Loaded 8161 variants\n",
      "  Filtered from 8161 to 8161 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 4 samples for patient G12: B, C, E, F\n",
      "  Including all 3635 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/G12/G12_clonefinder_input.txt\n",
      "Processing patient M24...\n",
      "  Loaded 3305 variants\n",
      "  Filtered from 3305 to 3305 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 4 samples for patient M24: B, C, E, F\n",
      "  Including all 2172 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/M24/M24_clonefinder_input.txt\n",
      "Processing patient G23...\n",
      "  Loaded 5157 variants\n",
      "  Filtered from 5157 to 5157 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 6 samples for patient G23: B, B1, C, D, F, H\n",
      "  Including all 4438 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/G23/G23_clonefinder_input.txt\n",
      "Processing patient G40...\n",
      "  Loaded 4941 variants\n",
      "  Filtered from 4941 to 4941 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 9 samples for patient G40: A, C, D, E, F, G, I, K, M\n",
      "  Including all 3312 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/G40/G40_clonefinder_input.txt\n",
      "Processing patient M78...\n",
      "  Loaded 4871 variants\n",
      "  Filtered from 4871 to 4871 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 5 samples for patient M78: B, C, F, H, J\n",
      "  Including all 4516 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/M78/M78_clonefinder_input.txt\n",
      "Processing patient G85...\n",
      "  Loaded 8416 variants\n",
      "  Filtered from 8416 to 8416 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 7 samples for patient G85: B, C, D, F, G, H, I\n",
      "  Including all 6767 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/G85/G85_clonefinder_input.txt\n",
      "Processing patient p182...\n",
      "  Loaded 1905 variants\n",
      "  Filtered from 1905 to 1905 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 4 samples for patient p182: 111761, 111948, 112083, 11967\n",
      "  Including all 1627 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/p182/p182_clonefinder_input.txt\n",
      "Processing patient P158...\n",
      "  Loaded 2854 variants\n",
      "  Filtered from 2854 to 2854 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 5 samples for patient P158: 375150, 375266, 375281, 375284, 375546\n",
      "  Including all 2041 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/P158/P158_clonefinder_input.txt\n",
      "Processing patient G5...\n",
      "  Loaded 2967 variants\n",
      "  Filtered from 2967 to 2967 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 6 samples for patient G5: B, C, D, G, H, I\n",
      "  Including all 1922 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/G5/G5_clonefinder_input.txt\n",
      "Processing patient M95...\n",
      "  Loaded 5495 variants\n",
      "  Filtered from 5495 to 5495 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 6 samples for patient M95: B, C, D, F, L, M\n",
      "  Including all 4748 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/M95/M95_clonefinder_input.txt\n",
      "Processing patient G92...\n",
      "  Loaded 57596 variants\n",
      "  Filtered from 57596 to 57596 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 6 samples for patient G92: C, D, F, G, K, M\n",
      "  Including all 56916 variants (may not be present in all samples)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 283\u001b[0m\n\u001b[1;32m    281\u001b[0m clonefinder_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m patient_id, variant_file \u001b[38;5;129;01min\u001b[39;00m patient_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 283\u001b[0m     clonefinder_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_patient_variants_for_clonefinder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatient_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariant_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_coverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_read_coverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_vaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_vaf_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_simple_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Use simple S1, S2, etc. IDs\u001b[39;49;00m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequire_all_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Include variants not found in all samples\u001b[39;49;00m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_ref_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_ref_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Reference count placeholder for missing variants\u001b[39;49;00m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clonefinder_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m         clonefinder_results[patient_id] \u001b[38;5;241m=\u001b[39m clonefinder_df\n",
      "Cell \u001b[0;32mIn[22], line 178\u001b[0m, in \u001b[0;36mprocess_patient_variants_for_clonefinder\u001b[0;34m(patient_id, variant_file, min_coverage, min_vaf, use_simple_ids, require_all_samples, missing_ref_count)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var_id \u001b[38;5;129;01min\u001b[39;00m all_variant_ids:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m samples:\n\u001b[1;32m    177\u001b[0m         sample_var_data \u001b[38;5;241m=\u001b[39m filtered_variants[\n\u001b[0;32m--> 178\u001b[0m             (\u001b[43mfiltered_variants\u001b[49m\u001b[43m[\u001b[49m\u001b[43msnv_id_column\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvar_id\u001b[49m) \u001b[38;5;241m&\u001b[39m \n\u001b[1;32m    179\u001b[0m             (filtered_variants[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAMPLE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m sample)\n\u001b[1;32m    180\u001b[0m         ]\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sample_var_data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    183\u001b[0m             \u001b[38;5;66;03m# Variant exists in this sample\u001b[39;00m\n\u001b[1;32m    184\u001b[0m             ref_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(sample_var_data\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mREF_COUNT\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/series.py:6119\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6116\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   6117\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 6119\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/ops/array_ops.py:344\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 344\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/ops/array_ops.py:129\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    127\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# VCF to CloneFinder Converter - Final Version\n",
    "# This script reads variant data from TSV files in the results directory\n",
    "# and saves CloneFinder input files in the same patient directories\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "def get_patient_variant_files(results_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Find all patient variant TSV files in the results directory\n",
    "    \n",
    "    Args:\n",
    "        results_dir (str): Path to the results directory\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping patient IDs to their variant file paths\n",
    "    \"\"\"\n",
    "    patient_files = {}\n",
    "    \n",
    "    # Look for patient directories in the results directory\n",
    "    patient_dirs = glob.glob(os.path.join(results_dir, \"*\"))\n",
    "    \n",
    "    for patient_dir in patient_dirs:\n",
    "        if os.path.isdir(patient_dir):\n",
    "            # Extract patient ID from directory name\n",
    "            patient_id = os.path.basename(patient_dir)\n",
    "            \n",
    "            # Look for variant TSV file with patient ID prefix\n",
    "            variant_file_pattern = os.path.join(patient_dir, f\"{patient_id}_variants.tsv\")\n",
    "            variant_files = glob.glob(variant_file_pattern)\n",
    "            \n",
    "            if variant_files:\n",
    "                patient_files[patient_id] = variant_files[0]\n",
    "                print(f\"Found variant file for patient {patient_id}: {variant_files[0]}\")\n",
    "    \n",
    "    if not patient_files:\n",
    "        print(f\"No patient variant files found in {results_dir}\")\n",
    "    \n",
    "    return patient_files\n",
    "\n",
    "def process_patient_variants_for_clonefinder(patient_id, variant_file, \n",
    "                                           min_coverage=20, min_vaf=0.05,\n",
    "                                           use_simple_ids=False, \n",
    "                                           require_all_samples=False,\n",
    "                                           missing_ref_count=100):\n",
    "    \"\"\"\n",
    "    Process a single patient's variants and convert to CloneFinder format\n",
    "    \n",
    "    Args:\n",
    "        patient_id (str): Patient identifier\n",
    "        variant_file (str): Path to patient's variant TSV file\n",
    "        min_coverage (int): Minimum read coverage to include a variant\n",
    "        min_vaf (float): Minimum VAF to include a variant\n",
    "        use_simple_ids (bool): If True, generate simple SNV IDs (S1, S2, etc.)\n",
    "        require_all_samples (bool): If True, only include variants found in all samples\n",
    "        missing_ref_count (int): Reference count to use for variants not found in a sample\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: CloneFinder formatted data for the patient\n",
    "    \"\"\"\n",
    "    output_dir = os.path.dirname(variant_file)\n",
    "    output_file = os.path.join(output_dir, f\"{patient_id}_clonefinder_input.txt\")\n",
    "    \n",
    "    print(f\"Processing patient {patient_id}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read variant file\n",
    "        variants_df = pd.read_csv(variant_file, sep='\\t')\n",
    "        print(f\"  Loaded {len(variants_df)} variants\")\n",
    "        \n",
    "        # Check for required columns\n",
    "        required_cols = ['CHROM', 'POS', 'REF', 'ALT', 'REF_COUNT', 'ALT_COUNT', 'SAMPLE']\n",
    "        missing_columns = [col for col in required_cols if col not in variants_df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns in variant file: {', '.join(missing_columns)}\")\n",
    "        \n",
    "        # Ensure all string columns are strings (not integers/numerics)\n",
    "        variants_df['CHROM'] = variants_df['CHROM'].astype(str)\n",
    "        variants_df['POS'] = variants_df['POS'].astype(str)\n",
    "        variants_df['REF'] = variants_df['REF'].astype(str)\n",
    "        variants_df['ALT'] = variants_df['ALT'].astype(str)\n",
    "        variants_df['SAMPLE'] = variants_df['SAMPLE'].astype(str)\n",
    "        \n",
    "        # Ensure numeric columns are numeric\n",
    "        variants_df['REF_COUNT'] = pd.to_numeric(variants_df['REF_COUNT'])\n",
    "        variants_df['ALT_COUNT'] = pd.to_numeric(variants_df['ALT_COUNT'])\n",
    "        \n",
    "        # Filter variants by coverage and VAF\n",
    "        if 'TOTAL_COUNT' in variants_df.columns:\n",
    "            total_count = pd.to_numeric(variants_df['TOTAL_COUNT'])\n",
    "        else:\n",
    "            total_count = variants_df['REF_COUNT'] + variants_df['ALT_COUNT']\n",
    "            \n",
    "        if 'VAF' in variants_df.columns:\n",
    "            vaf = pd.to_numeric(variants_df['VAF'])\n",
    "        else:\n",
    "            vaf = variants_df['ALT_COUNT'] / total_count.replace(0, 1)  # Avoid division by zero\n",
    "        \n",
    "        filtered_variants = variants_df[\n",
    "            (total_count >= min_coverage) & \n",
    "            (vaf >= min_vaf)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"  Filtered from {len(variants_df)} to {len(filtered_variants)} variants\")\n",
    "        print(f\"    Minimum coverage: {min_coverage}\")\n",
    "        print(f\"    Minimum VAF: {min_vaf}\")\n",
    "        \n",
    "        if len(filtered_variants) == 0:\n",
    "            raise ValueError(\"No variants remain after filtering. Consider lowering thresholds.\")\n",
    "        \n",
    "        # Get list of unique samples for this patient\n",
    "        samples = sorted(filtered_variants['SAMPLE'].unique())\n",
    "        print(f\"  Found {len(samples)} samples for patient {patient_id}: {', '.join(samples)}\")\n",
    "        \n",
    "        # Create mutation IDs\n",
    "        if use_simple_ids:\n",
    "            # Create simple sequential IDs (S1, S2, etc.)\n",
    "            unique_variants = filtered_variants.drop_duplicates(['CHROM', 'POS', 'REF', 'ALT'])\n",
    "            id_map = {\n",
    "                (str(row['CHROM']), str(row['POS']), str(row['REF']), str(row['ALT'])): f\"S{i+1}\" \n",
    "                for i, (_, row) in enumerate(unique_variants.iterrows())\n",
    "            }\n",
    "            filtered_variants['SNVID'] = filtered_variants.apply(\n",
    "                lambda row: id_map[(str(row['CHROM']), str(row['POS']), str(row['REF']), str(row['ALT']))], axis=1\n",
    "            )\n",
    "            snv_id_column = 'SNVID'\n",
    "        else:\n",
    "            print(\"  Creating mutation IDs based on genomic coordinates\")\n",
    "            filtered_variants['SNVID'] = filtered_variants.apply(\n",
    "                lambda row: f\"{str(row['CHROM'])}:{str(row['POS'])}:{str(row['REF'])}>{str(row['ALT'])}\", axis=1\n",
    "            )\n",
    "            snv_id_column = 'SNVID'\n",
    "        \n",
    "        # Find variants across samples based on setting\n",
    "        if len(samples) > 1:\n",
    "            variant_counts = filtered_variants.groupby(snv_id_column)['SAMPLE'].nunique()\n",
    "            \n",
    "            if require_all_samples:\n",
    "                # Only keep variants present in all samples\n",
    "                common_variants = variant_counts[variant_counts == len(samples)].index.tolist()\n",
    "                \n",
    "                if len(common_variants) == 0:\n",
    "                    print(f\"  Warning: No variants found in all samples for patient {patient_id}. Consider setting require_all_samples=False\")\n",
    "                    if len(variant_counts) > 0:\n",
    "                        # Find variants in the maximum number of samples\n",
    "                        max_samples = variant_counts.max()\n",
    "                        common_variants = variant_counts[variant_counts == max_samples].index.tolist()\n",
    "                        print(f\"  Including {len(common_variants)} variants found in {max_samples}/{len(samples)} samples\")\n",
    "                else:\n",
    "                    print(f\"  Found {len(common_variants)} variants present in all {len(samples)} samples\")\n",
    "            else:\n",
    "                # Keep all variants\n",
    "                common_variants = filtered_variants[snv_id_column].unique()\n",
    "                print(f\"  Including all {len(common_variants)} variants (may not be present in all samples)\")\n",
    "                \n",
    "            filtered_variants = filtered_variants[filtered_variants[snv_id_column].isin(common_variants)]\n",
    "        \n",
    "        # Create a reference for the alleles\n",
    "        variant_alleles = {}\n",
    "        for _, row in filtered_variants.iterrows():\n",
    "            var_id = row[snv_id_column]\n",
    "            if var_id not in variant_alleles:\n",
    "                variant_alleles[var_id] = {\n",
    "                    'REF': str(row['REF']), \n",
    "                    'ALT': str(row['ALT'])\n",
    "                }\n",
    "        \n",
    "        # Create a complete sample/variant matrix with appropriate values for missing variants\n",
    "        all_variant_ids = filtered_variants[snv_id_column].unique()\n",
    "        all_combinations = []\n",
    "        \n",
    "        for var_id in all_variant_ids:\n",
    "            for sample in samples:\n",
    "                sample_var_data = filtered_variants[\n",
    "                    (filtered_variants[snv_id_column] == var_id) & \n",
    "                    (filtered_variants['SAMPLE'] == sample)\n",
    "                ]\n",
    "                \n",
    "                if len(sample_var_data) > 0:\n",
    "                    # Variant exists in this sample\n",
    "                    ref_count = int(sample_var_data.iloc[0]['REF_COUNT'])\n",
    "                    alt_count = int(sample_var_data.iloc[0]['ALT_COUNT'])\n",
    "                else:\n",
    "                    # Variant doesn't exist in this sample\n",
    "                    # Use placeholder value for reference count and 0 for alt count\n",
    "                    ref_count = missing_ref_count\n",
    "                    alt_count = 0\n",
    "                \n",
    "                all_combinations.append({\n",
    "                    snv_id_column: var_id,\n",
    "                    'SAMPLE': sample,\n",
    "                    'REF_COUNT': ref_count,\n",
    "                    'ALT_COUNT': alt_count\n",
    "                })\n",
    "        \n",
    "        # Create a new DataFrame with all combinations\n",
    "        complete_variants = pd.DataFrame(all_combinations)\n",
    "        \n",
    "        # Create pivot tables for the reference and alternate counts\n",
    "        pivot_ref = complete_variants.pivot_table(\n",
    "            index=snv_id_column, \n",
    "            columns='SAMPLE', \n",
    "            values='REF_COUNT', \n",
    "            aggfunc='first'\n",
    "        ).reset_index()\n",
    "        \n",
    "        pivot_alt = complete_variants.pivot_table(\n",
    "            index=snv_id_column, \n",
    "            columns='SAMPLE', \n",
    "            values='ALT_COUNT', \n",
    "            aggfunc='first'\n",
    "        )\n",
    "        \n",
    "        # Rename columns to match CloneFinder format\n",
    "        pivot_ref.columns.name = None\n",
    "        for sample in samples:\n",
    "            pivot_ref.rename(columns={sample: f\"{sample}:ref\"}, inplace=True)\n",
    "        \n",
    "        pivot_alt.columns.name = None\n",
    "        for sample in samples:\n",
    "            pivot_alt.rename(columns={sample: f\"{sample}:alt\"}, inplace=True)\n",
    "        \n",
    "        # Merge the pivot tables\n",
    "        clonefinder_data = pivot_ref.copy()\n",
    "        \n",
    "        # Add Wild and Mut columns (reference and alternate alleles)\n",
    "        clonefinder_data['Wild'] = clonefinder_data[snv_id_column].map(\n",
    "            {var_id: alleles['REF'] for var_id, alleles in variant_alleles.items()}\n",
    "        )\n",
    "        clonefinder_data['Mut'] = clonefinder_data[snv_id_column].map(\n",
    "            {var_id: alleles['ALT'] for var_id, alleles in variant_alleles.items()}\n",
    "        )\n",
    "        \n",
    "        # Reorder columns to match expected format\n",
    "        cols = [snv_id_column, 'Wild', 'Mut']\n",
    "        \n",
    "        # Add reference count columns\n",
    "        for sample in samples:\n",
    "            cols.append(f\"{sample}:ref\")\n",
    "        \n",
    "        # Add alternate count columns\n",
    "        for sample in samples:\n",
    "            cols.append(f\"{sample}:alt\")\n",
    "            clonefinder_data[f\"{sample}:alt\"] = pivot_alt[f\"{sample}:alt\"].values\n",
    "        \n",
    "        # Select and order columns\n",
    "        clonefinder_data = clonefinder_data[cols]\n",
    "        \n",
    "        # Save to file\n",
    "        clonefinder_data.to_csv(output_file, sep='\\t', index=False)\n",
    "        print(f\"  CloneFinder input saved to {output_file}\")\n",
    "        \n",
    "        return clonefinder_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing patient {patient_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()  # Print the full traceback for debugging\n",
    "        return None\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Set parameters\n",
    "    results_directory = \"results\"  # Directory containing patient subdirectories\n",
    "    min_read_coverage = 1         # Minimum total reads required\n",
    "    min_vaf_threshold = 0.0       # Minimum VAF required (0%)\n",
    "    missing_ref_value = 100       # Reference count for variants not detected in a sample\n",
    "    \n",
    "    # Get all patient variant files\n",
    "    print(f\"Finding patient variant files in {results_directory}\")\n",
    "    patient_files = get_patient_variant_files(results_dir=results_directory)\n",
    "    \n",
    "    if not patient_files:\n",
    "        print(\"No patient variant files found. Exiting.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Process each patient independently\n",
    "    clonefinder_results = {}\n",
    "    for patient_id, variant_file in patient_files.items():\n",
    "        clonefinder_df = process_patient_variants_for_clonefinder(\n",
    "            patient_id,\n",
    "            variant_file,\n",
    "            min_coverage=min_read_coverage,\n",
    "            min_vaf=min_vaf_threshold,\n",
    "            use_simple_ids=True,                # Use simple S1, S2, etc. IDs\n",
    "            require_all_samples=False,          # Include variants not found in all samples\n",
    "            missing_ref_count=missing_ref_value # Reference count placeholder for missing variants\n",
    "        )\n",
    "        \n",
    "        if clonefinder_df is not None:\n",
    "            clonefinder_results[patient_id] = clonefinder_df\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\nCloneFinder conversion complete:\")\n",
    "    for patient, data in clonefinder_results.items():\n",
    "        sample_count = len([col for col in data.columns if ':ref' in col])\n",
    "        print(f\"  Patient {patient}: {len(data)} variants processed across {sample_count} samples\")\n",
    "        \n",
    "    # Display the first few rows of the first patient's CloneFinder input\n",
    "    if clonefinder_results:\n",
    "        first_patient = list(clonefinder_results.keys())[0]\n",
    "        print(f\"\\nFirst few rows of CloneFinder input for patient {first_patient}:\")\n",
    "        print(clonefinder_results[first_patient].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### faster version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding patient variant files in results\n",
      "Found variant file for patient p065: results/p065/p065_variants.tsv\n",
      "Found variant file for patient p129: results/p129/p129_variants.tsv\n",
      "Skipped 26 patients with existing CloneFinder input files\n",
      "Using sequential processing\n",
      "Processing patient p065...\n",
      "  Loaded 37838 variants\n",
      "  Filtered from 37838 to 37838 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 6 samples for patient p065: 358799, 358802, 359303, 359306, 359319, 359322\n",
      "  Including all 37341 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/p065/p065_clonefinder_input.txt\n",
      "  Processing time: 0.92 seconds\n",
      "Processing patient p129...\n",
      "  Loaded 5492 variants\n",
      "  Filtered from 5492 to 5492 variants\n",
      "    Minimum coverage: 1\n",
      "    Minimum VAF: 0.0\n",
      "  Found 10 samples for patient p129: 375688, 375918, 375970, 375973, 376050, 376085, 376165, 376167, 376176, 376304\n",
      "  Including all 4061 variants (may not be present in all samples)\n",
      "  CloneFinder input saved to results/p129/p129_clonefinder_input.txt\n",
      "  Processing time: 0.14 seconds\n",
      "\n",
      "CloneFinder conversion complete:\n",
      "  Patient p065: 37341 variants processed across 6 samples\n",
      "  Patient p129: 4061 variants processed across 10 samples\n",
      "\n",
      "Total processing time: 1.08 seconds\n",
      "\n",
      "First few rows of CloneFinder input for patient p065:\n",
      "  SNVID Wild Mut  358799:ref  358802:ref  359303:ref  359306:ref  359319:ref  \\\n",
      "0    S1    C   T         100         100         107         100         100   \n",
      "1    S2    G   T         100         100          81         100         100   \n",
      "2    S3    T   G         100         100           5         100         100   \n",
      "3    S4    C   A         100         100         123         100         100   \n",
      "4    S5    C   T         100         100          49         100         100   \n",
      "\n",
      "   359322:ref  358799:alt  358802:alt  359303:alt  359306:alt  359319:alt  \\\n",
      "0         100           0           0           4           0           0   \n",
      "1         100           0           0           4           0           0   \n",
      "2         100           0           0           2           0           0   \n",
      "3         100           0           0           2           0           0   \n",
      "4         100           0           0           4           0           0   \n",
      "\n",
      "   359322:alt  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           0  \n",
      "4           0  \n"
     ]
    }
   ],
   "source": [
    "# VCF to CloneFinder Converter - Optimized Version\n",
    "# This script reads variant data from TSV files in the results directory\n",
    "# and saves CloneFinder input files in the same patient directories\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def get_patient_variant_files(results_dir=\"results\", skip_existing=True):\n",
    "    \"\"\"\n",
    "    Find all patient variant TSV files in the results directory\n",
    "    \n",
    "    Args:\n",
    "        results_dir (str): Path to the results directory\n",
    "        skip_existing (bool): Skip patients with existing CloneFinder input files\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping patient IDs to their variant file paths\n",
    "    \"\"\"\n",
    "    patient_files = {}\n",
    "    skipped_count = 0\n",
    "    \n",
    "    # Look for patient directories in the results directory\n",
    "    patient_dirs = glob.glob(os.path.join(results_dir, \"*\"))\n",
    "    \n",
    "    for patient_dir in patient_dirs:\n",
    "        if os.path.isdir(patient_dir):\n",
    "            # Extract patient ID from directory name\n",
    "            patient_id = os.path.basename(patient_dir)\n",
    "            \n",
    "            # Check if CloneFinder file already exists\n",
    "            clonefinder_file = os.path.join(patient_dir, f\"{patient_id}_clonefinder_input.txt\")\n",
    "            if skip_existing and os.path.exists(clonefinder_file):\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Look for variant TSV file with patient ID prefix\n",
    "            variant_file_pattern = os.path.join(patient_dir, f\"{patient_id}_variants.tsv\")\n",
    "            variant_files = glob.glob(variant_file_pattern)\n",
    "            \n",
    "            if variant_files:\n",
    "                patient_files[patient_id] = variant_files[0]\n",
    "                print(f\"Found variant file for patient {patient_id}: {variant_files[0]}\")\n",
    "    \n",
    "    if skipped_count > 0:\n",
    "        print(f\"Skipped {skipped_count} patients with existing CloneFinder input files\")\n",
    "    \n",
    "    if not patient_files:\n",
    "        print(f\"No patient variant files found in {results_dir} that need processing\")\n",
    "    \n",
    "    return patient_files\n",
    "\n",
    "def process_patient_variants_for_clonefinder(patient_id, variant_file, \n",
    "                                           min_coverage=20, min_vaf=0.05,\n",
    "                                           use_simple_ids=False, \n",
    "                                           require_all_samples=False,\n",
    "                                           missing_ref_count=100):\n",
    "    \"\"\"\n",
    "    Process a single patient's variants and convert to CloneFinder format\n",
    "    \n",
    "    Args:\n",
    "        patient_id (str): Patient identifier\n",
    "        variant_file (str): Path to patient's variant TSV file\n",
    "        min_coverage (int): Minimum read coverage to include a variant\n",
    "        min_vaf (float): Minimum VAF to include a variant\n",
    "        use_simple_ids (bool): If True, generate simple SNV IDs (S1, S2, etc.)\n",
    "        require_all_samples (bool): If True, only include variants found in all samples\n",
    "        missing_ref_count (int): Reference count to use for variants not found in a sample\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: CloneFinder formatted data for the patient\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    output_dir = os.path.dirname(variant_file)\n",
    "    output_file = os.path.join(output_dir, f\"{patient_id}_clonefinder_input.txt\")\n",
    "    \n",
    "    print(f\"Processing patient {patient_id}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read variant file - use optimized parameters\n",
    "        variants_df = pd.read_csv(variant_file, sep='\\t', \n",
    "                                 dtype={\n",
    "                                     'CHROM': str, \n",
    "                                     'POS': str, \n",
    "                                     'REF': str, \n",
    "                                     'ALT': str, \n",
    "                                     'SAMPLE': str,\n",
    "                                     'REF_COUNT': 'Int64',  # Changed to integer type\n",
    "                                     'ALT_COUNT': 'Int64'   # Changed to integer type\n",
    "                                 })\n",
    "        print(f\"  Loaded {len(variants_df)} variants\")\n",
    "        \n",
    "        # Check for required columns\n",
    "        required_cols = ['CHROM', 'POS', 'REF', 'ALT', 'REF_COUNT', 'ALT_COUNT', 'SAMPLE']\n",
    "        missing_columns = [col for col in required_cols if col not in variants_df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns in variant file: {', '.join(missing_columns)}\")\n",
    "        \n",
    "        # Convert any float counts to integers if they weren't already loaded as integers\n",
    "        if not pd.api.types.is_integer_dtype(variants_df['REF_COUNT']):\n",
    "            variants_df['REF_COUNT'] = variants_df['REF_COUNT'].round().astype('Int64')\n",
    "        \n",
    "        if not pd.api.types.is_integer_dtype(variants_df['ALT_COUNT']):\n",
    "            variants_df['ALT_COUNT'] = variants_df['ALT_COUNT'].round().astype('Int64')\n",
    "        \n",
    "        # Filter variants by coverage and VAF - vectorized operations\n",
    "        if 'TOTAL_COUNT' in variants_df.columns:\n",
    "            total_count = variants_df['TOTAL_COUNT']\n",
    "            if not pd.api.types.is_integer_dtype(total_count):\n",
    "                total_count = total_count.round().astype('Int64')\n",
    "        else:\n",
    "            total_count = (variants_df['REF_COUNT'] + variants_df['ALT_COUNT']).astype('Int64')\n",
    "            \n",
    "        if 'VAF' in variants_df.columns:\n",
    "            vaf = variants_df['VAF']\n",
    "        else:\n",
    "            # Fast vectorized calculation with safe division - convert to float for division\n",
    "            vaf = variants_df['ALT_COUNT'].astype(float) / np.maximum(total_count.astype(float), 1)  # Avoid division by zero\n",
    "        \n",
    "        # Apply filters in a single step\n",
    "        filtered_variants = variants_df[\n",
    "            (total_count >= min_coverage) & \n",
    "            (vaf >= min_vaf)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"  Filtered from {len(variants_df)} to {len(filtered_variants)} variants\")\n",
    "        print(f\"    Minimum coverage: {min_coverage}\")\n",
    "        print(f\"    Minimum VAF: {min_vaf}\")\n",
    "        \n",
    "        if len(filtered_variants) == 0:\n",
    "            raise ValueError(\"No variants remain after filtering. Consider lowering thresholds.\")\n",
    "        \n",
    "        # Get list of unique samples for this patient\n",
    "        samples = sorted(filtered_variants['SAMPLE'].unique())\n",
    "        print(f\"  Found {len(samples)} samples for patient {patient_id}: {', '.join(samples)}\")\n",
    "        \n",
    "        # Create mutation IDs\n",
    "        if use_simple_ids:\n",
    "            # Create unique variants array using pandas operations instead of loops\n",
    "            unique_variants = filtered_variants.drop_duplicates(['CHROM', 'POS', 'REF', 'ALT'])\n",
    "            \n",
    "            # Create efficient ID mapping\n",
    "            unique_variants = unique_variants.reset_index(drop=True)\n",
    "            unique_variants['SNVID'] = [f\"S{i+1}\" for i in range(len(unique_variants))]\n",
    "            \n",
    "            # Create a temporary key for joining\n",
    "            filtered_variants['temp_key'] = filtered_variants['CHROM'] + \":\" + \\\n",
    "                                          filtered_variants['POS'] + \":\" + \\\n",
    "                                          filtered_variants['REF'] + \":\" + \\\n",
    "                                          filtered_variants['ALT']\n",
    "            unique_variants['temp_key'] = unique_variants['CHROM'] + \":\" + \\\n",
    "                                        unique_variants['POS'] + \":\" + \\\n",
    "                                        unique_variants['REF'] + \":\" + \\\n",
    "                                        unique_variants['ALT']\n",
    "            \n",
    "            # Join to transfer SNVIDs to filtered_variants\n",
    "            id_map = unique_variants[['temp_key', 'SNVID']].set_index('temp_key')['SNVID']\n",
    "            filtered_variants['SNVID'] = filtered_variants['temp_key'].map(id_map)\n",
    "            filtered_variants.drop('temp_key', axis=1, inplace=True)\n",
    "            \n",
    "            snv_id_column = 'SNVID'\n",
    "        else:\n",
    "            print(\"  Creating mutation IDs based on genomic coordinates\")\n",
    "            # Vectorized string concatenation\n",
    "            filtered_variants['SNVID'] = filtered_variants['CHROM'] + \":\" + \\\n",
    "                                       filtered_variants['POS'] + \":\" + \\\n",
    "                                       filtered_variants['REF'] + \">\" + \\\n",
    "                                       filtered_variants['ALT']\n",
    "            snv_id_column = 'SNVID'\n",
    "        \n",
    "        # Find variants across samples based on setting\n",
    "        if len(samples) > 1:\n",
    "            # Count number of samples per variant\n",
    "            variant_counts = filtered_variants.groupby(snv_id_column)['SAMPLE'].nunique()\n",
    "            \n",
    "            if require_all_samples:\n",
    "                # Only keep variants present in all samples\n",
    "                common_variants = variant_counts[variant_counts == len(samples)].index.tolist()\n",
    "                \n",
    "                if len(common_variants) == 0:\n",
    "                    print(f\"  Warning: No variants found in all samples for patient {patient_id}. Consider setting require_all_samples=False\")\n",
    "                    if len(variant_counts) > 0:\n",
    "                        # Find variants in the maximum number of samples\n",
    "                        max_samples = variant_counts.max()\n",
    "                        common_variants = variant_counts[variant_counts == max_samples].index.tolist()\n",
    "                        print(f\"  Including {len(common_variants)} variants found in {max_samples}/{len(samples)} samples\")\n",
    "                else:\n",
    "                    print(f\"  Found {len(common_variants)} variants present in all {len(samples)} samples\")\n",
    "            else:\n",
    "                # Keep all variants\n",
    "                common_variants = filtered_variants[snv_id_column].unique()\n",
    "                print(f\"  Including all {len(common_variants)} variants (may not be present in all samples)\")\n",
    "                \n",
    "            filtered_variants = filtered_variants[filtered_variants[snv_id_column].isin(common_variants)]\n",
    "        \n",
    "        # Create a reference for the alleles (using pandas operations)\n",
    "        variant_alleles = filtered_variants[[snv_id_column, 'REF', 'ALT']].drop_duplicates(snv_id_column)\n",
    "        variant_alleles_dict = {\n",
    "            'REF': dict(zip(variant_alleles[snv_id_column], variant_alleles['REF'])),\n",
    "            'ALT': dict(zip(variant_alleles[snv_id_column], variant_alleles['ALT']))\n",
    "        }\n",
    "        \n",
    "        # Get all variants and samples\n",
    "        all_variant_ids = filtered_variants[snv_id_column].unique()\n",
    "        \n",
    "        # Optimization: Use a dictionary to store counts for faster lookup\n",
    "        counts_dict = {}\n",
    "        for _, row in filtered_variants.iterrows():\n",
    "            key = (row[snv_id_column], row['SAMPLE'])\n",
    "            counts_dict[key] = (int(row['REF_COUNT']), int(row['ALT_COUNT']))  # Explicitly convert to int\n",
    "        \n",
    "        # Create a complete matrix of variants and samples\n",
    "        result_data = []\n",
    "        for var_id in all_variant_ids:\n",
    "            row_data = {snv_id_column: var_id, \n",
    "                      'Wild': variant_alleles_dict['REF'][var_id], \n",
    "                      'Mut': variant_alleles_dict['ALT'][var_id]}\n",
    "            \n",
    "            # Add reference and alternate counts for each sample\n",
    "            for sample in samples:\n",
    "                key = (var_id, sample)\n",
    "                if key in counts_dict:\n",
    "                    ref_count, alt_count = counts_dict[key]\n",
    "                else:\n",
    "                    ref_count, alt_count = missing_ref_count, 0\n",
    "                \n",
    "                row_data[f\"{sample}:ref\"] = int(ref_count)  # Ensure integer values\n",
    "                row_data[f\"{sample}:alt\"] = int(alt_count)  # Ensure integer values\n",
    "            \n",
    "            result_data.append(row_data)\n",
    "        \n",
    "        # Create DataFrame from result data\n",
    "        clonefinder_data = pd.DataFrame(result_data)\n",
    "        \n",
    "        # Ensure all count columns are integers\n",
    "        for col in clonefinder_data.columns:\n",
    "            if ':ref' in col or ':alt' in col:\n",
    "                clonefinder_data[col] = clonefinder_data[col].astype(int)\n",
    "        \n",
    "        # Reorder columns to match expected format\n",
    "        ref_cols = [f\"{sample}:ref\" for sample in samples]\n",
    "        alt_cols = [f\"{sample}:alt\" for sample in samples]\n",
    "        cols = [snv_id_column, 'Wild', 'Mut'] + ref_cols + alt_cols\n",
    "        \n",
    "        # Select and order columns\n",
    "        existing_cols = [col for col in cols if col in clonefinder_data.columns]\n",
    "        clonefinder_data = clonefinder_data[existing_cols]\n",
    "        \n",
    "        # Check and add any missing columns\n",
    "        for col in cols:\n",
    "            if col not in clonefinder_data.columns:\n",
    "                if ':ref' in col:\n",
    "                    clonefinder_data[col] = int(missing_ref_count)\n",
    "                elif ':alt' in col:\n",
    "                    clonefinder_data[col] = 0\n",
    "        \n",
    "        # Final reordering\n",
    "        clonefinder_data = clonefinder_data[cols]\n",
    "        \n",
    "        # Save to file with integer format (no decimal places)\n",
    "        clonefinder_data.to_csv(output_file, sep='\\t', index=False, float_format='%d')\n",
    "        end_time = time.time()\n",
    "        print(f\"  CloneFinder input saved to {output_file}\")\n",
    "        print(f\"  Processing time: {end_time - start_time:.2f} seconds\")\n",
    "        \n",
    "        return clonefinder_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        print(f\"  Error processing patient {patient_id}: {e}\")\n",
    "        print(f\"  Processing time: {end_time - start_time:.2f} seconds\")\n",
    "        import traceback\n",
    "        traceback.print_exc()  # Print the full traceback for debugging\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_patient_parallel(args):\n",
    "    \"\"\"Wrapper for parallel processing\"\"\"\n",
    "    patient_id, variant_file, params = args\n",
    "    return process_patient_variants_for_clonefinder(\n",
    "        patient_id=patient_id,\n",
    "        variant_file=variant_file,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    # Set parameters\n",
    "    results_directory = \"results\"  # Directory containing patient subdirectories\n",
    "    min_read_coverage = 1         # Minimum total reads required\n",
    "    min_vaf_threshold = 0.0       # Minimum VAF required (0%)\n",
    "    missing_ref_value = 100       # Reference count for variants not detected in a sample\n",
    "    skip_existing_files = True    # Skip patients with existing CloneFinder input files\n",
    "    use_parallel = False          # Disable parallel processing by default due to potential issues\n",
    "    max_workers = 4               # Number of parallel workers\n",
    "    \n",
    "    # Process parameters\n",
    "    processing_params = {\n",
    "        'min_coverage': min_read_coverage,\n",
    "        'min_vaf': min_vaf_threshold,\n",
    "        'use_simple_ids': True,             # Use simple S1, S2, etc. IDs\n",
    "        'require_all_samples': False,       # Include variants not found in all samples\n",
    "        'missing_ref_count': missing_ref_value # Reference count placeholder for missing variants\n",
    "    }\n",
    "    \n",
    "    # Get all patient variant files, skipping those with existing output files\n",
    "    print(f\"Finding patient variant files in {results_directory}\")\n",
    "    patient_files = get_patient_variant_files(\n",
    "        results_dir=results_directory,\n",
    "        skip_existing=skip_existing_files\n",
    "    )\n",
    "    \n",
    "    if not patient_files:\n",
    "        print(\"No patient variant files found that need processing. Exiting.\")\n",
    "        exit(0)\n",
    "    \n",
    "    # Process each patient\n",
    "    clonefinder_results = {}\n",
    "    \n",
    "    # Use sequential processing to avoid multiprocessing issues\n",
    "    print(\"Using sequential processing\")\n",
    "    for patient_id, variant_file in patient_files.items():\n",
    "        clonefinder_df = process_patient_variants_for_clonefinder(\n",
    "            patient_id,\n",
    "            variant_file,\n",
    "            **processing_params\n",
    "        )\n",
    "        \n",
    "        if clonefinder_df is not None:\n",
    "            clonefinder_results[patient_id] = clonefinder_df\n",
    "    \n",
    "    # Display summary\n",
    "    end_time = time.time()\n",
    "    print(\"\\nCloneFinder conversion complete:\")\n",
    "    for patient, data in clonefinder_results.items():\n",
    "        sample_count = len([col for col in data.columns if ':ref' in col])\n",
    "        print(f\"  Patient {patient}: {len(data)} variants processed across {sample_count} samples\")\n",
    "    \n",
    "    print(f\"\\nTotal processing time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Display the first few rows of the first patient's CloneFinder input\n",
    "    if clonefinder_results:\n",
    "        first_patient = list(clonefinder_results.keys())[0]\n",
    "        print(f\"\\nFirst few rows of CloneFinder input for patient {first_patient}:\")\n",
    "        print(clonefinder_results[first_patient].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LICHeE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the VAF data into the format expected by LICHeE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding patient variant files in results\n",
      "Found variant file for patient G99: results/G99/G99_variants.tsv\n",
      "Processing patient G99...\n",
      "  Using minimum coverage: 400\n",
      "  Using minimum VAF: 0.05\n",
      "  Loaded 45509 variants\n",
      "  Filtered from 45509 to 755 variants\n",
      "  Found 4 tumor samples for patient G99: E, F, I, K\n",
      "  LICHeE input saved to results/G99/G99_lichee_input.txt\n"
     ]
    }
   ],
   "source": [
    "# VCF to LICHeE Converter\n",
    "# This script reads variant data from TSV files in the results directory\n",
    "# and saves LICHeE input files in the same patient directories\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "def get_patient_variant_files(results_dir=\"results\", specific_patient=None):\n",
    "    \"\"\"\n",
    "    Find all patient variant TSV files in the results directory\n",
    "    \n",
    "    Args:\n",
    "        results_dir (str): Path to the results directory\n",
    "        specific_patient (str, optional): Specific patient ID to process\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping patient IDs to their variant file paths\n",
    "    \"\"\"\n",
    "    patient_files = {}\n",
    "    \n",
    "    # Look for patient directories in the results directory\n",
    "    patient_dirs = glob.glob(os.path.join(results_dir, \"*\"))\n",
    "    \n",
    "    for patient_dir in patient_dirs:\n",
    "        if os.path.isdir(patient_dir):\n",
    "            # Extract patient ID from directory name\n",
    "            patient_id = os.path.basename(patient_dir)\n",
    "            \n",
    "            # Skip if specific patient is requested and this isn't it\n",
    "            if specific_patient and patient_id != specific_patient:\n",
    "                continue\n",
    "                \n",
    "            # Look for variant TSV file with patient ID prefix\n",
    "            variant_file_pattern = os.path.join(patient_dir, f\"{patient_id}_variants.tsv\")\n",
    "            variant_files = glob.glob(variant_file_pattern)\n",
    "            \n",
    "            if variant_files:\n",
    "                patient_files[patient_id] = variant_files[0]\n",
    "                print(f\"Found variant file for patient {patient_id}: {variant_files[0]}\")\n",
    "    \n",
    "    if not patient_files:\n",
    "        if specific_patient:\n",
    "            print(f\"No variant file found for patient {specific_patient} in {results_dir}\")\n",
    "        else:\n",
    "            print(f\"No patient variant files found in {results_dir}\")\n",
    "    \n",
    "    return patient_files\n",
    "\n",
    "def process_patient_variants_for_lichee(patient_id, variant_file, \n",
    "                                      min_coverage=20, min_vaf=0.05,\n",
    "                                      normal_sample=None, normal_vaf=0.0,\n",
    "                                      gene_column=None):\n",
    "    \"\"\"\n",
    "    Process a single patient's variants and convert to LICHeE format\n",
    "    \n",
    "    Args:\n",
    "        patient_id (str): Patient identifier\n",
    "        variant_file (str): Path to patient's variant TSV file\n",
    "        min_coverage (int): Minimum read coverage to include a variant\n",
    "        min_vaf (float): Minimum VAF to include a variant\n",
    "        normal_sample (str, optional): Name of normal sample if included in data\n",
    "        normal_vaf (float): VAF value to use for normal sample if not in data\n",
    "        gene_column (str, optional): Column name containing gene names\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: LICHeE formatted data for the patient\n",
    "    \"\"\"\n",
    "    output_dir = os.path.dirname(variant_file)\n",
    "    output_file = os.path.join(output_dir, f\"{patient_id}_lichee_input.txt\")\n",
    "    \n",
    "    print(f\"Processing patient {patient_id}...\")\n",
    "    print(f\"  Using minimum coverage: {min_coverage}\")\n",
    "    print(f\"  Using minimum VAF: {min_vaf}\")\n",
    "    \n",
    "    try:\n",
    "        # Read variant file\n",
    "        variants_df = pd.read_csv(variant_file, sep='\\t')\n",
    "        print(f\"  Loaded {len(variants_df)} variants\")\n",
    "        \n",
    "        # Check for required columns\n",
    "        required_cols = ['CHROM', 'POS', 'REF', 'ALT', 'REF_COUNT', 'ALT_COUNT', 'SAMPLE']\n",
    "        missing_columns = [col for col in required_cols if col not in variants_df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns in variant file: {', '.join(missing_columns)}\")\n",
    "        \n",
    "        # Ensure string columns are strings and numeric columns are numeric\n",
    "        variants_df['CHROM'] = variants_df['CHROM'].astype(str)\n",
    "        variants_df['POS'] = pd.to_numeric(variants_df['POS'])\n",
    "        variants_df['REF'] = variants_df['REF'].astype(str)\n",
    "        variants_df['ALT'] = variants_df['ALT'].astype(str)\n",
    "        variants_df['SAMPLE'] = variants_df['SAMPLE'].astype(str)\n",
    "        variants_df['REF_COUNT'] = pd.to_numeric(variants_df['REF_COUNT'])\n",
    "        variants_df['ALT_COUNT'] = pd.to_numeric(variants_df['ALT_COUNT'])\n",
    "        \n",
    "        # Calculate VAF if not provided\n",
    "        if 'VAF' not in variants_df.columns:\n",
    "            print(\"  Calculating VAF from REF_COUNT and ALT_COUNT\")\n",
    "            total_count = variants_df['REF_COUNT'] + variants_df['ALT_COUNT']\n",
    "            variants_df['VAF'] = variants_df['ALT_COUNT'] / total_count.replace(0, 1)  # Avoid division by zero\n",
    "        else:\n",
    "            variants_df['VAF'] = pd.to_numeric(variants_df['VAF'])\n",
    "        \n",
    "        # Filter variants by coverage and VAF and not mitochondrial DNA\n",
    "        total_count = variants_df['REF_COUNT'] + variants_df['ALT_COUNT']\n",
    "        \n",
    "        filtered_variants = variants_df[\n",
    "            (total_count >= min_coverage) & \n",
    "            (variants_df['VAF'] >= min_vaf) &\n",
    "            (variants_df['CHROM'] != 'chrM')\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"  Filtered from {len(variants_df)} to {len(filtered_variants)} variants\")\n",
    "        \n",
    "        if len(filtered_variants) == 0:\n",
    "            raise ValueError(\"No variants remain after filtering. Consider lowering thresholds.\")\n",
    "        \n",
    "        # Get list of unique samples for this patient\n",
    "        samples = sorted(filtered_variants['SAMPLE'].unique().astype(str))\n",
    "        \n",
    "        # Remove normal sample from tumor sample list if specified\n",
    "        if normal_sample and normal_sample in samples:\n",
    "            samples.remove(normal_sample)\n",
    "            print(f\"  Found normal sample: {normal_sample}\")\n",
    "        \n",
    "        print(f\"  Found {len(samples)} tumor samples for patient {patient_id}: {', '.join(samples)}\")\n",
    "        \n",
    "        # Create a description column\n",
    "        if gene_column and gene_column in filtered_variants.columns:\n",
    "            # Use gene name if available\n",
    "            filtered_variants['description'] = filtered_variants.apply(\n",
    "                lambda row: f\"{row['REF']}/{row['ALT']} {str(row[gene_column])}\", axis=1\n",
    "            )\n",
    "        else:\n",
    "            # Just use the allele change\n",
    "            filtered_variants['description'] = filtered_variants.apply(\n",
    "                lambda row: f\"{row['REF']}/{row['ALT']}\", axis=1\n",
    "            )\n",
    "        \n",
    "        # Convert chromosome format if needed (ensure \"chr\" prefix is removed)\n",
    "        filtered_variants['clean_chrom'] = filtered_variants['CHROM'].str.replace('chr', '')\n",
    "        \n",
    "        # Create a unique variant ID from chromosome and position\n",
    "        filtered_variants['var_id'] = filtered_variants.apply(\n",
    "            lambda row: f\"{row['clean_chrom']}_{row['POS']}_{row['REF']}_{row['ALT']}\", axis=1\n",
    "        )\n",
    "        \n",
    "        # Extract unique variants (same chr, pos, ref, alt may appear in multiple samples)\n",
    "        unique_variants = filtered_variants.drop_duplicates('var_id')\n",
    "        \n",
    "        # Create a pivot table with VAFs for each sample\n",
    "        pivot_vaf = filtered_variants.pivot_table(\n",
    "            index='var_id', \n",
    "            columns='SAMPLE', \n",
    "            values='VAF', \n",
    "            aggfunc='mean'  # Use mean in case of duplicates\n",
    "        ).fillna(0)  # Fill missing values with 0 (variant not found in that sample)\n",
    "        \n",
    "        # Get chromosome, position, and description for each variant\n",
    "        var_info = unique_variants.set_index('var_id')[['clean_chrom', 'POS', 'description']]\n",
    "        \n",
    "        # Merge VAFs with variant info\n",
    "        lichee_data = var_info.join(pivot_vaf).reset_index()\n",
    "        \n",
    "        # Create output DataFrame in LICHeE format\n",
    "        output_columns = ['clean_chrom', 'POS', 'description']\n",
    "        \n",
    "        # Add normal sample column if needed\n",
    "        if normal_sample and normal_sample in pivot_vaf.columns:\n",
    "            # Normal sample exists in the data\n",
    "            output_columns.append(normal_sample)\n",
    "        else:\n",
    "            # Add a placeholder normal sample with VAF 0.0\n",
    "            lichee_data['Normal'] = normal_vaf\n",
    "            output_columns.append('Normal')\n",
    "        \n",
    "        # Add tumor sample columns\n",
    "        output_columns.extend(samples)\n",
    "        \n",
    "        # Select and rename columns\n",
    "        lichee_data = lichee_data[output_columns]\n",
    "        lichee_data.columns = ['#chr', 'position', 'description'] + ([normal_sample] if normal_sample in pivot_vaf.columns else ['Normal']) + samples\n",
    "        \n",
    "        # Convert numeric values to appropriate format\n",
    "        lichee_data['position'] = lichee_data['position'].astype(int)\n",
    "        for sample in samples + (['Normal'] if 'Normal' in lichee_data.columns else [normal_sample]):\n",
    "            if sample is not None:  # Skip None values\n",
    "                lichee_data[sample] = lichee_data[sample].round(2)\n",
    "        \n",
    "        # Save to file\n",
    "        lichee_data.to_csv(output_file, sep='\\t', index=False, float_format='%.2f')\n",
    "        print(f\"  LICHeE input saved to {output_file}\")\n",
    "        \n",
    "        return lichee_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing patient {patient_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()  # Print the full traceback for debugging\n",
    "        return None\n",
    "\n",
    "\n",
    "results_directory = \"results\"  # Directory containing patient subdirectories\n",
    "min_read_coverage = 1         # Default minimum total reads required\n",
    "min_vaf_threshold = 0.05       # Minimum VAF required (5%)\n",
    "normal_sample = None           # Specify normal sample name if in your data\n",
    "normal_vaf = 0.0               # VAF to use for normal sample (usually 0.0)\n",
    "gene_column = None             # Column with gene names (if available)\n",
    "\n",
    "# Create a dictionary for patient-specific min_coverage values\n",
    "patient_coverage_map = {\n",
    "    \"G23\": 20,\n",
    "    \"G43\": 20,\n",
    "    \"G53\": 20,\n",
    "    \"G77\": 20,\n",
    "    \"G87\": 20,\n",
    "    \"G97\": 20,\n",
    "    \"G99\": 400,\n",
    "    \"M95\": 20,\n",
    "    \"p129\": 20,\n",
    "    \"P163\": 20,\n",
    "    \"P190\": 20\n",
    "}\n",
    "\n",
    "# Specify a single patient ID to process (set to None to process all patients)\n",
    "specific_patient_id = \"G99\" \n",
    "\n",
    "# Get all patient variant files (or specific patient if requested)\n",
    "print(f\"Finding patient variant files in {results_directory}\")\n",
    "patient_files = get_patient_variant_files(\n",
    "    results_dir=results_directory,\n",
    "    specific_patient=specific_patient_id\n",
    ")\n",
    "\n",
    "if not patient_files:\n",
    "    print(\"No patient variant files found. Exiting.\")\n",
    "else:\n",
    "    # Process each patient independently\n",
    "    lichee_results = {}\n",
    "    for patient_id, variant_file in patient_files.items():\n",
    "        # Use patient-specific coverage if available, otherwise use default\n",
    "        patient_min_coverage = patient_coverage_map.get(patient_id, min_read_coverage)\n",
    "        \n",
    "        lichee_df = process_patient_variants_for_lichee(\n",
    "            patient_id,\n",
    "            variant_file,\n",
    "            min_coverage=patient_min_coverage,\n",
    "            min_vaf=min_vaf_threshold,\n",
    "            normal_sample=normal_sample,\n",
    "            normal_vaf=normal_vaf,\n",
    "            gene_column=gene_column\n",
    "        )\n",
    "        \n",
    "        if lichee_df is not None:\n",
    "            lichee_results[patient_id] = lichee_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "./lichee -build -i ../../../lichee_input.txt -maxVAFAbsent 0.005 -minVAFPresent 0.005 -n 0 -showTree 1 -s 1 -v\n",
    "\n",
    "run the following from the lichee release folder\n",
    "\n",
    "need to leave off the -showTree 1 -s 1 for the file to generate\n",
    "\n",
    "i also had to use a high min read coverage threshold (150)\n",
    "\n",
    "./lichee -build -i ../../../lichee_input.txt -maxVAFAbsent 0.005 -minVAFPresent 0.005 -n 0 -minClusterSize 20 -minPrivateClusterSize 20\n",
    "\n",
    "i also had to rebuild the release jar according to the suggestions in this post https://github.com/viq854/lichee/issues/14\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run each tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def run_pyclone(patient_id, input_file, output_dir, chains=40, density=\"beta-binomial\", restarts=10):\n",
    "    \"\"\"\n",
    "    Run PyClone-VI for a patient\n",
    "    \n",
    "    Args:\n",
    "        patient_id (str): Patient identifier\n",
    "        input_file (str): Path to PyClone-VI input file\n",
    "        output_dir (str): Directory to save results\n",
    "        chains (int): Number of MCMC chains\n",
    "        density (str): Density to use (beta-binomial or binomial)\n",
    "        restarts (int): Number of restarts\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    print(f\"Running PyClone-VI for patient {patient_id}...\")\n",
    "    \n",
    "    # Create output file path\n",
    "    output_h5 = os.path.join(output_dir, f\"{patient_id}_pyclone_results.h5\")\n",
    "    output_tsv = os.path.join(output_dir, f\"{patient_id}_pyclone_results.tsv\")\n",
    "    \n",
    "    # Construct command for fit\n",
    "    fit_cmd = [\n",
    "        \"pyclone-vi\", \"fit\",\n",
    "        \"-i\", input_file,\n",
    "        \"-o\", output_h5,\n",
    "        \"-c\", str(chains),\n",
    "        \"-d\", density,\n",
    "        \"-r\", str(restarts)\n",
    "    ]\n",
    "    \n",
    "    print(f\"  Fit command: {' '.join(fit_cmd)}\")\n",
    "    \n",
    "    try:\n",
    "        # Run fit command\n",
    "        result = subprocess.run(fit_cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        print(f\"  PyClone-VI fit completed successfully: {output_h5}\")\n",
    "        \n",
    "        # Construct command for write-results-file\n",
    "        write_cmd = [\n",
    "            \"pyclone-vi\", \"write-results-file\",\n",
    "            \"-i\", output_h5,\n",
    "            \"-o\", output_tsv\n",
    "        ]\n",
    "        \n",
    "        print(f\"  Write results command: {' '.join(write_cmd)}\")\n",
    "        \n",
    "        # Run write-results-file command\n",
    "        write_result = subprocess.run(write_cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        print(f\"  PyClone-VI results written to TSV: {output_tsv}\")\n",
    "        \n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"  Error running PyClone-VI: {e}\")\n",
    "        print(f\"  stderr: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "def run_clonefinder(patient_id, input_file, output_dir, clonefinder_path):\n",
    "    \"\"\"\n",
    "    Run CloneFinder for a patient\n",
    "    \n",
    "    Args:\n",
    "        patient_id (str): Patient identifier\n",
    "        input_file (str): Path to CloneFinder input file\n",
    "        output_dir (str): Directory to save results\n",
    "        clonefinder_path (str): Path to CloneFinder directory\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    print(f\"Running CloneFinder for patient {patient_id}...\")\n",
    "    \n",
    "    # Get the basename of the input file\n",
    "    input_basename = os.path.basename(input_file)\n",
    "    \n",
    "    # Copy input file to CloneFinder directory (required by CloneFinder)\n",
    "    temp_input = os.path.join(clonefinder_path, input_basename)\n",
    "    shutil.copy2(input_file, temp_input)\n",
    "    \n",
    "    # Construct command\n",
    "    cmd = [\n",
    "        \"python3\", \"clonefinder.py\", \n",
    "        \"snv\", input_basename\n",
    "    ]\n",
    "    \n",
    "    print(f\"  Command: {' '.join(cmd)}\")\n",
    "    \n",
    "    try:\n",
    "        # Run command from CloneFinder directory\n",
    "        result = subprocess.run(cmd, cwd=clonefinder_path, check=True, \n",
    "                             stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        \n",
    "        # Move output files to patient directory\n",
    "        output_files = glob.glob(os.path.join(clonefinder_path, f\"{input_basename}*\"))\n",
    "        for file in output_files:\n",
    "            if os.path.isfile(file):\n",
    "                dest_file = os.path.join(output_dir, os.path.basename(file))\n",
    "                shutil.move(file, dest_file)\n",
    "                print(f\"  Moved output file to: {dest_file}\")\n",
    "        \n",
    "        # Move any other CloneFinder output files that might not match the input filename pattern\n",
    "        # (e.g., more general result files that CloneFinder might generate)\n",
    "        additional_outputs = glob.glob(os.path.join(clonefinder_path, f\"*{patient_id}*\"))\n",
    "        for file in additional_outputs:\n",
    "            if os.path.isfile(file):\n",
    "                dest_file = os.path.join(output_dir, os.path.basename(file))\n",
    "                shutil.move(file, dest_file)\n",
    "                print(f\"  Moved additional output file to: {dest_file}\")\n",
    "        \n",
    "        print(f\"  CloneFinder completed successfully: {output_dir}\")\n",
    "        \n",
    "        # Clean up temporary input file\n",
    "        if os.path.exists(temp_input):\n",
    "            os.remove(temp_input)\n",
    "            \n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"  Error running CloneFinder: {e}\")\n",
    "        print(f\"  stderr: {e.stderr}\")\n",
    "        \n",
    "        # Clean up temporary input file\n",
    "        if os.path.exists(temp_input):\n",
    "            os.remove(temp_input)\n",
    "            \n",
    "        return False\n",
    "\n",
    "\n",
    "def run_lichee_tool(patient_id, input_file, output_dir, lichee_path, \n",
    "              max_vaf_absent=0.005, min_vaf_present=0.005, \n",
    "              normal_sample_idx=0, min_cluster_size=20, min_private_cluster_size=20):\n",
    "    \"\"\"\n",
    "    Run LICHeE for a patient\n",
    "    \n",
    "    Args:\n",
    "        patient_id (str): Patient identifier\n",
    "        input_file (str): Path to LICHeE input file\n",
    "        output_dir (str): Directory to save results\n",
    "        lichee_path (str): Path to LICHeE executable\n",
    "        max_vaf_absent (float): Maximum VAF to consider a variant absent\n",
    "        min_vaf_present (float): Minimum VAF to consider a variant present\n",
    "        normal_sample_idx (int): Index of normal sample (0-based)\n",
    "        min_cluster_size (int): Minimum cluster size\n",
    "        min_private_cluster_size (int): Minimum private cluster size\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    print(f\"Running LICHeE for patient {patient_id}...\")\n",
    "    \n",
    "    # Create output directory path\n",
    "    patient_output_dir = os.path.join(output_dir, f\"{patient_id}_lichee_results\")\n",
    "    os.makedirs(patient_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Construct command\n",
    "    cmd = [\n",
    "        \"./lichee\",\n",
    "        \"-build\",\n",
    "        \"-i\", input_file,\n",
    "        \"-maxVAFAbsent\", str(max_vaf_absent),\n",
    "        \"-minVAFPresent\", str(min_vaf_present),\n",
    "        \"-n\", str(normal_sample_idx),\n",
    "        \"-minClusterSize\", str(min_cluster_size),\n",
    "        \"-minPrivateClusterSize\", str(min_private_cluster_size),\n",
    "        \"-o\", os.path.join(patient_output_dir, f\"{patient_id}_lichee_output\")\n",
    "    ]\n",
    "    \n",
    "    print(f\"  Command: {' '.join(cmd)}\")\n",
    "    \n",
    "    try:\n",
    "        # Run command\n",
    "        # result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        # /opt/homebrew/opt/openjdk/bin/java\n",
    "        env = os.environ.copy()\n",
    "        result = subprocess.run(cmd, cwd=lichee_path, check=True, shell=True, env=env,\n",
    "                             stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        print(f\"  LICHeE completed successfully: {patient_output_dir}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"  Error running LICHeE: {e}\")\n",
    "        print(f\"  stderr: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def run_tools_for_patient(patient_id, results_dir=\"results\", \n",
    "                         clonefinder_path=\"CloneFinder\",\n",
    "                         lichee_path=\"./lichee\",\n",
    "                         run_pyclone_vi=True, run_clone_finder=True, run_lichee=True):\n",
    "    \"\"\"\n",
    "    Run selected tools for a single patient\n",
    "    \n",
    "    Args:\n",
    "        patient_id (str): Patient identifier\n",
    "        results_dir (str): Base directory for results\n",
    "        clonefinder_path (str): Path to CloneFinder directory\n",
    "        run_pyclone_vi (bool): Whether to run PyClone-VI\n",
    "        run_clone_finder (bool): Whether to run CloneFinder\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with results for each tool\n",
    "    \"\"\"\n",
    "    # Get patient directory\n",
    "    patient_dir = os.path.join(results_dir, patient_id)\n",
    "    \n",
    "    if not os.path.isdir(patient_dir):\n",
    "        print(f\"Error: Patient directory '{patient_dir}' not found.\")\n",
    "        return {}\n",
    "    \n",
    "    # Find input files\n",
    "    pyclone_input = os.path.join(patient_dir, f\"{patient_id}_pyclone_input.tsv\")\n",
    "    clonefinder_input = os.path.join(patient_dir, f\"{patient_id}_clonefinder_input.txt\")\n",
    "    lichee_input = os.path.join(patient_dir, f\"{patient_id}_lichee_input.txt\")\n",
    "\n",
    "    # Check if input files exist and run tools\n",
    "    results = {}\n",
    "    \n",
    "    if run_pyclone_vi:\n",
    "        if os.path.isfile(pyclone_input):\n",
    "            results['pyclone'] = run_pyclone(patient_id, pyclone_input, patient_dir)\n",
    "        else:\n",
    "            print(f\"Warning: PyClone-VI input file '{pyclone_input}' not found.\")\n",
    "            results['pyclone'] = False\n",
    "    \n",
    "    if run_clone_finder:\n",
    "        if os.path.isfile(clonefinder_input):\n",
    "            results['clonefinder'] = run_clonefinder(patient_id, clonefinder_input, patient_dir, clonefinder_path)\n",
    "        else:\n",
    "            print(f\"Warning: CloneFinder input file '{clonefinder_input}' not found.\")\n",
    "            results['clonefinder'] = False\n",
    "    \n",
    "    if run_lichee:\n",
    "        if os.path.isfile(lichee_input):\n",
    "            results['lichee'] = run_lichee_tool(patient_id, lichee_input, patient_dir, lichee_path)\n",
    "        else:\n",
    "            print(f\"Warning: LICHeE input file '{lichee_input}' not found.\")\n",
    "            results['lichee'] = False\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def find_all_patients(results_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Find all patient directories in the results directory\n",
    "    \n",
    "    Args:\n",
    "        results_dir (str): Path to the results directory\n",
    "        \n",
    "    Returns:\n",
    "        list: List of patient IDs\n",
    "    \"\"\"\n",
    "    patient_dirs = [os.path.basename(d) for d in glob.glob(os.path.join(results_dir, \"*\")) \n",
    "                   if os.path.isdir(d)]\n",
    "    return patient_dirs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PyClone-VI for patient G12...\n",
      "  Fit command: pyclone-vi fit -i results/G12/G12_pyclone_input.tsv -o results/G12/G12_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/G12/G12_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/G12/G12_pyclone_results.h5 -o results/G12/G12_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/G12/G12_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient G23...\n",
      "  Fit command: pyclone-vi fit -i results/G23/G23_pyclone_input.tsv -o results/G23/G23_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/G23/G23_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/G23/G23_pyclone_results.h5 -o results/G23/G23_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/G23/G23_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient G39...\n",
      "  Fit command: pyclone-vi fit -i results/G39/G39_pyclone_input.tsv -o results/G39/G39_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/G39/G39_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/G39/G39_pyclone_results.h5 -o results/G39/G39_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/G39/G39_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient G40...\n",
      "  Fit command: pyclone-vi fit -i results/G40/G40_pyclone_input.tsv -o results/G40/G40_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/G40/G40_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/G40/G40_pyclone_results.h5 -o results/G40/G40_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/G40/G40_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient G43...\n",
      "  Fit command: pyclone-vi fit -i results/G43/G43_pyclone_input.tsv -o results/G43/G43_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/G43/G43_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/G43/G43_pyclone_results.h5 -o results/G43/G43_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/G43/G43_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient G45...\n",
      "  Fit command: pyclone-vi fit -i results/G45/G45_pyclone_input.tsv -o results/G45/G45_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/G45/G45_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/G45/G45_pyclone_results.h5 -o results/G45/G45_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/G45/G45_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient G5...\n",
      "  Fit command: pyclone-vi fit -i results/G5/G5_pyclone_input.tsv -o results/G5/G5_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/G5/G5_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/G5/G5_pyclone_results.h5 -o results/G5/G5_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/G5/G5_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient G52...\n",
      "  Fit command: pyclone-vi fit -i results/G52/G52_pyclone_input.tsv -o results/G52/G52_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/G52/G52_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/G52/G52_pyclone_results.h5 -o results/G52/G52_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/G52/G52_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient G53...\n",
      "  Fit command: pyclone-vi fit -i results/G53/G53_pyclone_input.tsv -o results/G53/G53_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/G53/G53_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/G53/G53_pyclone_results.h5 -o results/G53/G53_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/G53/G53_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient G59...\n",
      "  Fit command: pyclone-vi fit -i results/G59/G59_pyclone_input.tsv -o results/G59/G59_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/G59/G59_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/G59/G59_pyclone_results.h5 -o results/G59/G59_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/G59/G59_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient G77...\n",
      "  Fit command: pyclone-vi fit -i results/G77/G77_pyclone_input.tsv -o results/G77/G77_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/G77/G77_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/G77/G77_pyclone_results.h5 -o results/G77/G77_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/G77/G77_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient G85...\n",
      "  Fit command: pyclone-vi fit -i results/G85/G85_pyclone_input.tsv -o results/G85/G85_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/G85/G85_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/G85/G85_pyclone_results.h5 -o results/G85/G85_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/G85/G85_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient G87...\n",
      "  Fit command: pyclone-vi fit -i results/G87/G87_pyclone_input.tsv -o results/G87/G87_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/G87/G87_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/G87/G87_pyclone_results.h5 -o results/G87/G87_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/G87/G87_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient G92...\n",
      "  Fit command: pyclone-vi fit -i results/G92/G92_pyclone_input.tsv -o results/G92/G92_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/G92/G92_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/G92/G92_pyclone_results.h5 -o results/G92/G92_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/G92/G92_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient G97...\n",
      "  Fit command: pyclone-vi fit -i results/G97/G97_pyclone_input.tsv -o results/G97/G97_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/G97/G97_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/G97/G97_pyclone_results.h5 -o results/G97/G97_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/G97/G97_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient G99...\n",
      "  Fit command: pyclone-vi fit -i results/G99/G99_pyclone_input.tsv -o results/G99/G99_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/G99/G99_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/G99/G99_pyclone_results.h5 -o results/G99/G99_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/G99/G99_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient M24...\n",
      "  Fit command: pyclone-vi fit -i results/M24/M24_pyclone_input.tsv -o results/M24/M24_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/M24/M24_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/M24/M24_pyclone_results.h5 -o results/M24/M24_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/M24/M24_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient M36...\n",
      "  Fit command: pyclone-vi fit -i results/M36/M36_pyclone_input.tsv -o results/M36/M36_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/M36/M36_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/M36/M36_pyclone_results.h5 -o results/M36/M36_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/M36/M36_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient M78...\n",
      "  Fit command: pyclone-vi fit -i results/M78/M78_pyclone_input.tsv -o results/M78/M78_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/M78/M78_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/M78/M78_pyclone_results.h5 -o results/M78/M78_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/M78/M78_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient M95...\n",
      "  Fit command: pyclone-vi fit -i results/M95/M95_pyclone_input.tsv -o results/M95/M95_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/M95/M95_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/M95/M95_pyclone_results.h5 -o results/M95/M95_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/M95/M95_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient P158...\n",
      "  Fit command: pyclone-vi fit -i results/P158/P158_pyclone_input.tsv -o results/P158/P158_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/P158/P158_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/P158/P158_pyclone_results.h5 -o results/P158/P158_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/P158/P158_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient P163...\n",
      "  Fit command: pyclone-vi fit -i results/P163/P163_pyclone_input.tsv -o results/P163/P163_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/P163/P163_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/P163/P163_pyclone_results.h5 -o results/P163/P163_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/P163/P163_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient P190...\n",
      "  Fit command: pyclone-vi fit -i results/P190/P190_pyclone_input.tsv -o results/P190/P190_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/P190/P190_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/P190/P190_pyclone_results.h5 -o results/P190/P190_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/P190/P190_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient p065...\n",
      "  Fit command: pyclone-vi fit -i results/p065/p065_pyclone_input.tsv -o results/p065/p065_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/p065/p065_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/p065/p065_pyclone_results.h5 -o results/p065/p065_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/p065/p065_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient p129...\n",
      "  Fit command: pyclone-vi fit -i results/p129/p129_pyclone_input.tsv -o results/p129/p129_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/p129/p129_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/p129/p129_pyclone_results.h5 -o results/p129/p129_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/p129/p129_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient p152...\n",
      "  Fit command: pyclone-vi fit -i results/p152/p152_pyclone_input.tsv -o results/p152/p152_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/p152/p152_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/p152/p152_pyclone_results.h5 -o results/p152/p152_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/p152/p152_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient p182...\n",
      "  Fit command: pyclone-vi fit -i results/p182/p182_pyclone_input.tsv -o results/p182/p182_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/p182/p182_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/p182/p182_pyclone_results.h5 -o results/p182/p182_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/p182/p182_pyclone_results.tsv\n",
      "pyclone: Success\n",
      "Running PyClone-VI for patient p191...\n",
      "  Fit command: pyclone-vi fit -i results/p191/p191_pyclone_input.tsv -o results/p191/p191_pyclone_results.h5 -c 40 -d beta-binomial -r 10\n",
      "  PyClone-VI fit completed successfully: results/p191/p191_pyclone_results.h5\n",
      "  Write results command: pyclone-vi write-results-file -i results/p191/p191_pyclone_results.h5 -o results/p191/p191_pyclone_results.tsv\n",
      "  PyClone-VI results written to TSV: results/p191/p191_pyclone_results.tsv\n",
      "pyclone: Success\n"
     ]
    }
   ],
   "source": [
    "# Set paths to tools\n",
    "clonefinder_path = \"CloneFinder\"  # Change this to your CloneFinder directory\n",
    "lichee_path=\"./lichee/LICHeE/release\"\n",
    "\n",
    "for patient_id in patient_ids:\n",
    "    # Run PyClone-VI and CloneFinder for a specific patient\n",
    "    patient_id = patient_id  # Change this to your patient ID\n",
    "    results = run_tools_for_patient(\n",
    "        patient_id, \n",
    "        results_dir=\"results\",\n",
    "        clonefinder_path=clonefinder_path,\n",
    "        lichee_path=lichee_path,\n",
    "        run_pyclone_vi=True,  # Set to False to skip PyClone-VI\n",
    "        run_clone_finder=False,  # Set to False to skip CloneFinder\n",
    "        run_lichee=False\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    for tool, success in results.items():\n",
    "        status = \"Success\" if success else \"Failed\"\n",
    "        print(f\"{tool}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd CloneFinder/ && python3 clonefinder.py snv ../results/G45/G45_clonefinder_input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse results and create visualizations for each tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# def parse_pyclone_results(results_file):\n",
    "#     \"\"\"\n",
    "#     Parse PyClone-VI results into a format suitable for heatmap visualization,\n",
    "#     using cellular prevalence values to represent clone frequencies.\n",
    "    \n",
    "#     Args:\n",
    "#         results_file (str): Path to PyClone-VI results TSV file\n",
    "    \n",
    "#     Returns:\n",
    "#         pandas.DataFrame: DataFrame with samples as rows and clones as columns\n",
    "#     \"\"\"\n",
    "#     print(f\"Parsing PyClone-VI results from: {results_file}\")\n",
    "    \n",
    "#     # Load PyClone results\n",
    "#     df = pd.read_csv(results_file, sep='\\t')\n",
    "    \n",
    "#     # Get unique samples and clusters\n",
    "#     samples = df['sample_id'].unique()\n",
    "#     clusters = sorted(df['cluster_id'].unique())\n",
    "    \n",
    "#     print(f\"Found {len(samples)} samples and {len(clusters)} clusters\")\n",
    "    \n",
    "#     # Create a DataFrame to store mean cellular prevalence per cluster per sample\n",
    "#     prev_matrix = pd.DataFrame(index=samples, columns=[f'Clone {c}' for c in clusters])\n",
    "    \n",
    "#     # For each sample and cluster, calculate the mean cellular prevalence\n",
    "#     for sample in samples:\n",
    "#         sample_df = df[df['sample_id'] == sample]\n",
    "        \n",
    "#         for cluster in clusters:\n",
    "#             # Get cellular prevalence values for this sample and cluster\n",
    "#             cluster_data = sample_df[sample_df['cluster_id'] == cluster]\n",
    "            \n",
    "#             if not cluster_data.empty:\n",
    "#                 # Calculate mean cellular prevalence\n",
    "#                 mean_prev = cluster_data['cellular_prevalence'].mean()\n",
    "#                 prev_matrix.loc[sample, f'Clone {cluster}'] = mean_prev\n",
    "#                 # print(f\"Sample {sample}, Clone {cluster}: Mean prevalence = {mean_prev:.4f}\")\n",
    "#             else:\n",
    "#                 prev_matrix.loc[sample, f'Clone {cluster}'] = 0.0\n",
    "    \n",
    "#     # Fill any NaN values with 0\n",
    "#     prev_matrix = prev_matrix.fillna(0.0)\n",
    "    \n",
    "#     return prev_matrix\n",
    "\n",
    "def parse_pyclone_results(results_file):\n",
    "    \"\"\"\n",
    "    Parse PyClone-VI results into a format suitable for heatmap visualization,\n",
    "    using cellular prevalence values to represent clone frequencies.\n",
    "    \n",
    "    Args:\n",
    "        results_file (str): Path to PyClone-VI results TSV file\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with samples as rows and clones as columns\n",
    "    \"\"\"\n",
    "    print(f\"Parsing PyClone-VI results from: {results_file}\")\n",
    "    \n",
    "    # Load PyClone results\n",
    "    df = pd.read_csv(results_file, sep='\\t')\n",
    "    \n",
    "    # Get unique samples and clusters\n",
    "    samples = df['sample_id'].unique()\n",
    "    clusters = sorted(df['cluster_id'].unique())\n",
    "    print(f\"Found {len(samples)} samples and {len(clusters)} clusters\")\n",
    "    \n",
    "    # Create a DataFrame to store clone frequencies\n",
    "    freq_matrix = pd.DataFrame(index=samples, columns=[f'Clone {c}' for c in clusters])\n",
    "    \n",
    "    # Calculate frequencies for each sample\n",
    "    for sample in samples:\n",
    "        sample_df = df[df['sample_id'] == sample]\n",
    "        \n",
    "        # Filter clusters with cellular prevalence > 0 for this sample\n",
    "        valid_clusters = {}\n",
    "        \n",
    "        for cluster in clusters:\n",
    "            cluster_data = sample_df[sample_df['cluster_id'] == cluster]\n",
    "            if not cluster_data.empty:\n",
    "                mean_prevalence = cluster_data['cellular_prevalence'].mean()\n",
    "                if mean_prevalence > 0:\n",
    "                    valid_clusters[cluster] = mean_prevalence\n",
    "        \n",
    "        # Calculate total prevalence for normalization\n",
    "        total_prevalence = sum(valid_clusters.values())\n",
    "        \n",
    "        # Calculate normalized frequencies\n",
    "        if total_prevalence > 0:\n",
    "            for cluster in clusters:\n",
    "                if cluster in valid_clusters:\n",
    "                    # Normalize by total prevalence to get frequency\n",
    "                    freq = valid_clusters[cluster] / total_prevalence\n",
    "                    freq_matrix.loc[sample, f'Clone {cluster}'] = freq\n",
    "                else:\n",
    "                    freq_matrix.loc[sample, f'Clone {cluster}'] = 0.0\n",
    "        else:\n",
    "            # Set all frequencies to 0 if no clusters have prevalence > 0\n",
    "            for cluster in clusters:\n",
    "                freq_matrix.loc[sample, f'Clone {cluster}'] = 0.0\n",
    "    \n",
    "    # Fill any NaN values with 0\n",
    "    freq_matrix = freq_matrix.fillna(0.0)\n",
    "    \n",
    "    return freq_matrix\n",
    "\n",
    "def parse_clonefinder_results(results_file):\n",
    "    \"\"\"\n",
    "    Parse CloneFinder results into a format suitable for heatmap visualization\n",
    "    \n",
    "    Args:\n",
    "        results_file (str): Path to CloneFinder results file\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with samples as rows and clones as columns\n",
    "    \"\"\"\n",
    "    print(f\"Parsing CloneFinder results from: {results_file}\")\n",
    "    \n",
    "    # Load CloneFinder results\n",
    "    df = pd.read_csv(results_file, sep='\\t')\n",
    "    \n",
    "    # The first column contains sample IDs, and other columns are clone frequencies\n",
    "    # Rename the first column to 'Sample'\n",
    "    df = df.rename(columns={df.columns[0]: 'Sample'})\n",
    "    \n",
    "    # Set Sample column as index\n",
    "    df = df.set_index('Sample')\n",
    "    \n",
    "    # Sort columns by clone number\n",
    "    # Extract numeric part from clone names and sort\n",
    "    def get_clone_num(col_name):\n",
    "        # Extract numeric part from strings like \"Clone1\", \"Clone2\", etc.\n",
    "        if col_name.startswith('Clone'):\n",
    "            try:\n",
    "                return int(col_name[5:])\n",
    "            except ValueError:\n",
    "                return float('inf')  # Non-numeric parts go to the end\n",
    "        else:\n",
    "            try:\n",
    "                # Handle cases like \"1\", \"2\", etc.\n",
    "                return int(col_name)\n",
    "            except ValueError:\n",
    "                return float('inf')  # Non-numeric parts go to the end\n",
    "    \n",
    "    # Sort columns based on clone number\n",
    "    sorted_cols = sorted(df.columns, key=get_clone_num)\n",
    "    df = df[sorted_cols]\n",
    "    \n",
    "    # Convert all values to float (important for heatmap visualization)\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def parse_lichee_results(results_file):\n",
    "    \"\"\"\n",
    "    Parse LICHeE results into a format suitable for heatmap visualization,\n",
    "    correctly handling the hierarchical structure of clones\n",
    "    \n",
    "    Args:\n",
    "        results_file (str): Path to LICHeE results file\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with samples as rows and clones (nodes) as columns\n",
    "    \"\"\"\n",
    "    print(f\"Parsing LICHeE results from: {results_file}\")\n",
    "    \n",
    "    # Initialize data structures\n",
    "    lichee_data = {\n",
    "        'nodes': {},          # Store node info: binary profile, mutations, etc.\n",
    "        'tree': [],           # Store parent->child relationships\n",
    "        'node_mapping': {},   # Map binary profiles to node IDs\n",
    "        'sample_freqs': {}    # Store direct frequency of each node in each sample\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(results_file, 'r') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Parse Nodes section\n",
    "        nodes_section = re.search(r'Nodes:(.*?)(?=\\*\\*\\*\\*Tree)', content, re.DOTALL)\n",
    "        if nodes_section:\n",
    "            nodes_text = nodes_section.group(1).strip()\n",
    "            for line in nodes_text.split('\\n'):\n",
    "                if line.strip():\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) >= 4:  # Node ID, binary profile, frequencies, mutations...\n",
    "                        node_id = parts[0]\n",
    "                        binary_profile = parts[1]\n",
    "                        \n",
    "                        # Extract frequencies from the format like [ 0.45 0.45 0.41 0.32]\n",
    "                        frequencies_str = parts[2].strip('[]')\n",
    "                        frequencies = [float(f) for f in frequencies_str.split() if f]\n",
    "                        \n",
    "                        # Extract mutations\n",
    "                        mutations = parts[3:]\n",
    "                        \n",
    "                        lichee_data['nodes'][node_id] = {\n",
    "                            'binary_profile': binary_profile,\n",
    "                            'frequencies': frequencies,\n",
    "                            'mutations': mutations\n",
    "                        }\n",
    "                        \n",
    "                        # Map binary profile to node ID to help with tree parsing\n",
    "                        if binary_profile not in lichee_data['node_mapping']:\n",
    "                            lichee_data['node_mapping'][binary_profile] = []\n",
    "                        lichee_data['node_mapping'][binary_profile].append(node_id)\n",
    "            \n",
    "            print(f\"  Found {len(lichee_data['nodes'])} nodes\")\n",
    "            \n",
    "        # Parse Tree section to get parent-child relationships\n",
    "        tree_section = re.search(r'\\*\\*\\*\\*Tree.*?\\*\\*\\*\\*(.*?)(?=Sample decomposition:|$)', content, re.DOTALL)\n",
    "        if tree_section:\n",
    "            tree_text = tree_section.group(1).strip()\n",
    "            for line in tree_text.split('\\n'):\n",
    "                if '->' in line:\n",
    "                    parent, child = line.split('->')\n",
    "                    lichee_data['tree'].append((parent.strip(), child.strip()))\n",
    "            \n",
    "            print(f\"  Found {len(lichee_data['tree'])} parent-child relationships\")\n",
    "        \n",
    "        # Parse Sample decomposition section\n",
    "        sample_section = re.search(r'Sample decomposition:(.*?)(?=SNV info:|$)', content, re.DOTALL)\n",
    "        if sample_section:\n",
    "            sample_text = sample_section.group(1)\n",
    "            \n",
    "            # Initialize sample frequencies dictionary\n",
    "            for node_id in lichee_data['nodes'].keys():\n",
    "                lichee_data['sample_freqs'][f\"Node {node_id}\"] = {}\n",
    "            \n",
    "            # Parse each sample\n",
    "            current_sample = None\n",
    "            current_path = []  # Keep track of the hierarchical path\n",
    "            \n",
    "            lines = sample_text.split('\\n')\n",
    "            i = 0\n",
    "            while i < len(lines):\n",
    "                line = lines[i].strip()\n",
    "                \n",
    "                # Check for sample declaration\n",
    "                if 'Sample lineage decomposition:' in line:\n",
    "                    current_sample = line.split(':')[1].strip()\n",
    "                    current_path = []\n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                # Skip empty lines or GL line\n",
    "                if not line or line == 'GL':\n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                # Process lineage lines\n",
    "                if line.startswith('.'):\n",
    "                    # Determine the indentation level by counting dots\n",
    "                    dot_count = len(line) - len(line.lstrip('.'))\n",
    "                    \n",
    "                    # Truncate path to the appropriate level\n",
    "                    current_path = current_path[:dot_count]\n",
    "                    \n",
    "                    # Extract binary profile and frequency\n",
    "                    match = re.search(r'\\.+([01]+): ([0-9.]+)', line)\n",
    "                    if match:\n",
    "                        binary_profile = match.group(1)\n",
    "                        frequency = float(match.group(2))\n",
    "                        \n",
    "                        # Find the corresponding node ID\n",
    "                        if binary_profile in lichee_data['node_mapping']:\n",
    "                            # If multiple nodes have the same binary profile, we need to determine which one\n",
    "                            # is at this position in the hierarchy\n",
    "                            matched_nodes = lichee_data['node_mapping'][binary_profile]\n",
    "                            \n",
    "                            # For now, just use the first matching node (simplification)\n",
    "                            # In a more complex implementation, we would trace the tree structure\n",
    "                            node_id = matched_nodes[0]\n",
    "                            \n",
    "                            # More sophisticated approach: try to find the correct node by checking the tree\n",
    "                            if len(matched_nodes) > 1 and current_path:\n",
    "                                for potential_node in matched_nodes:\n",
    "                                    # Check if this node is a child of the last node in current_path\n",
    "                                    if any(parent == current_path[-1].split()[1] and child == potential_node \n",
    "                                          for parent, child in lichee_data['tree']):\n",
    "                                        node_id = potential_node\n",
    "                                        break\n",
    "                            \n",
    "                            # Store the frequency for this node in this sample\n",
    "                            node_key = f\"Node {node_id}\"\n",
    "                            if current_sample not in lichee_data['sample_freqs'][node_key]:\n",
    "                                lichee_data['sample_freqs'][node_key][current_sample] = frequency\n",
    "                            \n",
    "                            # Add this node to the current path\n",
    "                            current_path.append(node_key)\n",
    "                \n",
    "                i += 1\n",
    "            \n",
    "            # Extract samples\n",
    "            samples = [s for s in set([s for freqs in lichee_data['sample_freqs'].values() for s in freqs.keys()]) \n",
    "                      if s != 'Normal']\n",
    "            print(f\"  Found {len(samples)} samples: {', '.join(samples)}\")\n",
    "        \n",
    "        # Create DataFrame from sample frequencies\n",
    "        result_df = pd.DataFrame(index=samples)\n",
    "        \n",
    "        # Add columns for each node\n",
    "        for node_key, freqs in lichee_data['sample_freqs'].items():\n",
    "            result_df[node_key] = pd.Series({s: freqs.get(s, 0.0) for s in samples})\n",
    "        \n",
    "        # Fill NaN values with 0 (indicating absence of that clone in that sample)\n",
    "        result_df = result_df.fillna(0)\n",
    "        \n",
    "        # Print the DataFrame for debugging\n",
    "        print(f\"  Final DataFrame shape: {result_df.shape}\")\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing LICHeE file: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# def plot_sample_bar_graphs(pyclone_data, clonefinder_data, lichee_data, patient_id, output_dir):\n",
    "    \"\"\"\n",
    "    Create bar graphs for each sample showing clonal percentages from all three tools\n",
    "    and save them all to a single file\n",
    "    \"\"\"\n",
    "    # Get all sample IDs from all datasets\n",
    "    all_samples = set()\n",
    "    if pyclone_data is not None:\n",
    "        all_samples.update(pyclone_data.index)\n",
    "    if clonefinder_data is not None:\n",
    "        all_samples.update(clonefinder_data.index)\n",
    "    if lichee_data is not None:\n",
    "        all_samples.update(lichee_data.index)\n",
    "    \n",
    "    # Convert all samples to strings to ensure consistent typing\n",
    "    all_samples = {str(sample) for sample in all_samples}\n",
    "    \n",
    "    # Count the number of tools with data\n",
    "    available_tools = sum([pyclone_data is not None, clonefinder_data is not None, lichee_data is not None])\n",
    "    \n",
    "    # Create a single figure with subplots for all samples and tools\n",
    "    num_samples = len(all_samples)\n",
    "    fig, axes = plt.subplots(num_samples, available_tools, \n",
    "                            figsize=(6 * available_tools, 4 * num_samples),\n",
    "                            squeeze=False)\n",
    "    \n",
    "    # Sort samples for consistent ordering - now all are strings\n",
    "    sorted_samples = sorted(all_samples)\n",
    "    \n",
    "    for sample_idx, sample in enumerate(sorted_samples):\n",
    "        col_idx = 0\n",
    "        \n",
    "        # PyClone plot\n",
    "        if pyclone_data is not None:\n",
    "            # Check for both string and original type versions of the sample ID\n",
    "            if sample in pyclone_data.index:\n",
    "                sample_pyclone = pyclone_data.loc[sample]\n",
    "            elif sample.isdigit() and int(sample) in pyclone_data.index:\n",
    "                sample_pyclone = pyclone_data.loc[int(sample)]\n",
    "            else:\n",
    "                axes[sample_idx, col_idx].text(0.5, 0.5, \"No PyClone-VI Data\", \n",
    "                                            ha='center', va='center', fontsize=12)\n",
    "                axes[sample_idx, col_idx].set_title(f\"PyClone-VI: Sample {sample}\", fontsize=12)\n",
    "                col_idx += 1\n",
    "                continue\n",
    "                \n",
    "            axes[sample_idx, col_idx].bar(sample_pyclone.index, sample_pyclone.values)\n",
    "            axes[sample_idx, col_idx].set_title(f\"PyClone-VI: Sample {sample}\", fontsize=12)\n",
    "            axes[sample_idx, col_idx].set_ylim(0, 1.0)\n",
    "            plt.setp(axes[sample_idx, col_idx].get_xticklabels(), rotation=45, ha=\"right\", \n",
    "                    rotation_mode=\"anchor\", fontsize=10)\n",
    "            \n",
    "            # Only add y-label to the first column\n",
    "            if col_idx == 0:\n",
    "                axes[sample_idx, col_idx].set_ylabel(\"Frequency\", fontsize=12)\n",
    "                \n",
    "            col_idx += 1\n",
    "            \n",
    "        # CloneFinder plot - similar pattern for the other tools\n",
    "        if clonefinder_data is not None:\n",
    "            # Same fix for CloneFinder\n",
    "            if sample in clonefinder_data.index:\n",
    "                sample_clonefinder = clonefinder_data.loc[sample]\n",
    "            elif sample.isdigit() and int(sample) in clonefinder_data.index:\n",
    "                sample_clonefinder = clonefinder_data.loc[int(sample)]\n",
    "            else:\n",
    "                axes[sample_idx, col_idx].text(0.5, 0.5, \"No CloneFinder Data\", \n",
    "                                            ha='center', va='center', fontsize=12)\n",
    "                axes[sample_idx, col_idx].set_title(f\"CloneFinder: Sample {sample}\", fontsize=12)\n",
    "                col_idx += 1\n",
    "                continue\n",
    "                \n",
    "            axes[sample_idx, col_idx].bar(sample_clonefinder.index, sample_clonefinder.values)\n",
    "            axes[sample_idx, col_idx].set_title(f\"CloneFinder: Sample {sample}\", fontsize=12)\n",
    "            axes[sample_idx, col_idx].set_ylim(0, 1.0)\n",
    "            plt.setp(axes[sample_idx, col_idx].get_xticklabels(), rotation=45, ha=\"right\", \n",
    "                    rotation_mode=\"anchor\", fontsize=10)\n",
    "                \n",
    "            # Only add y-label to the first column if it's the first tool\n",
    "            if col_idx == 0:\n",
    "                axes[sample_idx, col_idx].set_ylabel(\"Frequency\", fontsize=12)\n",
    "                \n",
    "            col_idx += 1\n",
    "            \n",
    "        # LICHeE plot - same fix pattern\n",
    "        if lichee_data is not None:\n",
    "            if sample in lichee_data.index:\n",
    "                sample_lichee = lichee_data.loc[sample]\n",
    "            elif sample.isdigit() and int(sample) in lichee_data.index:\n",
    "                sample_lichee = lichee_data.loc[int(sample)]\n",
    "            else:\n",
    "                axes[sample_idx, col_idx].text(0.5, 0.5, \"No LICHeE Data\", \n",
    "                                            ha='center', va='center', fontsize=12)\n",
    "                axes[sample_idx, col_idx].set_title(f\"LICHeE: Sample {sample}\", fontsize=12)\n",
    "                col_idx += 1\n",
    "                continue\n",
    "                \n",
    "            axes[sample_idx, col_idx].bar(sample_lichee.index, sample_lichee.values)\n",
    "            axes[sample_idx, col_idx].set_title(f\"LICHeE: Sample {sample}\", fontsize=12)\n",
    "            axes[sample_idx, col_idx].set_ylim(0, 1.0)\n",
    "            plt.setp(axes[sample_idx, col_idx].get_xticklabels(), rotation=45, ha=\"right\", \n",
    "                    rotation_mode=\"anchor\", fontsize=10)\n",
    "                \n",
    "            # Only add y-label to the first column if it's the first tool\n",
    "            if col_idx == 0:\n",
    "                axes[sample_idx, col_idx].set_ylabel(\"Frequency\", fontsize=12)\n",
    "    \n",
    "    # Add a main title for the entire figure\n",
    "    fig.suptitle(f\"Clonal Composition Comparison - Patient {patient_id}\", fontsize=16, y=0.99)\n",
    "    \n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])  # Leave room for suptitle\n",
    "    \n",
    "    # Save the combined figure\n",
    "    save_path = os.path.join(output_dir, f\"{patient_id}_all_samples_bar_graphs.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"Saved combined bar graphs for all samples to: {save_path}\")\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "def plot_clone_heatmap(data, title, cmap='coolwarm', figsize=(10, 8), save_path=None):\n",
    "    \"\"\"\n",
    "    Plot a heatmap visualization of clone frequencies\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.DataFrame): DataFrame with samples as rows and clones as columns\n",
    "        title (str): Title for the heatmap\n",
    "        cmap (str): Colormap to use (default: 'coolwarm' - blue for low, red for high)\n",
    "        figsize (tuple): Figure size (width, height)\n",
    "        save_path (str, optional): Path to save the figure\n",
    "    \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The heatmap figure\n",
    "    \"\"\"\n",
    "    print(data)\n",
    "    # Ensure data is numeric\n",
    "    data_numeric = data.copy()\n",
    "    # Convert to float - this will help identify any non-numeric values\n",
    "    for col in data_numeric.columns:\n",
    "        data_numeric[col] = pd.to_numeric(data_numeric[col], errors='coerce')\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create heatmap with numeric data - using coolwarm colormap (blue for low, red for high)\n",
    "    ax = sns.heatmap(data_numeric, annot=True, cmap=cmap, \n",
    "                     vmin=0, vmax=1, \n",
    "                     fmt='.2f', linewidths=.5)\n",
    "    \n",
    "    # Set title and labels\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.ylabel('Samples', fontsize=12)\n",
    "    plt.xlabel('Clones', fontsize=12)\n",
    "    \n",
    "    # Rotate y-axis labels\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure if path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved figure to: {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_clonality_for_patient(patient_id, results_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Visualize PyClone-VI, CloneFinder, and LICHeE results for a patient and save the heatmaps\n",
    "    and sample-specific bar graphs directly in the patient's results directory\n",
    "    \n",
    "    Args:\n",
    "        patient_id (str): Patient identifier\n",
    "        results_dir (str): Base directory for results\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with paths to generated figures\n",
    "    \"\"\"\n",
    "    # Get patient directory\n",
    "    patient_dir = os.path.join(results_dir, patient_id)\n",
    "    \n",
    "    if not os.path.isdir(patient_dir):\n",
    "        print(f\"Error: Patient directory '{patient_dir}' not found.\")\n",
    "        return {}\n",
    "    \n",
    "    outputs = {}\n",
    "    pyclone_data = None\n",
    "    clonefinder_data = None\n",
    "    lichee_data = None\n",
    "    \n",
    "    # Check for and process PyClone-VI results\n",
    "    pyclone_file = os.path.join(patient_dir, f\"{patient_id}_pyclone_results.tsv\")\n",
    "    if os.path.isfile(pyclone_file):\n",
    "        print(f\"Processing PyClone-VI results for patient {patient_id}...\")\n",
    "        \n",
    "        # Parse results using our fixed function\n",
    "        pyclone_data = parse_pyclone_results(pyclone_file)\n",
    "        \n",
    "        # Create heatmap visualization and save directly in patient directory\n",
    "        save_path = os.path.join(patient_dir, f\"{patient_id}_pyclone_heatmap.png\")\n",
    "        fig = plot_clone_heatmap(pyclone_data, \n",
    "                               title=f\"PyClone-VI Clonal Composition - Patient {patient_id}\",\n",
    "                               save_path=save_path)\n",
    "        plt.close(fig)  # Close figure to avoid display in notebook\n",
    "        \n",
    "        outputs['pyclone_heatmap'] = save_path\n",
    "        \n",
    "        print(f\"PyClone-VI heatmap saved to: {save_path}\")\n",
    "    else:\n",
    "        print(f\"Warning: PyClone-VI results file '{pyclone_file}' not found.\")\n",
    "        \n",
    "    # Check for and process CloneFinder results\n",
    "    clonefinder_file = os.path.join(patient_dir, f\"{patient_id}_clonefinder_input_CloneFinder.txt\")\n",
    "    if os.path.isfile(clonefinder_file):\n",
    "        print(f\"Processing CloneFinder results for patient {patient_id}...\")\n",
    "        \n",
    "        # Parse results\n",
    "        clonefinder_data = parse_clonefinder_results(clonefinder_file)\n",
    "        \n",
    "        # Create heatmap visualization and save directly in patient directory\n",
    "        save_path = os.path.join(patient_dir, f\"{patient_id}_clonefinder_heatmap.png\")\n",
    "        fig = plot_clone_heatmap(clonefinder_data, \n",
    "                               title=f\"CloneFinder Clonal Composition - Patient {patient_id}\",\n",
    "                               save_path=save_path)\n",
    "        plt.close(fig)  # Close figure to avoid display in notebook\n",
    "        \n",
    "        outputs['clonefinder_heatmap'] = save_path\n",
    "        \n",
    "        print(f\"CloneFinder heatmap saved to: {save_path}\")\n",
    "    else:\n",
    "        # Try alternative file naming patterns\n",
    "        alt_pattern = os.path.join(patient_dir, f\"{patient_id}_clonefinder_inputsnv_CloneFinder.txt\")\n",
    "        if os.path.isfile(alt_pattern):\n",
    "            print(f\"Processing CloneFinder results for patient {patient_id} (using alternative file)...\")\n",
    "            \n",
    "            # Parse results\n",
    "            clonefinder_data = parse_clonefinder_results(alt_pattern)\n",
    "            \n",
    "            # Create heatmap visualization and save directly in patient directory\n",
    "            save_path = os.path.join(patient_dir, f\"{patient_id}_clonefinder_heatmap.png\")\n",
    "            fig = plot_clone_heatmap(clonefinder_data, \n",
    "                                   title=f\"CloneFinder Clonal Composition - Patient {patient_id}\",\n",
    "                                   save_path=save_path)\n",
    "            plt.close(fig)  # Close figure to avoid display in notebook\n",
    "            \n",
    "            outputs['clonefinder_heatmap'] = save_path\n",
    "            \n",
    "            print(f\"CloneFinder heatmap saved to: {save_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: CloneFinder results file not found for patient {patient_id}.\")\n",
    "    \n",
    "    # Check for and process LICHeE results\n",
    "    lichee_file = os.path.join(patient_dir, f\"{patient_id}_lichee_input.txt.trees.txt\")\n",
    "    if os.path.isfile(lichee_file):\n",
    "        print(f\"Processing LICHeE results for patient {patient_id}...\")\n",
    "        \n",
    "        # Parse results\n",
    "        lichee_data = parse_lichee_results(lichee_file)\n",
    "        \n",
    "        if not lichee_data.empty:\n",
    "            # Create heatmap visualization and save directly in patient directory\n",
    "            save_path = os.path.join(patient_dir, f\"{patient_id}_lichee_heatmap.png\")\n",
    "            fig = plot_clone_heatmap(lichee_data, \n",
    "                                   title=f\"LICHeE Clonal Composition - Patient {patient_id}\",\n",
    "                                   save_path=save_path)\n",
    "            plt.close(fig)  # Close figure to avoid display in notebook\n",
    "            \n",
    "            outputs['lichee_heatmap'] = save_path\n",
    "            \n",
    "            print(f\"LICHeE heatmap saved to: {save_path}\")\n",
    "    else:\n",
    "        # Try alternative file naming patterns\n",
    "        alt_patterns = [\n",
    "            os.path.join(patient_dir, f\"{patient_id}.lichee.output.txt\"),\n",
    "            os.path.join(patient_dir, f\"{patient_id}_lichee_output.txt\"),\n",
    "            os.path.join(patient_dir, f\"{patient_id}_trees.txt\")\n",
    "        ]\n",
    "        \n",
    "        found_file = False\n",
    "        for alt_pattern in alt_patterns:\n",
    "            if os.path.isfile(alt_pattern):\n",
    "                print(f\"Processing LICHeE results for patient {patient_id} (using file: {alt_pattern})...\")\n",
    "                \n",
    "                # Parse results\n",
    "                lichee_data = parse_lichee_results(alt_pattern)\n",
    "                \n",
    "                if not lichee_data.empty:\n",
    "                    # Create heatmap visualization and save directly in patient directory\n",
    "                    save_path = os.path.join(patient_dir, f\"{patient_id}_lichee_heatmap.png\")\n",
    "                    fig = plot_clone_heatmap(lichee_data, \n",
    "                                           title=f\"LICHeE Clonal Composition - Patient {patient_id}\",\n",
    "                                           save_path=save_path)\n",
    "                    plt.close(fig)  # Close figure to avoid display in notebook\n",
    "                    \n",
    "                    outputs['lichee_heatmap'] = save_path\n",
    "                    \n",
    "                    print(f\"LICHeE heatmap saved to: {save_path}\")\n",
    "                    found_file = True\n",
    "                    break\n",
    "        \n",
    "        if not found_file:\n",
    "            print(f\"Warning: LICHeE results file not found for patient {patient_id}.\")\n",
    "    \n",
    "    # Create sample-specific bar graphs if we have data from at least one tool\n",
    "    data_sources = [d for d in [pyclone_data, clonefinder_data, lichee_data] if d is not None]\n",
    "    if len(data_sources) >= 1:\n",
    "        print(f\"Creating combined bar graphs for patient {patient_id}...\")\n",
    "        bar_graph_file = plot_sample_bar_graphs(pyclone_data, clonefinder_data, lichee_data,\n",
    "                                             patient_id, patient_dir)\n",
    "        outputs['sample_bar_graphs'] = bar_graph_file\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def visualize_all_patients(results_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Visualize PyClone-VI, CloneFinder, and LICHeE results for all patients and save\n",
    "    directly in each patient's directory\n",
    "    \n",
    "    Args:\n",
    "        results_dir (str): Base directory for results\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with paths to generated figures for each patient\n",
    "    \"\"\"\n",
    "    # Find all patient directories\n",
    "    patient_dirs = [d for d in os.listdir(results_dir) \n",
    "                   if os.path.isdir(os.path.join(results_dir, d))]\n",
    "    \n",
    "    if not patient_dirs:\n",
    "        print(f\"No patient directories found in {results_dir}\")\n",
    "        return {}\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for patient_id in patient_dirs:\n",
    "        outputs = visualize_clonality_for_patient(patient_id, results_dir)\n",
    "        if outputs:\n",
    "            results[patient_id] = outputs\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_all_patient_ids(results_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Get all patient IDs by listing subdirectories under the results directory.\n",
    "    \n",
    "    Args:\n",
    "        results_dir (str): Path to the base results directory\n",
    "        \n",
    "    Returns:\n",
    "        list: List of patient IDs (directory names)\n",
    "    \"\"\"\n",
    "    # Check if the results directory exists\n",
    "    if not os.path.isdir(results_dir):\n",
    "        print(f\"Warning: Results directory '{results_dir}' not found.\")\n",
    "        return []\n",
    "    \n",
    "    # Get all directories (potential patient IDs)\n",
    "    patient_ids = []\n",
    "    for item in os.listdir(results_dir):\n",
    "        full_path = os.path.join(results_dir, item)\n",
    "        # Only include directories (not files)\n",
    "        if os.path.isdir(full_path):\n",
    "            patient_ids.append(item)\n",
    "    \n",
    "    print(f\"Found {len(patient_ids)} patient directories: {', '.join(patient_ids)}\")\n",
    "    return sorted(patient_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PyClone-VI results for patient G12...\n",
      "Parsing PyClone-VI results from: results/G12/G12_pyclone_results.tsv\n",
      "Found 4 samples and 13 clusters\n",
      "    Clone 0   Clone 1   Clone 2   Clone 3  Clone 4   Clone 5   Clone 6  \\\n",
      "B  0.093872  0.000000  0.171427  0.005709  0.00000  0.000000  0.184994   \n",
      "C  0.082438  0.165917  0.152502  0.000000  0.00000  0.161551  0.070769   \n",
      "E  0.089788  0.000000  0.139395  0.006949  0.00000  0.156406  0.000000   \n",
      "F  0.085420  0.054390  0.192806  0.000000  0.03228  0.113197  0.062903   \n",
      "\n",
      "    Clone 7   Clone 8   Clone 9  Clone 10  Clone 11  Clone 12  \n",
      "B  0.000000  0.188423  0.029130  0.000000  0.172633  0.153810  \n",
      "C  0.000000  0.167898  0.031733  0.011854  0.155322  0.000017  \n",
      "E  0.093864  0.172007  0.039906  0.000000  0.142474  0.159210  \n",
      "F  0.002528  0.278604  0.000436  0.000000  0.072985  0.104451  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/zks035_n5vn9xb6d7n4xk5200000gn/T/ipykernel_21952/272564227.py:111: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  freq_matrix = freq_matrix.fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figure to: results/G12/G12_pyclone_heatmap.png\n",
      "PyClone-VI heatmap saved to: results/G12/G12_pyclone_heatmap.png\n",
      "Processing CloneFinder results for patient G12 (using alternative file)...\n",
      "Parsing CloneFinder results from: results/G12/G12_clonefinder_inputsnv_CloneFinder.txt\n",
      "          Clone1   Clone2   Clone3    Clone4    Clone5    Clone6    Clone7\n",
      "Sample                                                                    \n",
      "B       0.591059  0.35222  0.00000  0.000000  0.000000  0.000000  0.000000\n",
      "C       0.000000  0.00000  0.73277  0.217834  0.000000  0.000000  0.000000\n",
      "E       0.726283  0.00000  0.00000  0.000000  0.129531  0.000000  0.000000\n",
      "F       0.000000  0.00000  0.00000  0.000000  0.000000  0.518508  0.134874\n",
      "Saved figure to: results/G12/G12_clonefinder_heatmap.png\n",
      "CloneFinder heatmap saved to: results/G12/G12_clonefinder_heatmap.png\n",
      "Processing LICHeE results for patient G12...\n",
      "Parsing LICHeE results from: results/G12/G12_lichee_input.txt.trees.txt\n",
      "  Found 10 nodes\n",
      "  Found 10 parent-child relationships\n",
      "  Found 4 samples: C, B, E, F\n",
      "  Final DataFrame shape: (4, 10)\n",
      "   Node 3  Node 6  Node 1  Node 2  Node 4  Node 5  Node 7  Node 8  Node 9  \\\n",
      "C   0.448   0.415   0.000   0.000   0.043   0.313   0.000   0.000   0.000   \n",
      "B   0.450   0.444   0.000   0.000   0.000   0.000   0.057   0.321   0.000   \n",
      "E   0.406   0.405   0.000   0.000   0.000   0.000   0.000   0.000   0.055   \n",
      "F   0.320   0.000   0.066   0.313   0.000   0.000   0.000   0.000   0.000   \n",
      "\n",
      "   Node 10  \n",
      "C     0.00  \n",
      "B     0.00  \n",
      "E     0.31  \n",
      "F     0.00  \n",
      "Saved figure to: results/G12/G12_lichee_heatmap.png\n",
      "LICHeE heatmap saved to: results/G12/G12_lichee_heatmap.png\n",
      "Creating combined bar graphs for patient G12...\n",
      "Saved combined bar graphs for all samples to: results/G12/G12_all_samples_bar_graphs.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pyclone_heatmap': 'results/G12/G12_pyclone_heatmap.png',\n",
       " 'clonefinder_heatmap': 'results/G12/G12_clonefinder_heatmap.png',\n",
       " 'lichee_heatmap': 'results/G12/G12_lichee_heatmap.png',\n",
       " 'sample_bar_graphs': 'results/G12/G12_all_samples_bar_graphs.png'}"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# patient_ids = get_all_patient_ids() \n",
    "\n",
    "# for patient_id in patient_ids:\n",
    "#     visualize_clonality_for_patient(patient_id, results_dir=\"results\")\n",
    "\n",
    "visualize_clonality_for_patient(\"G12\", results_dir=\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import subprocess\n",
    "# import tempfile\n",
    "# import sys\n",
    "\n",
    "# def prepare_pyclone_for_clonevol(pyclone_file, output_dir):\n",
    "#     \"\"\"\n",
    "#     Prepare PyClone-VI results for ClonEvol input,\n",
    "#     ensuring cluster IDs are consecutive integers starting at 1\n",
    "    \n",
    "#     Args:\n",
    "#         pyclone_file (str): Path to PyClone-VI results TSV file\n",
    "#         output_dir (str): Directory for output files\n",
    "        \n",
    "#     Returns:\n",
    "#         str: Path to the prepared ClonEvol input file\n",
    "#     \"\"\"\n",
    "#     # Create output directory if it doesn't exist\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     try:\n",
    "#         # Load PyClone results\n",
    "#         df = pd.read_csv(pyclone_file, sep='\\t')\n",
    "        \n",
    "#         # Create a mapping of original cluster IDs to consecutive integers starting at 1\n",
    "#         original_clusters = sorted(df['cluster_id'].unique())\n",
    "#         cluster_mapping = {original: new+1 for new, original in enumerate(original_clusters)}\n",
    "        \n",
    "#         print(f\"Remapping cluster IDs: {cluster_mapping}\")\n",
    "        \n",
    "#         # Apply the mapping to create new consecutive cluster IDs\n",
    "#         df['remapped_cluster_id'] = df['cluster_id'].map(cluster_mapping)\n",
    "        \n",
    "#         # Extract necessary columns for ClonEvol\n",
    "#         clonevol_df = df[['remapped_cluster_id', 'sample_id', 'cellular_prevalence']]\n",
    "        \n",
    "#         # Rename columns to match ClonEvol expected format\n",
    "#         clonevol_df = clonevol_df.rename(columns={\n",
    "#             'remapped_cluster_id': 'cluster',\n",
    "#             'sample_id': 'sample',\n",
    "#             'cellular_prevalence': 'vaf'\n",
    "#         })\n",
    "        \n",
    "#         # Sort by cluster and sample\n",
    "#         clonevol_df = clonevol_df.sort_values(['cluster', 'sample'])\n",
    "        \n",
    "#         # Write to file\n",
    "#         clonevol_input = os.path.join(output_dir, 'pyclone_for_clonevol.tsv')\n",
    "#         clonevol_df.to_csv(clonevol_input, sep='\\t', index=False)\n",
    "        \n",
    "#         # Save the mapping for reference\n",
    "#         mapping_file = os.path.join(output_dir, 'cluster_id_mapping.txt')\n",
    "#         with open(mapping_file, 'w') as f:\n",
    "#             f.write(\"Original_ID,Remapped_ID\\n\")\n",
    "#             for orig, new in cluster_mapping.items():\n",
    "#                 f.write(f\"{orig},{new}\\n\")\n",
    "        \n",
    "#         print(f\"Prepared ClonEvol input file: {clonevol_input}\")\n",
    "#         print(f\"Saved cluster ID mapping to: {mapping_file}\")\n",
    "#         return clonevol_input\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error preparing PyClone data for ClonEvol: {str(e)}\")\n",
    "#         return None\n",
    "\n",
    "# def check_r_installation():\n",
    "#     \"\"\"Check if R is installed and available\"\"\"\n",
    "#     try:\n",
    "#         result = subprocess.run([\"Rscript\", \"--version\"], \n",
    "#                                stdout=subprocess.PIPE, \n",
    "#                                stderr=subprocess.PIPE,\n",
    "#                                text=True)\n",
    "#         if result.returncode == 0:\n",
    "#             print(f\"R is installed: {result.stdout or result.stderr}\")\n",
    "#             return True\n",
    "#         else:\n",
    "#             print(\"R does not appear to be installed or is not in the PATH\")\n",
    "#             return False\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error checking R installation: {str(e)}\")\n",
    "#         return False\n",
    "\n",
    "# def check_clonevol_installation():\n",
    "#     \"\"\"Check if the ClonEvol package is installed in R\"\"\"\n",
    "#     try:\n",
    "#         # Create a temporary R script\n",
    "#         with tempfile.NamedTemporaryFile(suffix='.R', mode='w', delete=False) as f:\n",
    "#             f.write('if(\"clonevol\" %in% rownames(installed.packages())) { cat(\"ClonEvol is installed\") } else { cat(\"ClonEvol is NOT installed\") }')\n",
    "#             temp_script = f.name\n",
    "        \n",
    "#         # Run the script\n",
    "#         result = subprocess.run([\"Rscript\", temp_script], \n",
    "#                                stdout=subprocess.PIPE, \n",
    "#                                stderr=subprocess.PIPE,\n",
    "#                                text=True)\n",
    "        \n",
    "#         # Clean up\n",
    "#         os.unlink(temp_script)\n",
    "        \n",
    "#         if \"ClonEvol is installed\" in result.stdout:\n",
    "#             print(\"ClonEvol R package is installed\")\n",
    "#             return True\n",
    "#         else:\n",
    "#             print(\"ClonEvol R package is NOT installed\")\n",
    "#             print(\"To install ClonEvol in R, run:\")\n",
    "#             print(\"install.packages('devtools')\")\n",
    "#             print(\"devtools::install_github('hdng/clonevol')\")\n",
    "#             return False\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error checking ClonEvol installation: {str(e)}\")\n",
    "#         return False\n",
    "\n",
    "# def run_clonevol(clonevol_input, output_dir, patient_id):\n",
    "#     \"\"\"\n",
    "#     Run ClonEvol on the prepared input file using an R script\n",
    "    \n",
    "#     Args:\n",
    "#         clonevol_input (str): Path to ClonEvol input file\n",
    "#         output_dir (str): Directory for output files\n",
    "#         patient_id (str): Patient identifier\n",
    "        \n",
    "#     Returns:\n",
    "#         str: Path to the ClonEvol output file\n",
    "#     \"\"\"\n",
    "#     # Check prerequisites\n",
    "#     if not check_r_installation():\n",
    "#         print(\"R is required to run ClonEvol\")\n",
    "#         return None\n",
    "    \n",
    "#     if not check_clonevol_installation():\n",
    "#         print(\"ClonEvol R package is required\")\n",
    "#         return None\n",
    "    \n",
    "#     # Create R script path\n",
    "#     r_script_path = os.path.join(output_dir, f\"run_clonevol_{patient_id}.R\")\n",
    "#     output_file = os.path.join(output_dir, 'clonevol_clone_frequencies.tsv')\n",
    "    \n",
    "#     # Write R script content with better error handling\n",
    "#     with open(r_script_path, 'w') as f:\n",
    "#         f.write(f\"\"\"\n",
    "# # Load required libraries with error handling\n",
    "# tryCatch({{\n",
    "#     library(clonevol)\n",
    "#     library(ggplot2)\n",
    "# }}, error = function(e) {{\n",
    "#     cat(\"Error loading required libraries:\", conditionMessage(e), \"\\\\n\")\n",
    "#     quit(status = 1)\n",
    "# }})\n",
    "\n",
    "# # Set error handler to write to log\n",
    "# error_log <- file(\"{os.path.join(output_dir, 'clonevol_error.log')}\", \"w\")\n",
    "# options(error = function() {{\n",
    "#     cat(geterrmessage(), file = error_log)\n",
    "#     close(error_log)\n",
    "#     quit(status = 1)\n",
    "# }})\n",
    "\n",
    "# # Read input data\n",
    "# cat(\"Reading input data...\\\\n\")\n",
    "# vafs <- tryCatch({{\n",
    "#     read.table(\"{clonevol_input}\", header=TRUE, sep=\"\\\\t\")\n",
    "# }}, error = function(e) {{\n",
    "#     cat(\"Error reading input file:\", conditionMessage(e), \"\\\\n\")\n",
    "#     stop(e)\n",
    "# }})\n",
    "\n",
    "# # Convert to ClonEvol format\n",
    "# cat(\"Converting to ClonEvol format...\\\\n\")\n",
    "# samples <- unique(vafs$sample)\n",
    "# clusters <- unique(vafs$cluster)\n",
    "\n",
    "# cat(\"Found\", length(samples), \"samples and\", length(clusters), \"clusters\\\\n\")\n",
    "\n",
    "# # Prepare cluster data frame\n",
    "# cluster.data <- data.frame()\n",
    "# for (cluster in clusters) {{\n",
    "#     cluster.vafs <- vafs[vafs$cluster == cluster, ]\n",
    "#     row <- c(cluster)\n",
    "#     for (sample in samples) {{\n",
    "#         sample.vaf <- cluster.vafs[cluster.vafs$sample == sample, \"vaf\"]\n",
    "#         if (length(sample.vaf) > 0) {{\n",
    "#             row <- c(row, sample.vaf[1])\n",
    "#         }} else {{\n",
    "#             row <- c(row, 0)\n",
    "#         }}\n",
    "#     }}\n",
    "#     if (nrow(cluster.data) == 0) {{\n",
    "#         cluster.data <- as.data.frame(t(row))\n",
    "#     }} else {{\n",
    "#         cluster.data <- rbind(cluster.data, row)\n",
    "#     }}\n",
    "# }}\n",
    "\n",
    "# # Set column names\n",
    "# colnames(cluster.data) <- c(\"cluster\", samples)\n",
    "\n",
    "# # Convert to numeric\n",
    "# for (col in 2:ncol(cluster.data)) {{\n",
    "#     cluster.data[,col] <- as.numeric(as.character(cluster.data[,col]))\n",
    "# }}\n",
    "\n",
    "# cat(\"Cluster data prepared. Building phylogeny models...\\\\n\")\n",
    "\n",
    "# # Build phylogeny models\n",
    "# models <- list()\n",
    "# try_model_sizes <- TRUE\n",
    "# i <- 2\n",
    "\n",
    "# while(try_model_sizes && i <= length(clusters)) {{\n",
    "#     cat(\"Trying model with\", i, \"clones...\\\\n\")\n",
    "#     tryCatch({{\n",
    "#         # Try models with i clones\n",
    "#         model <- infer.clonal.models(variants=cluster.data, cluster.col.name=\"cluster\",\n",
    "#                               vaf.col.names=samples, subclonal.test=\"bootstrap\",\n",
    "#                               subclonal.test.model=\"non-parametric\", num.boots=1000,\n",
    "#                               founding.cluster=1,\n",
    "#                               cluster.center=\"mean\", ignore.clusters=NULL,\n",
    "#                               min.cluster.vaf=0.05, sum.p=0.05, alpha=0.05)\n",
    "        \n",
    "#         # Store successful model\n",
    "#         if (!is.null(model)) {{\n",
    "#             models[[i-1]] <- model\n",
    "#             cat(\"  - Model with\", i, \"clones successful\\\\n\")\n",
    "#         }}\n",
    "#     }}, error = function(e) {{\n",
    "#         cat(\"  - Model with\", i, \"clones failed:\", conditionMessage(e), \"\\\\n\")\n",
    "#         if (grepl(\"maximum number of iterations reached\", conditionMessage(e)) || \n",
    "#             grepl(\"singular gradient\", conditionMessage(e))) {{\n",
    "#             # These are common errors when model is too complex\n",
    "#             try_model_sizes <<- FALSE\n",
    "#             cat(\"  - Stopping model search due to convergence issues\\\\n\")\n",
    "#         }}\n",
    "#     }})\n",
    "    \n",
    "#     i <- i + 1\n",
    "# }}\n",
    "\n",
    "# # Find the best model\n",
    "# best.model.idx <- 0\n",
    "# if (length(models) > 0) {{\n",
    "#     best.model.idx <- which.max(sapply(models, function(x) {{\n",
    "#         if (is.null(x)) return(0)\n",
    "#         return(length(x))\n",
    "#     }}))\n",
    "# }}\n",
    "\n",
    "# if (length(best.model.idx) > 0 && best.model.idx > 0 && !is.null(models[[best.model.idx]])) {{\n",
    "#     cat(\"Found best model with\", best.model.idx+1, \"clones\\\\n\")\n",
    "#     best.model <- models[[best.model.idx]]\n",
    "    \n",
    "#     # Create tree directory\n",
    "#     tree_dir <- \"{os.path.join(output_dir, 'clonevol_trees')}\"\n",
    "#     dir.create(tree_dir, showWarnings = FALSE, recursive = TRUE)\n",
    "    \n",
    "#     # Convert model to proper clone frequencies\n",
    "#     cat(\"Converting model to clone frequencies...\\\\n\")\n",
    "#     tryCatch({{\n",
    "#         # This will calculate clone frequencies based on evolutionary relationships\n",
    "#         clone.data <- convert.consensus.tree.clone.to.branch(cluster.data, best.model)\n",
    "        \n",
    "#         # Write results to file\n",
    "#         write.table(clone.data, file=\"{output_file}\",\n",
    "#                     sep=\"\\\\t\", quote=FALSE, row.names=FALSE)\n",
    "        \n",
    "#         cat(\"Plotting results...\\\\n\")\n",
    "#         # Plot clone frequencies\n",
    "#         pdf(file.path(tree_dir, \"clonevol_clone_plot.pdf\"))\n",
    "#         plot.clonal.models(best.model)\n",
    "#         dev.off()\n",
    "        \n",
    "#         # Plot tree\n",
    "#         pdf(file.path(tree_dir, \"clonevol_tree.pdf\"))\n",
    "#         plot.clones.as.fishplot(clone.data)\n",
    "#         dev.off()\n",
    "        \n",
    "#         cat(\"ClonEvol analysis completed successfully\\\\n\")\n",
    "#     }}, error = function(e) {{\n",
    "#         cat(\"Error generating clone frequencies:\", conditionMessage(e), \"\\\\n\")\n",
    "#         # Create a fallback simple clone frequency file using cluster.data\n",
    "#         # This at least gives us some output even if the phylogenetic analysis fails\n",
    "#         write.table(cluster.data, file=\"{os.path.join(output_dir, 'clonevol_simple_frequencies.tsv')}\",\n",
    "#                    sep=\"\\\\t\", quote=FALSE, row.names=FALSE)\n",
    "#         cat(\"Created fallback frequency file based on input clusters\\\\n\")\n",
    "#     }})\n",
    "# }} else {{\n",
    "#     cat(\"No valid models could be built\\\\n\")\n",
    "#     # Create a fallback simple frequency file\n",
    "#     write.table(cluster.data, file=\"{os.path.join(output_dir, 'clonevol_simple_frequencies.tsv')}\",\n",
    "#                sep=\"\\\\t\", quote=FALSE, row.names=FALSE)\n",
    "#     cat(\"Created fallback frequency file based on input clusters\\\\n\")\n",
    "# }}\n",
    "# \"\"\")\n",
    "    \n",
    "#     # Run R script with appropriate error handling\n",
    "#     try:\n",
    "#         print(f\"Running ClonEvol R script: {r_script_path}\")\n",
    "#         result = subprocess.run([\"Rscript\", r_script_path], \n",
    "#                               stdout=subprocess.PIPE, \n",
    "#                               stderr=subprocess.PIPE,\n",
    "#                               text=True,\n",
    "#                               check=False)  # Don't raise exception on non-zero return\n",
    "        \n",
    "#         print(\"=== ClonEvol Output ===\")\n",
    "#         print(result.stdout)\n",
    "        \n",
    "#         if result.stderr:\n",
    "#             print(\"=== ClonEvol Errors/Warnings ===\")\n",
    "#             print(result.stderr)\n",
    "        \n",
    "#         # Check if main output file exists\n",
    "#         if os.path.exists(output_file):\n",
    "#             print(f\"ClonEvol generated clone frequencies file: {output_file}\")\n",
    "#             return output_file\n",
    "        \n",
    "#         # Check if fallback file exists\n",
    "#         fallback_file = os.path.join(output_dir, 'clonevol_simple_frequencies.tsv')\n",
    "#         if os.path.exists(fallback_file):\n",
    "#             print(f\"Using fallback clone frequencies: {fallback_file}\")\n",
    "#             return fallback_file\n",
    "        \n",
    "#         print(\"ClonEvol failed to generate any output files\")\n",
    "#         return None\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error running ClonEvol: {str(e)}\")\n",
    "#         return None\n",
    "\n",
    "# def parse_clonevol_results(clonevol_output):\n",
    "#     \"\"\"\n",
    "#     Parse ClonEvol results into a DataFrame with proper clone frequencies\n",
    "    \n",
    "#     Args:\n",
    "#         clonevol_output (str): Path to ClonEvol output file\n",
    "    \n",
    "#     Returns:\n",
    "#         pandas.DataFrame: DataFrame with samples as rows and clones as columns\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(clonevol_output):\n",
    "#         print(f\"Error: ClonEvol output file not found: {clonevol_output}\")\n",
    "#         return None\n",
    "    \n",
    "#     try:\n",
    "#         # Read ClonEvol results\n",
    "#         df = pd.read_csv(clonevol_output, sep='\\t')\n",
    "        \n",
    "#         # Extract sample columns (all except 'cluster')\n",
    "#         samples = [col for col in df.columns if col != 'cluster']\n",
    "        \n",
    "#         # Check if columns are actually numeric\n",
    "#         for col in samples:\n",
    "#             if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "#                 print(f\"Converting column {col} to numeric\")\n",
    "#                 df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "#         # Pivot data to have samples as rows and clones as columns\n",
    "#         clone_freq_df = pd.DataFrame(index=samples)\n",
    "        \n",
    "#         for _, row in df.iterrows():\n",
    "#             clone_id = f\"Clone {int(float(row['cluster']))}\"\n",
    "#             for sample in samples:\n",
    "#                 clone_freq_df.loc[sample, clone_id] = float(row[sample])\n",
    "        \n",
    "#         # Fill NaN values with 0\n",
    "#         clone_freq_df = clone_freq_df.fillna(0)\n",
    "        \n",
    "#         # Verify that rows sum to approximately 1\n",
    "#         row_sums = clone_freq_df.sum(axis=1)\n",
    "#         print(\"\\nVerifying that clone frequencies sum to 1.0:\")\n",
    "#         for sample, row_sum in row_sums.items():\n",
    "#             print(f\"Sample {sample}: {row_sum:.4f}\")\n",
    "            \n",
    "#             # If rows don't sum to approximately 1, normalize them\n",
    "#             if not 0.95 <= row_sum <= 1.05:\n",
    "#                 print(f\"  Warning: Frequencies for {sample} don't sum to 1.0, normalizing...\")\n",
    "#                 clone_freq_df.loc[sample] = clone_freq_df.loc[sample] / row_sum\n",
    "        \n",
    "#         return clone_freq_df\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error parsing ClonEvol results: {str(e)}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "# def plot_clonevol_heatmap(clone_freq_df, patient_id, output_dir='.'):\n",
    "#     \"\"\"\n",
    "#     Create a heatmap from ClonEvol clone frequencies\n",
    "    \n",
    "#     Args:\n",
    "#         clone_freq_df (pandas.DataFrame): DataFrame with samples as rows and clones as columns\n",
    "#         patient_id (str): Patient identifier for title\n",
    "#         output_dir (str): Directory to save the output\n",
    "        \n",
    "#     Returns:\n",
    "#         str: Path to the saved heatmap\n",
    "#     \"\"\"\n",
    "#     if clone_freq_df is None or clone_freq_df.empty:\n",
    "#         print(\"No data to plot\")\n",
    "#         return None\n",
    "    \n",
    "#     try:\n",
    "#         # Create figure\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "        \n",
    "#         # Create heatmap with normalized data\n",
    "#         ax = sns.heatmap(clone_freq_df, annot=True, cmap='coolwarm', \n",
    "#                          vmin=0, vmax=1, fmt='.2f', linewidths=.5)\n",
    "        \n",
    "#         # Set title and labels\n",
    "#         plt.title(f\"ClonEvol Evolutionary Clone Frequencies - Patient {patient_id}\", fontsize=14)\n",
    "#         plt.ylabel('Samples', fontsize=12)\n",
    "#         plt.xlabel('Clones', fontsize=12)\n",
    "        \n",
    "#         # Adjust layout\n",
    "#         plt.tight_layout()\n",
    "        \n",
    "#         # Save figure\n",
    "#         save_path = os.path.join(output_dir, f\"{patient_id}_clonevol_heatmap.png\")\n",
    "#         plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "#         plt.close()\n",
    "#         print(f\"Saved ClonEvol heatmap to: {save_path}\")\n",
    "        \n",
    "#         return save_path\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error creating heatmap: {str(e)}\")\n",
    "#         return None\n",
    "\n",
    "# def normalize_pyclone_results(pyclone_file):\n",
    "#     \"\"\"\n",
    "#     Simple fallback method to normalize PyClone-VI results if ClonEvol fails\n",
    "    \n",
    "#     Args:\n",
    "#         pyclone_file (str): Path to PyClone-VI results file\n",
    "        \n",
    "#     Returns:\n",
    "#         pandas.DataFrame: DataFrame with normalized clone frequencies\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Load PyClone results\n",
    "#         df = pd.read_csv(pyclone_file, sep='\\t')\n",
    "        \n",
    "#         # Get unique samples and clusters\n",
    "#         samples = df['sample_id'].unique()\n",
    "#         clusters = sorted(df['cluster_id'].unique())\n",
    "        \n",
    "#         print(f\"Found {len(samples)} samples and {len(clusters)} clusters\")\n",
    "        \n",
    "#         # Create a temporary dictionary to hold values\n",
    "#         temp_data = {}\n",
    "        \n",
    "#         # Process each sample\n",
    "#         for sample in samples:\n",
    "#             temp_data[sample] = {}\n",
    "#             sample_df = df[df['sample_id'] == sample]\n",
    "            \n",
    "#             # Calculate average cellular prevalence for each cluster\n",
    "#             for cluster in clusters:\n",
    "#                 cluster_data = sample_df[sample_df['cluster_id'] == cluster]\n",
    "#                 if not cluster_data.empty:\n",
    "#                     temp_data[sample][f'Clone {cluster}'] = float(cluster_data['cellular_prevalence'].mean())\n",
    "#                 else:\n",
    "#                     temp_data[sample][f'Clone {cluster}'] = 0.0\n",
    "        \n",
    "#         # Convert to DataFrame\n",
    "#         result_df = pd.DataFrame.from_dict(temp_data, orient='index')\n",
    "        \n",
    "#         # Normalize each row to sum to 1.0\n",
    "#         for sample in result_df.index:\n",
    "#             row_sum = result_df.loc[sample].sum()\n",
    "#             if row_sum > 0:  # Avoid division by zero\n",
    "#                 result_df.loc[sample] = result_df.loc[sample] / row_sum\n",
    "        \n",
    "#         # Verify row sums\n",
    "#         for sample in result_df.index:\n",
    "#             print(f\"Sample {sample} sum: {result_df.loc[sample].sum():.4f}\")\n",
    "        \n",
    "#         return result_df\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error normalizing PyClone results: {str(e)}\")\n",
    "#         return None\n",
    "\n",
    "# def run_full_pipeline(pyclone_file, patient_id, output_dir='.'):\n",
    "#     \"\"\"\n",
    "#     Run the full pipeline from PyClone-VI results to clone frequency heatmap\n",
    "    \n",
    "#     Args:\n",
    "#         pyclone_file (str): Path to PyClone-VI results TSV file\n",
    "#         patient_id (str): Patient identifier\n",
    "#         output_dir (str): Directory for output files\n",
    "        \n",
    "#     Returns:\n",
    "#         str: Path to the saved heatmap\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*80}\\nProcessing patient {patient_id}\\n{'='*80}\")\n",
    "    \n",
    "#     # Create patient-specific output directory\n",
    "#     patient_dir = os.path.join(output_dir, patient_id)\n",
    "#     os.makedirs(patient_dir, exist_ok=True)\n",
    "    \n",
    "#     try:\n",
    "#         # Try ClonEvol approach first\n",
    "#         print(\"\\n[1/5] Preparing PyClone data for ClonEvol...\")\n",
    "#         clonevol_input = prepare_pyclone_for_clonevol(pyclone_file, patient_dir)\n",
    "        \n",
    "#         if clonevol_input:\n",
    "#             print(\"\\n[2/5] Running ClonEvol...\")\n",
    "#             clonevol_output = run_clonevol(clonevol_input, patient_dir, patient_id)\n",
    "            \n",
    "#             if clonevol_output:\n",
    "#                 print(\"\\n[3/5] Parsing ClonEvol results...\")\n",
    "#                 clone_freq_df = parse_clonevol_results(clonevol_output)\n",
    "                \n",
    "#                 if clone_freq_df is not None:\n",
    "#                     print(\"\\n[4/5] Creating ClonEvol heatmap...\")\n",
    "#                     heatmap_path = plot_clonevol_heatmap(clone_freq_df, patient_id, patient_dir)\n",
    "                    \n",
    "#                     if heatmap_path:\n",
    "#                         print(\"\\n[5/5] Pipeline completed successfully!\")\n",
    "#                         return heatmap_path\n",
    "        \n",
    "#         # If any step failed, fall back to simple normalization\n",
    "#         print(\"\\nClonEvol approach failed. Using simple normalization as fallback.\")\n",
    "        \n",
    "#         print(\"\\n[1/3] Normalizing PyClone results...\")\n",
    "#         normalized_df = normalize_pyclone_results(pyclone_file)\n",
    "        \n",
    "#         if normalized_df is not None:\n",
    "#             print(\"\\n[2/3] Creating normalized heatmap...\")\n",
    "#             # Create figure\n",
    "#             plt.figure(figsize=(10, 6))\n",
    "            \n",
    "#             # Create heatmap with normalized data\n",
    "#             sns.heatmap(normalized_df, annot=True, cmap='coolwarm', \n",
    "#                         vmin=0, vmax=1, fmt='.2f', linewidths=.5)\n",
    "            \n",
    "#             # Set title and labels\n",
    "#             plt.title(f\"Normalized PyClone-VI Clonal Composition - Patient {patient_id}\", fontsize=14)\n",
    "#             plt.ylabel('Samples', fontsize=12)\n",
    "#             plt.xlabel('Clones', fontsize=12)\n",
    "            \n",
    "#             # Adjust layout\n",
    "#             plt.tight_layout()\n",
    "            \n",
    "#             # Save figure\n",
    "#             save_path = os.path.join(patient_dir, f\"{patient_id}_normalized_pyclone_heatmap.png\")\n",
    "#             plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "#             plt.close()\n",
    "            \n",
    "#             print(f\"\\n[3/3] Saved normalized heatmap to: {save_path}\")\n",
    "#             return save_path\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in pipeline: {str(e)}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "    \n",
    "#     print(\"All approaches failed. Unable to generate clone frequency heatmap.\")\n",
    "#     return None\n",
    "\n",
    "# patient_ids = get_all_patient_ids() \n",
    "\n",
    "# for patient_id in patient_ids:\n",
    "#     run_full_pipeline(f\"results/{patient_id}/{patient_id}_pyclone_results.tsv\", patient_id, \"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import os\n",
    "\n",
    "# def convert_pyclone_to_clone_frequencies(pyclone_file, use_maximum=True):\n",
    "#     \"\"\"\n",
    "#     Convert PyClone-VI results to clone frequencies by:\n",
    "#     1. Grouping mutations by cluster and sample\n",
    "#     2. Taking maximum (or median) cellular prevalence as proxy for clonal frequency\n",
    "#     3. Normalizing to ensure frequencies sum to 1.0 per sample\n",
    "    \n",
    "#     Args:\n",
    "#         pyclone_file (str): Path to PyClone-VI results TSV file\n",
    "#         use_maximum (bool): If True, use maximum cellular prevalence; if False, use median\n",
    "    \n",
    "#     Returns:\n",
    "#         pandas.DataFrame: DataFrame with samples as rows and clones as columns\n",
    "#     \"\"\"\n",
    "#     print(f\"Processing PyClone-VI results from: {pyclone_file}\")\n",
    "    \n",
    "#     try:\n",
    "#         # Load PyClone results\n",
    "#         df = pd.read_csv(pyclone_file, sep='\\t')\n",
    "        \n",
    "#         # Get unique samples and clusters\n",
    "#         samples = sorted(df['sample_id'].unique())\n",
    "#         clusters = sorted(df['cluster_id'].unique())\n",
    "        \n",
    "#         print(f\"Found {len(samples)} samples and {len(clusters)} clusters\")\n",
    "        \n",
    "#         # Group by cluster_id and sample_id, and calculate representative value (max or median)\n",
    "#         grouped = df.groupby(['cluster_id', 'sample_id'])\n",
    "        \n",
    "#         # Create a DataFrame to store the representative values\n",
    "#         if use_maximum:\n",
    "#             cluster_prev = grouped['cellular_prevalence'].max().reset_index()\n",
    "#             print(\"Using MAXIMUM cellular prevalence as proxy for clonal frequency\")\n",
    "#         else:\n",
    "#             cluster_prev = grouped['cellular_prevalence'].median().reset_index()\n",
    "#             print(\"Using MEDIAN cellular prevalence as proxy for clonal frequency\")\n",
    "        \n",
    "#         # Create a pivot table with samples as rows and clusters as columns\n",
    "#         prev_matrix = pd.pivot_table(\n",
    "#             cluster_prev,\n",
    "#             values='cellular_prevalence',\n",
    "#             index='sample_id',\n",
    "#             columns='cluster_id',\n",
    "#             fill_value=0\n",
    "#         )\n",
    "        \n",
    "#         # Rename columns to 'Clone X'\n",
    "#         prev_matrix.columns = [f'Clone {int(col)}' for col in prev_matrix.columns]\n",
    "        \n",
    "#         # Normalize rows to sum to 1.0\n",
    "#         normalized_df = pd.DataFrame(index=prev_matrix.index, columns=prev_matrix.columns)\n",
    "        \n",
    "#         for sample in prev_matrix.index:\n",
    "#             row_sum = prev_matrix.loc[sample].sum()\n",
    "#             if row_sum > 0:  # Avoid division by zero\n",
    "#                 normalized_df.loc[sample] = prev_matrix.loc[sample] / row_sum\n",
    "#             else:\n",
    "#                 normalized_df.loc[sample] = prev_matrix.loc[sample]  # Keep zeros if sum is zero\n",
    "        \n",
    "#         # Verify that rows sum to 1.0\n",
    "#         print(\"\\nVerifying normalized frequencies sum to 1.0:\")\n",
    "#         for sample in normalized_df.index:\n",
    "#             row_sum = normalized_df.loc[sample].sum()\n",
    "#             print(f\"Sample {sample}: {row_sum:.4f}\")\n",
    "        \n",
    "#         return normalized_df\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing PyClone-VI results: {str(e)}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "# def plot_clone_frequency_heatmap(normalized_df, patient_id, output_dir='.', method='maximum'):\n",
    "#     \"\"\"\n",
    "#     Create a heatmap visualization of the clone frequencies\n",
    "    \n",
    "#     Args:\n",
    "#         normalized_df (pandas.DataFrame): DataFrame with normalized clone frequencies\n",
    "#         patient_id (str): Patient identifier for title\n",
    "#         output_dir (str): Directory to save output\n",
    "#         method (str): Method used for frequency estimation ('maximum' or 'median')\n",
    "        \n",
    "#     Returns:\n",
    "#         str: Path to saved heatmap\n",
    "#     \"\"\"\n",
    "#     if normalized_df is None or normalized_df.empty:\n",
    "#         print(\"No data to plot\")\n",
    "#         return None\n",
    "    \n",
    "#     # Create output directory if it doesn't exist\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     try:\n",
    "#         # Create figure\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "        \n",
    "#         # Plot heatmap\n",
    "#         sns.heatmap(normalized_df, annot=True, cmap='Blues_r', \n",
    "#                    vmin=0, vmax=1, fmt='.2f', linewidths=.5)\n",
    "        \n",
    "#         # Set title and labels\n",
    "#         method_str = \"Maximum\" if method == 'maximum' else \"Median\"\n",
    "#         plt.title(f\"PyClone-VI Clonal Composition - Patient {patient_id}\\n({method_str} Prevalence Method)\", fontsize=14)\n",
    "#         plt.ylabel('Samples', fontsize=12)\n",
    "#         plt.xlabel('Clones', fontsize=12)\n",
    "        \n",
    "#         # Adjust layout\n",
    "#         plt.tight_layout()\n",
    "        \n",
    "#         # Save figure\n",
    "#         save_path = os.path.join(output_dir, f\"{patient_id}_pyclone_clonefreq_{method}.png\")\n",
    "#         plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "#         plt.close()\n",
    "        \n",
    "#         print(f\"Saved clone frequency heatmap to: {save_path}\")\n",
    "#         return save_path\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error creating heatmap: {str(e)}\")\n",
    "#         return None\n",
    "\n",
    "# def create_clone_frequency_heatmap(pyclone_file, patient_id, output_dir='.', method='maximum'):\n",
    "#     \"\"\"\n",
    "#     Full pipeline to convert PyClone-VI results to clone frequencies and create heatmap\n",
    "    \n",
    "#     Args:\n",
    "#         pyclone_file (str): Path to PyClone-VI results file\n",
    "#         patient_id (str): Patient identifier\n",
    "#         output_dir (str): Directory to save output\n",
    "#         method (str): Method for frequency estimation ('maximum' or 'median')\n",
    "        \n",
    "#     Returns:\n",
    "#         str: Path to saved heatmap\n",
    "#     \"\"\"\n",
    "#     # Convert PyClone-VI results to clone frequencies\n",
    "#     use_maximum = (method.lower() == 'maximum')\n",
    "#     normalized_df = convert_pyclone_to_clone_frequencies(pyclone_file, use_maximum=use_maximum)\n",
    "    \n",
    "#     if normalized_df is not None:\n",
    "#         # Create and save heatmap\n",
    "#         return plot_clone_frequency_heatmap(normalized_df, patient_id, output_dir, method)\n",
    "#     else:\n",
    "#         print(\"Failed to generate clone frequencies\")\n",
    "#         return None\n",
    "\n",
    "# def compare_estimation_methods(pyclone_file, patient_id, output_dir='.'):\n",
    "#     \"\"\"\n",
    "#     Compare maximum and median methods for clone frequency estimation\n",
    "    \n",
    "#     Args:\n",
    "#         pyclone_file (str): Path to PyClone-VI results file\n",
    "#         patient_id (str): Patient identifier\n",
    "#         output_dir (str): Directory to save output\n",
    "        \n",
    "#     Returns:\n",
    "#         tuple: Paths to saved heatmaps (maximum, median)\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*80}\\nProcessing patient {patient_id} with maximum method\\n{'='*80}\")\n",
    "#     max_path = create_clone_frequency_heatmap(pyclone_file, patient_id, output_dir, method='maximum')\n",
    "    \n",
    "#     print(f\"\\n{'='*80}\\nProcessing patient {patient_id} with median method\\n{'='*80}\")\n",
    "#     med_path = create_clone_frequency_heatmap(pyclone_file, patient_id, output_dir, method='median')\n",
    "    \n",
    "#     return max_path, med_path\n",
    "\n",
    "# # Example usage:\n",
    "# compare_estimation_methods(\"results/G12/G12_pyclone_results.tsv\", \"G12\", \"results/G12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_all_patient_ids(results_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Get all patient IDs by listing subdirectories under the results directory.\n",
    "    \n",
    "    Args:\n",
    "        results_dir (str): Path to the base results directory\n",
    "        \n",
    "    Returns:\n",
    "        list: List of patient IDs (directory names)\n",
    "    \"\"\"\n",
    "    # Check if the results directory exists\n",
    "    if not os.path.isdir(results_dir):\n",
    "        print(f\"Warning: Results directory '{results_dir}' not found.\")\n",
    "        return []\n",
    "    \n",
    "    # Get all directories (potential patient IDs)\n",
    "    patient_ids = []\n",
    "    for item in os.listdir(results_dir):\n",
    "        full_path = os.path.join(results_dir, item)\n",
    "        # Only include directories (not files)\n",
    "        if os.path.isdir(full_path):\n",
    "            patient_ids.append(item)\n",
    "    \n",
    "    print(f\"Found {len(patient_ids)} patient directories: {', '.join(patient_ids)}\")\n",
    "    return sorted(patient_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_diversity_metrics_per_sample(patient_ids, results_dir=\"results\", save_to_file=True):\n",
    "#     \"\"\"\n",
    "#     Calculate diversity metrics for each patient and sample combination using CloneFinder and LICHeE results.\n",
    "#     Optionally saves results to a CSV file.\n",
    "    \n",
    "#     Args:\n",
    "#         patient_ids (list): List of patient identifiers\n",
    "#         results_dir (str): Base directory for results\n",
    "#         save_to_file (bool): Whether to save metrics to a CSV file\n",
    "    \n",
    "#     Returns:\n",
    "#         pandas.DataFrame: DataFrame with diversity metrics for each patient/sample combination\n",
    "#         str: Path to saved CSV file (if save_to_file is True)\n",
    "#     \"\"\"\n",
    "#     # Create a dataframe to store results\n",
    "#     columns = ['Patient_ID', 'Sample_ID', \n",
    "#                'CloneFinder_Num_Clones', 'CloneFinder_Shannon_Index',\n",
    "#                'LICHeE_Num_Clones', 'LICHeE_Shannon_Index']\n",
    "#     results = pd.DataFrame(columns=columns)\n",
    "    \n",
    "#     for patient_id in patient_ids:\n",
    "#         patient_dir = os.path.join(results_dir, patient_id)\n",
    "        \n",
    "#         if not os.path.isdir(patient_dir):\n",
    "#             print(f\"Warning: Patient directory '{patient_dir}' not found.\")\n",
    "#             continue\n",
    "        \n",
    "#         # Process CloneFinder results\n",
    "#         clonefinder_file = os.path.join(patient_dir, f\"{patient_id}_clonefinder_input_CloneFinder.txt\")\n",
    "#         if not os.path.isfile(clonefinder_file):\n",
    "#             # Try alternative file naming pattern\n",
    "#             clonefinder_file = os.path.join(patient_dir, f\"{patient_id}_clonefinder_inputsnv_CloneFinder.txt\")\n",
    "        \n",
    "#         clonefinder_data = None\n",
    "#         if os.path.isfile(clonefinder_file):\n",
    "#             try:\n",
    "#                 # Parse CloneFinder results\n",
    "#                 clonefinder_data = parse_clonefinder_results(clonefinder_file)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing CloneFinder data for patient {patient_id}: {e}\")\n",
    "        \n",
    "#         # Process LICHeE results\n",
    "#         lichee_file = os.path.join(patient_dir, f\"{patient_id}_lichee_input.txt.trees.txt\")\n",
    "#         if not os.path.isfile(lichee_file):\n",
    "#             # Try alternative file naming patterns\n",
    "#             alt_patterns = [\n",
    "#                 os.path.join(patient_dir, f\"{patient_id}.lichee.output.txt\"),\n",
    "#                 os.path.join(patient_dir, f\"{patient_id}_lichee_output.txt\"),\n",
    "#                 os.path.join(patient_dir, f\"{patient_id}_trees.txt\")\n",
    "#             ]\n",
    "            \n",
    "#             for alt_pattern in alt_patterns:\n",
    "#                 if os.path.isfile(alt_pattern):\n",
    "#                     lichee_file = alt_pattern\n",
    "#                     break\n",
    "        \n",
    "#         lichee_data = None\n",
    "#         if os.path.isfile(lichee_file):\n",
    "#             try:\n",
    "#                 # Parse LICHeE results\n",
    "#                 lichee_data = parse_lichee_results(lichee_file)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing LICHeE data for patient {patient_id}: {e}\")\n",
    "        \n",
    "#         # Get all unique samples across both tools\n",
    "#         all_samples = set()\n",
    "#         if clonefinder_data is not None and not clonefinder_data.empty:\n",
    "#             all_samples.update(clonefinder_data.index)\n",
    "#         if lichee_data is not None and not lichee_data.empty:\n",
    "#             all_samples.update(lichee_data.index)\n",
    "        \n",
    "#         # Calculate metrics for each sample\n",
    "#         for original_sample in all_samples:\n",
    "#             sample_metrics = {\n",
    "#                 'Patient_ID': patient_id,\n",
    "#                 'Sample_ID': original_sample,  # Keep original type\n",
    "#                 'CloneFinder_Num_Clones': 0, \n",
    "#                 'CloneFinder_Shannon_Index': np.nan,\n",
    "#                 'LICHeE_Num_Clones': 0, \n",
    "#                 'LICHeE_Shannon_Index': np.nan\n",
    "#             }\n",
    "            \n",
    "#             # Process CloneFinder data for this sample\n",
    "#             if clonefinder_data is not None and not clonefinder_data.empty:\n",
    "#                 # Try to find the sample in different formats\n",
    "#                 sample_found = False\n",
    "#                 sample_to_use = None\n",
    "                \n",
    "#                 # Try direct match first\n",
    "#                 if original_sample in clonefinder_data.index:\n",
    "#                     sample_found = True\n",
    "#                     sample_to_use = original_sample\n",
    "#                 # Try converting string to int\n",
    "#                 elif isinstance(original_sample, str) and original_sample.isdigit():\n",
    "#                     int_sample = int(original_sample)\n",
    "#                     if int_sample in clonefinder_data.index:\n",
    "#                         sample_found = True\n",
    "#                         sample_to_use = int_sample\n",
    "#                 # Try converting int to string\n",
    "#                 elif isinstance(original_sample, (int, float)):\n",
    "#                     str_sample = str(int(original_sample))\n",
    "#                     if str_sample in clonefinder_data.index:\n",
    "#                         sample_found = True\n",
    "#                         sample_to_use = str_sample\n",
    "                \n",
    "#                 if sample_found and sample_to_use is not None:\n",
    "#                     # Get clone frequencies for this sample\n",
    "#                     frequencies = clonefinder_data.loc[sample_to_use].values\n",
    "#                     # Only consider non-zero frequencies\n",
    "#                     cf_nonzero_freqs = frequencies[frequencies > 0.00001]\n",
    "                    \n",
    "#                     if len(cf_nonzero_freqs) > 0:\n",
    "#                         # Count non-zero clones\n",
    "#                         sample_metrics['CloneFinder_Num_Clones'] = len(cf_nonzero_freqs)\n",
    "                        \n",
    "#                         # Normalize frequencies to sum to 1\n",
    "#                         normalized_freqs = cf_nonzero_freqs / np.sum(cf_nonzero_freqs)\n",
    "                        \n",
    "#                         # Calculate Shannon diversity index: -sum(p * ln(p))\n",
    "#                         try:\n",
    "#                             shannon_index = -np.sum(normalized_freqs * np.log(normalized_freqs))\n",
    "#                             sample_metrics['CloneFinder_Shannon_Index'] = shannon_index\n",
    "#                         except Exception as e:\n",
    "#                             print(f\"Error calculating CloneFinder Shannon Index: {e}\")\n",
    "            \n",
    "#             # Process LICHeE data for this sample\n",
    "#             if lichee_data is not None and not lichee_data.empty:\n",
    "#                 # Try to find the sample in different formats\n",
    "#                 sample_found = False\n",
    "#                 sample_to_use = None\n",
    "                \n",
    "#                 # Try direct match first\n",
    "#                 if original_sample in lichee_data.index:\n",
    "#                     sample_found = True\n",
    "#                     sample_to_use = original_sample\n",
    "#                 # Try converting string to int\n",
    "#                 elif isinstance(original_sample, str) and original_sample.isdigit():\n",
    "#                     int_sample = int(original_sample)\n",
    "#                     if int_sample in lichee_data.index:\n",
    "#                         sample_found = True\n",
    "#                         sample_to_use = int_sample\n",
    "#                 # Try converting int to string\n",
    "#                 elif isinstance(original_sample, (int, float)):\n",
    "#                     str_sample = str(int(original_sample))\n",
    "#                     if str_sample in lichee_data.index:\n",
    "#                         sample_found = True\n",
    "#                         sample_to_use = str_sample\n",
    "                \n",
    "#                 if sample_found and sample_to_use is not None:\n",
    "#                     # Get clone frequencies for this sample\n",
    "#                     frequencies = lichee_data.loc[sample_to_use].values\n",
    "#                     # Only consider non-zero frequencies\n",
    "#                     lich_nonzero_freqs = frequencies[frequencies > 0.00001]\n",
    "                    \n",
    "#                     if len(lich_nonzero_freqs) > 0:\n",
    "#                         # Count non-zero clones\n",
    "#                         sample_metrics['LICHeE_Num_Clones'] = len(lich_nonzero_freqs)\n",
    "                        \n",
    "#                         # Normalize frequencies to sum to 1\n",
    "#                         normalized_freqs = lich_nonzero_freqs / np.sum(lich_nonzero_freqs)\n",
    "                        \n",
    "#                         # Calculate Shannon diversity index: -sum(p * ln(p))\n",
    "#                         try:\n",
    "#                             shannon_index = -np.sum(normalized_freqs * np.log(normalized_freqs))\n",
    "#                             sample_metrics['LICHeE_Shannon_Index'] = shannon_index\n",
    "#                         except Exception as e:\n",
    "#                             print(f\"Error calculating LICHeE Shannon Index: {e}\")\n",
    "            \n",
    "#             # Add sample metrics to results dataframe\n",
    "#             results = pd.concat([results, pd.DataFrame([sample_metrics])], ignore_index=True)\n",
    "    \n",
    "#     # Save results to file if requested\n",
    "#     saved_file_path = None\n",
    "#     if save_to_file and not results.empty:\n",
    "#         if len(patient_ids) == 1:\n",
    "#             # Single patient - save in patient directory\n",
    "#             patient_id = patient_ids[0]\n",
    "#             patient_dir = os.path.join(results_dir, patient_id)\n",
    "#             saved_file_path = os.path.join(patient_dir, f\"{patient_id}_diversity_metrics.csv\")\n",
    "#         else:\n",
    "#             saved_file_path = os.path.join(results_dir, \"all_patients_diversity_metrics.csv\")\n",
    "        \n",
    "#         results.to_csv(saved_file_path, index=False)\n",
    "#         print(f\"Diversity metrics saved to: {saved_file_path}\")\n",
    "    \n",
    "#     if save_to_file:\n",
    "#         return results, saved_file_path\n",
    "#     else:\n",
    "#         return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diversity_metrics_per_sample(patient_ids, results_dir=\"results\", save_to_file=True):\n",
    "    \"\"\"\n",
    "    Calculate diversity metrics for each patient and sample combination using PyClone-VI, CloneFinder, and LICHeE results.\n",
    "    Optionally saves results to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        patient_ids (list): List of patient identifiers\n",
    "        results_dir (str): Base directory for results\n",
    "        save_to_file (bool): Whether to save metrics to a CSV file\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with diversity metrics for each patient/sample combination\n",
    "        str: Path to saved CSV file (if save_to_file is True)\n",
    "    \"\"\"\n",
    "    # Create a dataframe to store results\n",
    "    columns = ['Patient_ID', 'Sample_ID', \n",
    "               'PyClone_Num_Clones', 'PyClone_Shannon_Index',\n",
    "               'CloneFinder_Num_Clones', 'CloneFinder_Shannon_Index',\n",
    "               'LICHeE_Num_Clones', 'LICHeE_Shannon_Index']\n",
    "    results = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "        patient_dir = os.path.join(results_dir, patient_id)\n",
    "        \n",
    "        if not os.path.isdir(patient_dir):\n",
    "            print(f\"Warning: Patient directory '{patient_dir}' not found.\")\n",
    "            continue\n",
    "        \n",
    "        # Process PyClone results\n",
    "        pyclone_file = os.path.join(patient_dir, f\"{patient_id}_pyclone_results.tsv\")\n",
    "        if not os.path.isfile(pyclone_file):\n",
    "            # Try alternative file naming patterns\n",
    "            alt_patterns = [\n",
    "                os.path.join(patient_dir, f\"{patient_id}.pyclone.output.tsv\"),\n",
    "                os.path.join(patient_dir, \"pyclone_output.tsv\"),\n",
    "                os.path.join(patient_dir, \"cluster_assignments.tsv\")\n",
    "            ]\n",
    "            \n",
    "            for alt_pattern in alt_patterns:\n",
    "                if os.path.isfile(alt_pattern):\n",
    "                    pyclone_file = alt_pattern\n",
    "                    break\n",
    "        \n",
    "        pyclone_data = None\n",
    "        if os.path.isfile(pyclone_file):\n",
    "            try:\n",
    "                # Parse PyClone results (using the modified function that only includes mutations with prevalence > 0)\n",
    "                pyclone_data = parse_pyclone_results(pyclone_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing PyClone data for patient {patient_id}: {e}\")\n",
    "        \n",
    "        # Process CloneFinder results\n",
    "        clonefinder_file = os.path.join(patient_dir, f\"{patient_id}_clonefinder_input_CloneFinder.txt\")\n",
    "        if not os.path.isfile(clonefinder_file):\n",
    "            # Try alternative file naming pattern\n",
    "            clonefinder_file = os.path.join(patient_dir, f\"{patient_id}_clonefinder_inputsnv_CloneFinder.txt\")\n",
    "        \n",
    "        clonefinder_data = None\n",
    "        if os.path.isfile(clonefinder_file):\n",
    "            try:\n",
    "                # Parse CloneFinder results\n",
    "                clonefinder_data = parse_clonefinder_results(clonefinder_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing CloneFinder data for patient {patient_id}: {e}\")\n",
    "        \n",
    "        # Process LICHeE results\n",
    "        lichee_file = os.path.join(patient_dir, f\"{patient_id}_lichee_input.txt.trees.txt\")\n",
    "        if not os.path.isfile(lichee_file):\n",
    "            # Try alternative file naming patterns\n",
    "            alt_patterns = [\n",
    "                os.path.join(patient_dir, f\"{patient_id}.lichee.output.txt\"),\n",
    "                os.path.join(patient_dir, f\"{patient_id}_lichee_output.txt\"),\n",
    "                os.path.join(patient_dir, f\"{patient_id}_trees.txt\")\n",
    "            ]\n",
    "            \n",
    "            for alt_pattern in alt_patterns:\n",
    "                if os.path.isfile(alt_pattern):\n",
    "                    lichee_file = alt_pattern\n",
    "                    break\n",
    "        \n",
    "        lichee_data = None\n",
    "        if os.path.isfile(lichee_file):\n",
    "            try:\n",
    "                # Parse LICHeE results\n",
    "                lichee_data = parse_lichee_results(lichee_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing LICHeE data for patient {patient_id}: {e}\")\n",
    "        \n",
    "        # Get all unique samples across all tools\n",
    "        all_samples = set()\n",
    "        if pyclone_data is not None and not pyclone_data.empty:\n",
    "            all_samples.update(pyclone_data.index)\n",
    "        if clonefinder_data is not None and not clonefinder_data.empty:\n",
    "            all_samples.update(clonefinder_data.index)\n",
    "        if lichee_data is not None and not lichee_data.empty:\n",
    "            all_samples.update(lichee_data.index)\n",
    "        \n",
    "        # Calculate metrics for each sample\n",
    "        for original_sample in all_samples:\n",
    "            sample_metrics = {\n",
    "                'Patient_ID': patient_id,\n",
    "                'Sample_ID': original_sample,  # Keep original type\n",
    "                'PyClone_Num_Clones': 0,\n",
    "                'PyClone_Shannon_Index': np.nan,\n",
    "                'CloneFinder_Num_Clones': 0, \n",
    "                'CloneFinder_Shannon_Index': np.nan,\n",
    "                'LICHeE_Num_Clones': 0, \n",
    "                'LICHeE_Shannon_Index': np.nan\n",
    "            }\n",
    "            \n",
    "            # Process PyClone data for this sample\n",
    "            if pyclone_data is not None and not pyclone_data.empty:\n",
    "                # Try to find the sample in different formats\n",
    "                sample_found = False\n",
    "                sample_to_use = None\n",
    "                \n",
    "                # Try direct match first\n",
    "                if original_sample in pyclone_data.index:\n",
    "                    sample_found = True\n",
    "                    sample_to_use = original_sample\n",
    "                # Try converting string to int\n",
    "                elif isinstance(original_sample, str) and original_sample.isdigit():\n",
    "                    int_sample = int(original_sample)\n",
    "                    if int_sample in pyclone_data.index:\n",
    "                        sample_found = True\n",
    "                        sample_to_use = int_sample\n",
    "                # Try converting int to string\n",
    "                elif isinstance(original_sample, (int, float)):\n",
    "                    str_sample = str(int(original_sample))\n",
    "                    if str_sample in pyclone_data.index:\n",
    "                        sample_found = True\n",
    "                        sample_to_use = str_sample\n",
    "                \n",
    "                if sample_found and sample_to_use is not None:\n",
    "                    # Get clone frequencies for this sample\n",
    "                    frequencies = pyclone_data.loc[sample_to_use].values\n",
    "                    # Only consider non-zero frequencies\n",
    "                    pc_nonzero_freqs = frequencies[frequencies > 0.00001]\n",
    "                    \n",
    "                    if len(pc_nonzero_freqs) > 0:\n",
    "                        # Count non-zero clones\n",
    "                        sample_metrics['PyClone_Num_Clones'] = len(pc_nonzero_freqs)\n",
    "                        \n",
    "                        # Normalize frequencies to sum to 1\n",
    "                        normalized_freqs = pc_nonzero_freqs / np.sum(pc_nonzero_freqs)\n",
    "                        \n",
    "                        # Calculate Shannon diversity index: -sum(p * ln(p))\n",
    "                        try:\n",
    "                            shannon_index = -np.sum(normalized_freqs * np.log(normalized_freqs))\n",
    "                            sample_metrics['PyClone_Shannon_Index'] = shannon_index\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error calculating PyClone Shannon Index: {e}\")\n",
    "            \n",
    "            # Process CloneFinder data for this sample\n",
    "            if clonefinder_data is not None and not clonefinder_data.empty:\n",
    "                # Try to find the sample in different formats\n",
    "                sample_found = False\n",
    "                sample_to_use = None\n",
    "                \n",
    "                # Try direct match first\n",
    "                if original_sample in clonefinder_data.index:\n",
    "                    sample_found = True\n",
    "                    sample_to_use = original_sample\n",
    "                # Try converting string to int\n",
    "                elif isinstance(original_sample, str) and original_sample.isdigit():\n",
    "                    int_sample = int(original_sample)\n",
    "                    if int_sample in clonefinder_data.index:\n",
    "                        sample_found = True\n",
    "                        sample_to_use = int_sample\n",
    "                # Try converting int to string\n",
    "                elif isinstance(original_sample, (int, float)):\n",
    "                    str_sample = str(int(original_sample))\n",
    "                    if str_sample in clonefinder_data.index:\n",
    "                        sample_found = True\n",
    "                        sample_to_use = str_sample\n",
    "                \n",
    "                if sample_found and sample_to_use is not None:\n",
    "                    # Get clone frequencies for this sample\n",
    "                    frequencies = clonefinder_data.loc[sample_to_use].values\n",
    "                    # Only consider non-zero frequencies\n",
    "                    cf_nonzero_freqs = frequencies[frequencies > 0.00001]\n",
    "                    \n",
    "                    if len(cf_nonzero_freqs) > 0:\n",
    "                        # Count non-zero clones\n",
    "                        sample_metrics['CloneFinder_Num_Clones'] = len(cf_nonzero_freqs)\n",
    "                        \n",
    "                        # Normalize frequencies to sum to 1\n",
    "                        normalized_freqs = cf_nonzero_freqs / np.sum(cf_nonzero_freqs)\n",
    "                        \n",
    "                        # Calculate Shannon diversity index: -sum(p * ln(p))\n",
    "                        try:\n",
    "                            shannon_index = -np.sum(normalized_freqs * np.log(normalized_freqs))\n",
    "                            sample_metrics['CloneFinder_Shannon_Index'] = shannon_index\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error calculating CloneFinder Shannon Index: {e}\")\n",
    "            \n",
    "            # Process LICHeE data for this sample\n",
    "            if lichee_data is not None and not lichee_data.empty:\n",
    "                # Try to find the sample in different formats\n",
    "                sample_found = False\n",
    "                sample_to_use = None\n",
    "                \n",
    "                # Try direct match first\n",
    "                if original_sample in lichee_data.index:\n",
    "                    sample_found = True\n",
    "                    sample_to_use = original_sample\n",
    "                # Try converting string to int\n",
    "                elif isinstance(original_sample, str) and original_sample.isdigit():\n",
    "                    int_sample = int(original_sample)\n",
    "                    if int_sample in lichee_data.index:\n",
    "                        sample_found = True\n",
    "                        sample_to_use = int_sample\n",
    "                # Try converting int to string\n",
    "                elif isinstance(original_sample, (int, float)):\n",
    "                    str_sample = str(int(original_sample))\n",
    "                    if str_sample in lichee_data.index:\n",
    "                        sample_found = True\n",
    "                        sample_to_use = str_sample\n",
    "                \n",
    "                if sample_found and sample_to_use is not None:\n",
    "                    # Get clone frequencies for this sample\n",
    "                    frequencies = lichee_data.loc[sample_to_use].values\n",
    "                    # Only consider non-zero frequencies\n",
    "                    lich_nonzero_freqs = frequencies[frequencies > 0.00001]\n",
    "                    \n",
    "                    if len(lich_nonzero_freqs) > 0:\n",
    "                        # Count non-zero clones\n",
    "                        sample_metrics['LICHeE_Num_Clones'] = len(lich_nonzero_freqs)\n",
    "                        \n",
    "                        # Normalize frequencies to sum to 1\n",
    "                        normalized_freqs = lich_nonzero_freqs / np.sum(lich_nonzero_freqs)\n",
    "                        \n",
    "                        # Calculate Shannon diversity index: -sum(p * ln(p))\n",
    "                        try:\n",
    "                            shannon_index = -np.sum(normalized_freqs * np.log(normalized_freqs))\n",
    "                            sample_metrics['LICHeE_Shannon_Index'] = shannon_index\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error calculating LICHeE Shannon Index: {e}\")\n",
    "            \n",
    "            # Add sample metrics to results dataframe\n",
    "            results = pd.concat([results, pd.DataFrame([sample_metrics])], ignore_index=True)\n",
    "    \n",
    "    # Save results to file if requested\n",
    "    saved_file_path = None\n",
    "    if save_to_file and not results.empty:\n",
    "        if len(patient_ids) == 1:\n",
    "            # Single patient - save in patient directory\n",
    "            patient_id = patient_ids[0]\n",
    "            patient_dir = os.path.join(results_dir, patient_id)\n",
    "            saved_file_path = os.path.join(patient_dir, f\"{patient_id}_diversity_metrics.csv\")\n",
    "        else:\n",
    "            saved_file_path = os.path.join(results_dir, \"all_patients_diversity_metrics.csv\")\n",
    "        \n",
    "        results.to_csv(saved_file_path, index=False)\n",
    "        print(f\"Diversity metrics saved to: {saved_file_path}\")\n",
    "    \n",
    "    if save_to_file:\n",
    "        return results, saved_file_path\n",
    "    else:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = get_all_patient_ids() \n",
    "\n",
    "# patient_ids = ['P158']\n",
    "# Calculate diversity metrics\n",
    "diversity_metrics = calculate_diversity_metrics_per_sample(patient_ids)\n",
    "\n",
    "# Display the results table\n",
    "display(diversity_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from scipy import stats\n",
    "\n",
    "# # Load the diversity metrics data\n",
    "# data = pd.read_csv('results/all_patients_diversity_metrics.csv')\n",
    "\n",
    "# # Display basic statistics about the data\n",
    "# print(\"Dataset summary:\")\n",
    "# print(f\"Total samples: {len(data)}\")\n",
    "# print(f\"Number of patients: {data['Patient_ID'].nunique()}\")\n",
    "# print(f\"Samples per patient (min, mean, max): {data.groupby('Patient_ID').size().min()}, {data.groupby('Patient_ID').size().mean():.2f}, {data.groupby('Patient_ID').size().max()}\")\n",
    "\n",
    "# # Prepare a figure for visualizing results\n",
    "# plt.figure(figsize=(15, 10))\n",
    "\n",
    "# # 1. Distribution of number of clones detected by each tool\n",
    "# plt.subplot(2, 2, 1)\n",
    "# clone_data = data[['PyClone_Num_Clones', 'CloneFinder_Num_Clones', 'LICHeE_Num_Clones']].melt()\n",
    "# sns.boxplot(x='variable', y='value', data=clone_data)\n",
    "# plt.title('Number of Clones by Tool')\n",
    "# plt.xlabel('Tool')\n",
    "# plt.ylabel('Number of Clones')\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "# # 2. Distribution of Shannon Index by each tool\n",
    "# plt.subplot(2, 2, 2)\n",
    "# shannon_data = data[['PyClone_Shannon_Index', 'CloneFinder_Shannon_Index', 'LICHeE_Shannon_Index']].melt()\n",
    "# sns.boxplot(x='variable', y='value', data=shannon_data)\n",
    "# plt.title('Shannon Index by Tool')\n",
    "# plt.xlabel('Tool')\n",
    "# plt.ylabel('Shannon Index')\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "# # 3. Tool correlation\n",
    "# plt.subplot(2, 2, 3)\n",
    "# # Calculate correlation matrix for number of clones\n",
    "# clone_corr = data[['PyClone_Num_Clones', 'CloneFinder_Num_Clones', 'LICHeE_Num_Clones']].corr()\n",
    "# sns.heatmap(clone_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "# plt.title('Correlation: Number of Clones')\n",
    "\n",
    "# # 4. Tool correlation for Shannon Index\n",
    "# plt.subplot(2, 2, 4)\n",
    "# shannon_corr = data[['PyClone_Shannon_Index', 'CloneFinder_Shannon_Index', 'LICHeE_Shannon_Index']].corr()\n",
    "# sns.heatmap(shannon_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "# plt.title('Correlation: Shannon Index')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('diversity_overview.png')\n",
    "\n",
    "# # Compare tools more directly with scatter plots\n",
    "# plt.figure(figsize=(15, 5))\n",
    "\n",
    "# # PyClone vs CloneFinder (Number of Clones)\n",
    "# plt.subplot(1, 3, 1)\n",
    "# plt.scatter(data['PyClone_Num_Clones'], data['CloneFinder_Num_Clones'], alpha=0.5)\n",
    "# plt.xlabel('PyClone Num Clones')\n",
    "# plt.ylabel('CloneFinder Num Clones')\n",
    "# plt.title('PyClone vs CloneFinder (Num Clones)')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "\n",
    "# # PyClone vs LICHeE (Number of Clones)\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.scatter(data['PyClone_Num_Clones'], data['LICHeE_Num_Clones'], alpha=0.5)\n",
    "# plt.xlabel('PyClone Num Clones')\n",
    "# plt.ylabel('LICHeE Num Clones')\n",
    "# plt.title('PyClone vs LICHeE (Num Clones)')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "\n",
    "# # CloneFinder vs LICHeE (Number of Clones)\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.scatter(data['CloneFinder_Num_Clones'], data['LICHeE_Num_Clones'], alpha=0.5)\n",
    "# plt.xlabel('CloneFinder Num Clones')\n",
    "# plt.ylabel('LICHeE Num Clones')\n",
    "# plt.title('CloneFinder vs LICHeE (Num Clones)')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('tool_comparisons.png')\n",
    "\n",
    "# # Sample-level comparisons (variability within patients)\n",
    "# # Select a few patients with multiple samples for detailed analysis\n",
    "# patients_with_multiple_samples = data.groupby('Patient_ID').filter(lambda x: len(x) > 2)['Patient_ID'].unique()\n",
    "\n",
    "# if len(patients_with_multiple_samples) > 0:\n",
    "#     # Select the first 3 (or fewer) patients with multiple samples\n",
    "#     selected_patients = patients_with_multiple_samples[:min(3, len(patients_with_multiple_samples))]\n",
    "    \n",
    "#     plt.figure(figsize=(15, 5 * len(selected_patients)))\n",
    "    \n",
    "#     for i, patient in enumerate(selected_patients):\n",
    "#         patient_data = data[data['Patient_ID'] == patient]\n",
    "        \n",
    "#         # Plot Number of Clones for this patient across samples\n",
    "#         plt.subplot(len(selected_patients), 2, 2*i + 1)\n",
    "#         patient_clone_data = patient_data[['Sample_ID', 'PyClone_Num_Clones', 'CloneFinder_Num_Clones', 'LICHeE_Num_Clones']].melt(id_vars='Sample_ID')\n",
    "#         sns.barplot(x='Sample_ID', y='value', hue='variable', data=patient_clone_data)\n",
    "#         plt.title(f'Patient {patient}: Number of Clones by Sample')\n",
    "#         plt.xlabel('Sample')\n",
    "#         plt.ylabel('Number of Clones')\n",
    "#         plt.xticks(rotation=45)\n",
    "#         plt.legend(title='Tool')\n",
    "        \n",
    "#         # Plot Shannon Index for this patient across samples\n",
    "#         plt.subplot(len(selected_patients), 2, 2*i + 2)\n",
    "#         patient_shannon_data = patient_data[['Sample_ID', 'PyClone_Shannon_Index', 'CloneFinder_Shannon_Index', 'LICHeE_Shannon_Index']].melt(id_vars='Sample_ID')\n",
    "#         sns.barplot(x='Sample_ID', y='value', hue='variable', data=patient_shannon_data)\n",
    "#         plt.title(f'Patient {patient}: Shannon Index by Sample')\n",
    "#         plt.xlabel('Sample')\n",
    "#         plt.ylabel('Shannon Index')\n",
    "#         plt.xticks(rotation=45)\n",
    "#         plt.legend(title='Tool')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('patient_sample_comparisons.png')\n",
    "\n",
    "# # Identify samples with unusually high or low diversity\n",
    "# # Create a function to identify outliers\n",
    "# def identify_outliers(df, column, threshold=1.5):\n",
    "#     Q1 = df[column].quantile(0.25)\n",
    "#     Q3 = df[column].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "#     lower_bound = Q1 - threshold * IQR\n",
    "#     upper_bound = Q3 + threshold * IQR\n",
    "    \n",
    "#     outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "#     return outliers\n",
    "\n",
    "# # Identify outliers in diversity metrics\n",
    "# outliers_pyclone = identify_outliers(data, 'PyClone_Shannon_Index')\n",
    "# outliers_clonefinder = identify_outliers(data, 'CloneFinder_Shannon_Index')\n",
    "# outliers_lichee = identify_outliers(data, 'LICHeE_Shannon_Index')\n",
    "\n",
    "# print(\"\\nOutlier analysis:\")\n",
    "# print(f\"PyClone Shannon Index outliers: {len(outliers_pyclone)}\")\n",
    "# print(f\"CloneFinder Shannon Index outliers: {len(outliers_clonefinder)}\")\n",
    "# print(f\"LICHeE Shannon Index outliers: {len(outliers_lichee)}\")\n",
    "\n",
    "# # Create a consensus score (average normalized diversity across tools)\n",
    "# # First, create normalized versions of each metric\n",
    "# for tool in ['PyClone', 'CloneFinder', 'LICHeE']:\n",
    "#     for metric in ['Num_Clones', 'Shannon_Index']:\n",
    "#         col = f\"{tool}_{metric}\"\n",
    "#         # Skip columns with all NaN values\n",
    "#         if not data[col].isna().all():\n",
    "#             min_val = data[col].min()\n",
    "#             max_val = data[col].max()\n",
    "#             if max_val > min_val:  # Avoid division by zero\n",
    "#                 data[f\"{col}_normalized\"] = (data[col] - min_val) / (max_val - min_val)\n",
    "\n",
    "# # Calculate consensus scores (mean of normalized values)\n",
    "# data['Consensus_Num_Clones'] = data[[col for col in data.columns if 'Num_Clones_normalized' in col]].mean(axis=1)\n",
    "# data['Consensus_Shannon_Index'] = data[[col for col in data.columns if 'Shannon_Index_normalized' in col]].mean(axis=1)\n",
    "\n",
    "# # Create a visualization of the consensus scores\n",
    "# plt.figure(figsize=(12, 8))\n",
    "\n",
    "# # Rank patients by average consensus diversity\n",
    "# patient_avg_diversity = data.groupby('Patient_ID')['Consensus_Shannon_Index'].mean().sort_values(ascending=False)\n",
    "# patient_order = patient_avg_diversity.index\n",
    "\n",
    "# # Filter to patients with data from all three tools\n",
    "# complete_data_patients = data.dropna(subset=['PyClone_Shannon_Index', 'CloneFinder_Shannon_Index', 'LICHeE_Shannon_Index'])['Patient_ID'].unique()\n",
    "\n",
    "# if len(complete_data_patients) > 0:\n",
    "#     complete_data = data[data['Patient_ID'].isin(complete_data_patients)]\n",
    "    \n",
    "#     # Create a heatmap of consensus diversity by patient\n",
    "#     patient_sample_pivot = complete_data.pivot_table(\n",
    "#         index='Patient_ID', \n",
    "#         columns='Sample_ID', \n",
    "#         values='Consensus_Shannon_Index',\n",
    "#         aggfunc='first'\n",
    "#     )\n",
    "    \n",
    "#     plt.figure(figsize=(12, max(8, len(patient_sample_pivot) * 0.4)))\n",
    "#     sns.heatmap(patient_sample_pivot, cmap='viridis', annot=True, fmt='.2f')\n",
    "#     plt.title('Consensus Shannon Index by Patient and Sample')\n",
    "#     plt.xlabel('Sample ID')\n",
    "#     plt.ylabel('Patient ID')\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('consensus_diversity_heatmap.png')\n",
    "\n",
    "# print(\"\\nKey insights:\")\n",
    "# print(\"1. Average number of clones detected:\")\n",
    "# for tool in ['PyClone', 'CloneFinder', 'LICHeE']:\n",
    "#     col = f\"{tool}_Num_Clones\"\n",
    "#     print(f\"   - {tool}: {data[col].mean():.2f} clones (range: {data[col].min()}-{data[col].max()})\")\n",
    "\n",
    "# print(\"\\n2. Average Shannon diversity index:\")\n",
    "# for tool in ['PyClone', 'CloneFinder', 'LICHeE']:\n",
    "#     col = f\"{tool}_Shannon_Index\"\n",
    "#     print(f\"   - {tool}: {data[col].mean():.2f} (range: {data[col].min():.2f}-{data[col].max():.2f})\")\n",
    "\n",
    "# # Identify patients with highest and lowest diversity\n",
    "# top_diverse_patients = patient_avg_diversity.head(3)\n",
    "# bottom_diverse_patients = patient_avg_diversity.tail(3)\n",
    "\n",
    "# print(\"\\n3. Most heterogeneous patients (highest average Shannon Index):\")\n",
    "# for patient, score in top_diverse_patients.items():\n",
    "#     print(f\"   - Patient {patient}: {score:.2f}\")\n",
    "\n",
    "# print(\"\\n4. Most homogeneous patients (lowest average Shannon Index):\")\n",
    "# for patient, score in bottom_diverse_patients.items():\n",
    "#     print(f\"   - Patient {patient}: {score:.2f}\")\n",
    "\n",
    "# # Agreement between tools\n",
    "# print(\"\\n5. Tool agreement analysis:\")\n",
    "# # Calculate percentage of samples where all tools agree on high/low diversity\n",
    "# # (using median as threshold)\n",
    "# median_pyclone = data['PyClone_Shannon_Index'].median()\n",
    "# median_clonefinder = data['CloneFinder_Shannon_Index'].median()\n",
    "# median_lichee = data['LICHeE_Shannon_Index'].median()\n",
    "\n",
    "# data['PyClone_High'] = data['PyClone_Shannon_Index'] > median_pyclone\n",
    "# data['CloneFinder_High'] = data['CloneFinder_Shannon_Index'] > median_clonefinder\n",
    "# data['LICHeE_High'] = data['LICHeE_Shannon_Index'] > median_lichee\n",
    "\n",
    "# # Calculate agreement\n",
    "# complete_rows = data.dropna(subset=['PyClone_High', 'CloneFinder_High', 'LICHeE_High'])\n",
    "# all_agree_high = complete_rows[complete_rows['PyClone_High'] & complete_rows['CloneFinder_High'] & complete_rows['LICHeE_High']]\n",
    "# all_agree_low = complete_rows[~complete_rows['PyClone_High'] & ~complete_rows['CloneFinder_High'] & ~complete_rows['LICHeE_High']]\n",
    "\n",
    "# print(f\"   - Samples where all tools agree on high diversity: {len(all_agree_high)} ({len(all_agree_high)/len(complete_rows)*100:.1f}%)\")\n",
    "# print(f\"   - Samples where all tools agree on low diversity: {len(all_agree_low)} ({len(all_agree_low)/len(complete_rows)*100:.1f}%)\")\n",
    "# print(f\"   - Samples with mixed results: {len(complete_rows) - len(all_agree_high) - len(all_agree_low)} ({(len(complete_rows) - len(all_agree_high) - len(all_agree_low))/len(complete_rows)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
